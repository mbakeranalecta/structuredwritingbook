chapter:(#chapter.audit) Auditing
    <<<(annotations.sam)
    
    index:: type, term
        concept, auditing
    
    If the {conformance algorithm} is about making sure that an individual item meets its constraints, the {audit algorithm} is about making sure that the content set as a whole meets its constraints.[*1] It is not a given that if every item in the set meets its individual constraints that the whole collection meets its constraints. Issues of completeness and referential integrity arise between the items in a collection and are a considerable source of complexity that affects every structured writing algorithm that deals with the relationship between different pieces of content. 

    footnote:(*1)
        {Content strategists}(concept "content strategy") often use the term "content audit" to mean a current state analysis performed at the beginning of a website redevelopment project. A content strategy content audit is about cataloging, and possibly categorizing, the content you already have. I am using the word audit to refer to an ongoing and or recurring activity in which a you ensure that a content set is meeting or continuing to meet its goals.

    As such, auditing is fundamentally a {content management} function. It is about making sure that:
    
    * The definition of the content set is correct (we know what types of content it should contain, and which instances of each type)
    
    * The content set is complete (it contains all the items of each type that it should)
    
    * The content set is uncontaminated (it does not contain any items or types it should not)
    
    * The content set is integrated (it expresses all of the relationships between items that it should)
    
    * Each item in the content set {conforms} to its constraints
        
    Auditing a large content set is difficult and many CMS solutions are deficient in audit capabilities. The main reason for this is that with the way most content is recorded and stored ({media domain} or generic {document domain} formats), it is very difficult to mechanically assess what content you have and what state it is in. It is hard to know if you have all the pieces you should have if you can't tell exactly what each piece is. 
    
    One of the biggest, and least appreciated, benefits of structured writing is that it makes content more auditable. When content management systems fail or become unmanageable, the root cause is often either an incorrect distribution of complexity from day one or a failure to audit: either lack of attention to regular audits, or the lack of ability to audit effectively. Without the ability to audit effectively, content sets often end up incomplete, corrupt, and poorly integrated, which reduces quality and increases costs at every stage of the process. It creates a body of unmanaged complexity that every downstream process and person has to deal with, including, of course, the reader. And a vicious cycle can develop in which writers, frustrated with the difficulties of the system, create workarounds that further corrupt the information set. Unmanaged complexity breeds more complexity. Whatever expenses you may incur to implement a more structured structured writing approach could well be offset by the savings associated with more effective auditing of the content set.  
    
    section: Correctness of the definition of the content set
    
        {Content strategists} will spend a great deal of time and effort developing a content plan (usually this is for a website, but the same principle applies to any content set). How they do this is beyond the scope of this book, but the result should be a definition of the content set: which types of information it is supposed to contain and what instances of those types. (This definition is based, of course on the goals it is designed to achieve, which it is the business of the content strategist to define.) 
        
        The definition of a content set is not necessarily static. It is not necessarily a fixed list of topic types or of specific topics to be developed. For one thing, the subject matter may change during the course of content development, which would change the content pieces needed, and perhaps require new content types or modifications to existing types. Second, the exact set of pieces or types may not be knowable at the outset. Content development explores a complex set of relationships between subject matter and the needs and background of the reader that cannot be fully known without traveling the ground in detail. 
        
        It is hard to be disciplined and deliberate in evolving the big picture model of the content set if you are not disciplined and deliberate in how you create the pieces. If you ask a writer to write a piece and then ask them after they are finished to describe the job it does by assigning CMS metadata to it, they will tag it using the terms that seem like the closest fit to the content they have already written, but they will probably not revise the content to fit the labels they are applying to it. Their view will not be that the content is wrong, but that the labels don't fit the content. And indeed, since the labels won't fit the content, you won't really know what type of content you have in your collection.

        If you don't really know what type of content you have, you can't really tell if the definitions for your content are correct. Some content may perform poorly because it does not fit the type definition properly. But you can't tell whether it failed because it didn't fit the type definition or because the type definition is wrong. Thus you don't know what to fix. 
        
        Of course, a writer may come up with something that is better than the current type definition. This is a good thing. If it really is better, you want to change the type definition to match it so that all future content will be better as well. But if the author simply tags it according to the current CMS tags, you will never know that it is a different model, never have the chance to test the new model to see if it is better, and never have the chance to update the definition so that new content follows the successful new model. Unless your content types are codified and auditable, you won't detect improvements in the types and they won't carry over to other content. See [#chapter.repeatability] for more on this. 
        
        Having strong well defined content types makes it easier to audit your  types to make sure they are doing the job they were designed to do. Similarly, having strong well defined content types means that you can have greater assurance that each item is doing the job it is supposed to do, which helps you make sure you have covered all the subjects you should have. 
        
    section: Bottom-up content planning
        
        But structured writing can do more than this to help you audit the definition of the content set. If you create content in the {subject domain}, including {annotating the subjects}(concept "subject annotation") that you mention in the text, you can use algorithms to extract a list of the types and subjects that your content is actually talking about. In your initial top-down plan, you may not have thought about the need for content on a certain subject or to support a certain activity, but if that subject or that activity start showing up in the body of your content, that is a strong indication that those subjects and activities are related to the purpose of your content set and should probably be included in the definition of the content set. 
        
        Subject domain structured writing is how you know what your content is actually talking about, what every author is discovering or thinks needs saying. Content needs are ultimately driven by subject matter and it is your writers working with the subject matter every day who are on top of what the subject matter is and how it is changing. Bottom-up content planning distributes the responsibility for discovery outward and for coordination inward to keep you in touch with evolving content needs. Without this information flow it is very difficult to establish that the content set is meeting its coverage goals. (We will see the same pattern of information flow again in [#chapter.taxonomy].)
        
        This actually attacks two audit problems. If writers are writing about things outside your current coverage definition, either your coverage definition needs updating, or writers are polluting the content set with irrelevant material. 
        
    section: Ensuring the content set remains uncontaminated
    
        {Subject domain} content structures and annotations can help you prevent contamination of the content set by irrelevant material. But more important than catching writers in the act is catching the flaws in content types that allow for contamination to creep in. 

        A major form of contamination in any content set is redundant content. As we noted in [#chapter.duplication], we have to be careful in how we define redundancy, because it is not simply a matter of only addressing a subject once. It is a matter of addressing an audience need only once, and that may require several topics on the same subject addressed to different readers. But it is all too easy for duplicate content to sneak into a content set. Some of it comes in because the same functionality is repeated in many products or in content delivered to different media. Some comes in through writers simply not knowing that suitable content already exists. 
        
        {Content reuse} is a major motivator for structured writing for exactly this reason. But the {content reuse} algorithm only addresses the problem of how to reuse content. It provides a method to reuse content you are aware of. It does not prevent you from duplicating content because you did not look for or did not find existing reusable content. You need to audit your content regularly to make sure that content is findable by writers who may want to reuse it, and to make sure that duplication is not creeping in.   

        There are natural language processing algorithms that will attempt to identify redundant content in a content set, but such algorithms focus on similar texts. This is not enough. The same or similar sequences of words may occur in different places without being redundant. They may mean different things, or perform different roles, in context. On the other hand, redundant pieces of information may be expressed in very different words. It is redundant information, not redundant phrases, the we care about. 
        
        Even when redundancies are found, they may be very difficult to consolidate if they don't have similar boundaries within their respective documents (the {composability} problem -- see [#chapter.composition]). Strongly typed subject domain content, meaning content that conforms to a model that breaks down and enforces the various pieces of information required to correctly cover a topic, makes it possible to detect duplication in a much more formal and precise way. Duplication of subject matter is much easier to detect when content is captured in the subject domain.

        A person who consults a repository to see if there is a piece of content they can use relies on the ability to query the repository in a sensible way for the type of content they are looking for. They also rely on their ability to recognize the content when they see it, and on it actually being strongly conformant so that they can use it with confidence. Subject domain topic typing helps with all of these things. Subject domain labeling of document and media domain content can help as well, but only if it is conformant, a problem discussed in [#chapter.conformance]. The easier it is to correctly identify reusable content and use it, the less corruption of the repository will occur.   

    section: Ensuring that the content set is well integrated
    
        A content set is never a collection of wholly independent pieces. The items in the set have relationships to each other that matter to the reader. (We will look at this in more depth in [#chapter.architecture].) Whether you express those relationships through {links} or {cross references}, or whether you rely entirely on tables of contents and indexes, it is still important to understand and manage the relationships between items. 
        
        Relationships between items may matter for management reasons as well. If you have documentation for multiple releases of a product, the relationship between the documentation for feature X in version 3 and that for feature X in version 2 matters to you. It may matter because the feature has not changed and you can reuse the item. It may matter because an error was found in version 2 and you want to fix it in version 3 as well. And if you put this content online, the relationship may matter for the reader as well, if they search for feature X and get the result for version 2 when they are using version 3.
        
        You can describe the relationship between items externally. Items are related whenever they have the same value in any one of their metadata fields. Which field it is tells you what the relationship is. Finding the relevant metadata field to look at allows you to manage the relationship. But the same problem exists here as it always does with all external metadata (see [#chapter.content-management]) -- the content may not conform to the metadata, and without structured writing in the content itself, it is hard to audit the conformance of the content to its metadata. In-band information is always more reliable than out-of-band information.
        
        But the bigger problem is that external metadata does not map the important relationship that can exist between a part of one item and the whole of another item. Are function names mentioned in the programming topics all listed in the API reference? Are the utensils mentioned in a recipe all covered in the appendix of kitchen tools? These are important content relationship questions, but their importance cannot be mapped with external metadata. You need subject domain markup inside the piece that identifies function calls and the names of kitchen tools. 
        
        Structured writing, particularly in the subject domain, helps you discover and manage these relationships by making clear the subjects on which these relationships are based. 
        

    section: Making content auditable
    
        I have talked all through this chapter about how using strong content types makes content easier to audit. What is a strong content type? Fundamentally, a strong content type is one that makes explicit what the content is supposed to say an how it is supposed to say it. Or, to put it another way, a strong content type is one that captures, enforces, or factors out the major constraints of the content, including its major rhetorical constraints. A strong content type constrains the interpretation of content as well as its composition, and the more reliably content can be interpreted, the more reliably it can be audited. Strong content types are almost always in the {subject domain}.

        It is possible for content to conform to all of its rhetorical constraints without the use of structured writing techniques. But strong content types provide explicit guidance to the author and facilitate the use of {conformance} algorithms. They are created to meet your conformance goals. Similarly with auditing, you specify the content structures you need in order to meet your auditing goals. 
        
        Auditing is sometimes not as straightforward as conformance, even with structured writing techniques in place. Auditing often requires human review, not only to make sure that all subjects have been covered, but to discover new issues or subjects that need to be addressed. Human review of a large content set is difficult, though, due to the sheer amount of content. An {audit algorithm} can simplify this work by creating different views of the content set that humans can review more easily.

        Suppose, for instance, that an organization is using subject-domain annotations to drive linking as described in [#chapter.linking]. Every topic in the collection is supposed to be indexed to state the type and names of the subjects it covers. Every mention of a significant subject is supposed be annotated with its type. The {linking algorithm} uses these annotations and index entries to link the content without any need for authors to create or manage links in the source text. But that does not guarantee that all the right links get made. There could be errors in indexing or annotation that are impossible to detect when {conformance} testing individual topics.

        We can use those same index entries and annotations to create audit reports for several purposes. These are some of the things we can do:

        * We can create a sorted list of all the phrases that have been annotated and see if they are being annotated consistently. This will tell us a lot about the types we are using, how well they are understood, and what instances of each type we should be covering.
        
        * We can create a list of all the phrases that have been indexed and check it against our content plan (perhaps against a taxonomy, if we have one). This will tell us a lot about whether our coverage is complete, whether our writers are getting off track, or whether our content plan or our taxonomy is off base with reality.
        
        * We can create a sorted list of all the index terms and check it against the list of annotated phrases to find phrases are that being indexed but not annotated or annotated but not indexed. This can tell us is there are subject we are not covering, if writers are discussing subjects they should not be, or if some topics are not being indexed or annotated properly.
        
        * During the content development phase, the list of things that are annotated but not indexed will inevitable grow, as subjects are being referred to before the content that describes them is written. The trend line of the growth of new subjects being annotated vs subjects being indexed will allow you to track how close a content set is to completion, even in cases were defining the boundaries in advance is difficult.

        
    # FIXME: Needs an example audit report. Generate one for the book to illustrate the idea.
        
    section: Performance auditing
    
        Auditing marketing content can be easier than auditing other types of content because you can measure content against behavioral goals. Are readers taking the actions you want, and does a content change produce more of the desired action? All content aims at changing reader behavior in one way or another, but not all behavioral changes are easy to measure. Unless the behavior in question is an interaction with the web page that contains the content, behavioral changes are both hard to observe and hard to attribute.

        Whether auditing the performance of your content is easy of difficult, however, when you do have evidence that a particular piece of content is performing well, you want to be able to repeat that success. I will talk more about repeatability in [#chapter.repeatability].
        
        If a piece of content follows a well defined set of constraints, it is much easier to figure out why it was successful. Rather than trying A/B testing of individual pieces, for instance, you can try A/B testing of content that follows different structures. If one structure consistently outperforms the other, you can adopt that structure for all your content. 
        
        Of course, to ensure continued success (or to ensure that the same structure is continuing to perform well) you need to be sure that the new content you are producing does actually conform to the proven successful structure. This means that you need to apply the {conformance} algorithms to your new content creation, as well as {auditing} it for overall compliance and continued success. 

        

        
    
    