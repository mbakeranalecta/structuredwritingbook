!smart-quotes: on
chapter:(#chapter.audit) Auditing
    <<<(annotations.sam)
    
    subjects:: type, term
        algorithm, auditing 
    
    Although the appropriate structured writing techniques can help keep your content set from falling into disorder, maintaining a healthy content collection and a well-integrated information architecture still requires constant monitoring to find and fix errors and to ensure that your processes are working as well as possible.

    In [#chapter.conformance], I looked at how structured writing techniques can improve the conformance of individual pieces of content. In [#chapter.metadata], I looked at how they can help you maintain metadata and taxonomy across your information set. Now, let's look at how these techniques can help you audit a content set to ensure that it meets its constraints and that its constraints are consistent with your goals.[*1] 

    Even if every item in a collection meets its individual constraints, that does not mean that the whole collection meets its {constraints}+(index "constraints;auditing"). For instance, even if every item in your collection conforms, that does not mean that the collection is complete, that all the links that should exist do exist, or that your links point to the best resources. For issues like these, you need both a sound strategy for creating and supporting your {information architecture}+(index "information architecture;auditing your") and a sound audit process to make sure everything is in its place. 

    footnote:(*1)
        {Content strategists} often use the term "content audit" to mean a current-state analysis performed at the beginning of a website redevelopment project. A content strategy content {audit}+(index "auditing;current-state versus ongoing") is about cataloging, and possibly categorizing, the content you already have. Here, I use the word audit to refer to an ongoing activity to ensure that your content set continues to meet its goals.

    Auditing is about making sure that:
    
    * The content set is defined correctly (you know what types of content it should contain and which instances of each type)
    
    * The content set is complete (it contains all the items of each type that it should)
    
    * The content set is uncontaminated (it does not contain any items or types it should not)
    
    * The content set is integrated (it expresses all of the relationships between items that it should)
    
    * Each item in the content set {conforms}+(index "conformance;auditing") to its constraints
        
    Auditing a large content set is difficult, and many CMS solutions are deficient in audit capabilities. The main reason they are deficient is that most content is recorded and stored in {media-domain}+(index "media domain;auditing content in the") or generic {document-domain}+(index "document domain;auditing content in the") formats, which make it difficult to mechanically assess what content you have and what state it is in. It is hard to know if you have all the pieces you should have if you can't tell exactly what each piece is. 
    
    One of the biggest, and least appreciated, benefits of structured writing is that it makes content more auditable. When {content management systems}+(index "content management system;auditing content in a") fail or become unmanageable, the root cause is often either an incorrect distribution of complexity from day one or a failure to audit. Failure to audit may mean a lack of attention to regular audits or an inability to audit effectively. Without effective audits, your content set can end up incomplete, corrupt, and poorly integrated, which reduces quality, increases costs, and creates a body of unmanaged complexity that every downstream process and person has to deal with, including, of course, the reader. A vicious cycle can develop in which writers, frustrated with the difficulties of the system, create workarounds that further corrupt the information set. Unmanaged complexity breeds more complexity. Whatever expenses you may incur to implement a more structured writing approach could well be offset or even exceeded by the savings associated with more effective auditing.
    
    section: Auditing the definition of the content set
    
        {Content strategists} spend a great deal of time and effort developing a content plan (usually this is for a website, but the same principle applies to any content set). How they do this is beyond the scope of this book, but the result should be a definition of the content set: which types of information it is supposed to contain and which instances of those types. This definition is based, of course, on the goals for the content, which the content strategist needs to define.
        
        The definition of a content set is not necessarily static. It is not a fixed list of topic types or specific topics to be developed. First, the subject matter may change during content development, which would change the content pieces needed and, perhaps, require new content types or modifications to existing types. Second, the exact set of pieces or types may not be knowable at the outset. Only during content development can you fully explore and understand the complex set of relationships between subject matter and the needs and background of readers.
        
        It is hard to be disciplined and deliberate in evolving the big picture model of the content set if you are not disciplined and deliberate in how you create the pieces. If you ask writers to assign CMS metadata after they have written content, they will tag that content using the terms that seem like the closest fit to the content they have already written. However, they will probably not revise the content to fit the labels. Their view will not be that the content is wrong, but that the labels don't fit the content. And since the labels won't fit the content, you won't really know what type of content you have in your collection.

        If you don't know what type of content you have, you can't tell if you have defined your content set correctly. Content may perform poorly because it does not fit the type definition properly or because the type definition is wrong. Unless you can determine what the problem is, you won't know what to fix. 
        
        Of course, a writer may come up with a better type definition. This is a good thing. If it really is better, you will want to change all similar content to match this improved model. However, if your models are not formally defined and the writer executes this new model without defining it as a formal structure and then just tags it using the current CMS tags, you will never know about the new model, never have the chance to test it to see if it is better, and never have the chance to update the official definition so that new content follows the new model. Unless your content types are codified and auditable, you won't detect improvements in the types and they won't carry over to other content. See [#chapter.repeatability] for more on this. 
        
        Having strong well-defined content types makes it easier to audit your content types to make sure they do the job they were designed to do. Similarly, having strong, well-defined content types helps ensure that each item does the job it is supposed to do and that you cover all the subjects you should cover.
        
    section: Assessing completeness

        block-index:
            {completeness}+(index "completeness")
            
        Structured writing can also help you assess the completeness of your content set. If you create content in the {subject domain}+(index "subject domain;auditing completeness in the"), including {annotating the subjects}(concept "subject annotation") that you mention in the text, you can use algorithms to extract a list of the types and subjects that your content talks about. In your initial top-down plan, you may not have thought about the need for content that covers certain subjects or that supports certain activities, but if those subjects or activities start showing up in your content, that is a strong indication that they are related to the purpose of your content set and should probably be included in the definition of the content set. 
        
        {Subject-domain}+(index "subject domain;auditing completeness in the") structured writing lets you know what your content is actually talking about and what writers are discovering or think needs saying. Content needs are ultimately driven by subject matter, and it is your writers, who work with the subject matter every day, who are on top of what the subject matter is and how it is changing. Bottom-up content planning distributes the responsibility for discovery outward and for coordination inward, which keeps you in touch with evolving content needs. Without this information flow it is difficult to determine whether your content set is meeting its coverage goals. (We saw the same pattern of information flow with bottom-up taxonomy development in [#chapter.taxonomy].)
        
        The ability to compile lists of subjects your writers are writing about attacks two audit problems. If writers are writing about things outside your current coverage definition, either your coverage definition needs updating, or writers are polluting the content set with irrelevant material. 
        
    section: Avoiding contamination
    
        Subject-domain content structures and annotations can help prevent contamination of the content set by irrelevant or poor quality material. But more important than catching writers in the act is catching flaws in content types that allow contamination to creep in. 

        A major form of contamination is redundant content. As I noted in [#chapter.duplication], you have to carefully define what it means to avoid redundant content, because it is not simply a matter of addressing a subject only once. Avoiding redundancy means addressing an audience need only once, and that may require several topics on the same subject addressed to different readers.

        It is all too easy for duplicate content to sneak into a content set. Some of it comes in because the same functionality is repeated in many products or in content delivered to different media. Some comes in because writers simply don't know that suitable content already exists. 
        
        {Content reuse}+(index "content reuse;detecting duplicate content") is a major motivator for structured writing for exactly this reason. But the {content-reuse} algorithm addresses only the problem of how to reuse content. It provides a method to reuse content you are aware of. It does not prevent you from duplicating content when you did not look for or did not find existing reusable content. You need to audit your content regularly to make sure that writers can find potentially reusable content and to make sure that duplication does not creep in.   

        Even when you find redundancies, they can be difficult to consolidate if they don't have similar boundaries within their respective documents. This is an example of the {composability} problem, which I discussed in [#chapter.composition]. Strongly typed subject-domain content -- content that conforms to a model that breaks down and enforces constraints on the pieces of information required to cover a topic -- enables you to detect duplication in a much more formal and precise way. Duplicate subject matter is much easier to detect when content is captured in the subject domain.

        When you consult a {repository} to see if there is content you can use, you need to be able to query the repository in a sensible way for the type of content you are looking for and recognize appropriate content when you see it. And you need to be able to rely on that content conforming to its type in order to use it with confidence. Subject-domain topic typing helps with all of these things. Subject-domain labeling of document- and media-domain content can help as well, but only if it conforms to the appropriate constraints, a problem discussed in [#chapter.conformance]. The easier it is to correctly identify reusable content and use it, the less corruption of the repository will occur.   

    section: Maintaining integration
    
        A content set is never a collection of wholly independent pieces. The pieces have relationships with each other that matter to readers.  Whether you express those relationships through {links}+(index "linking;maintaining integration through") or cross references, or whether you rely entirely on tables of contents and indexes, it is still important to understand and manage them.
        
        Relationships matter for management reasons as well. If you have documentation for multiple releases of a product, the relationship between the documentation for feature X in version 3 and that for feature X in version 2 matters. It may matter because the feature has not changed, and you can reuse the item. Or it may matter because you found an error in version 2 and want to fix it in version 3. And if you put this content online, the relationship matters for readers. You don't want a search from a reader using version 3 to return information for version 2.
        
        You can describe relationships between items externally. Items are related whenever they have the same value in any one of their {metadata} fields. Which field it is tells you what the relationship is. Finding the relevant metadata field allows you to manage the relationship. But the same problem exists here as with all external metadata (see [#chapter.content-management]) -- the content may not conform to the metadata, and, without structured metadata in the content itself, it is hard to audit how well content conforms to its metadata. In-band information, such as subject-domain annotations, is always more reliable than out-of-band information, such as external metadata.
        
        But the bigger problem is that external metadata does not map the important relationship that can exist between a part of one item and the whole of another item. Are the function names mentioned in the programming topics all listed in the API reference? Are the utensils mentioned in a recipe all covered in the kitchen tools appendix? These are important content relationship questions, but these relationships cannot be mapped using external metadata. You need subject-domain markup inside the content that identifies function calls and the names of kitchen tools. 
        
        Structured writing, particularly in the subject domain, helps you discover and manage these relationships by clear identifying the subjects on which these relationships are based. 
        
    section: Making content auditable
    
        I have talked all through this chapter about how using strong content types makes content easier to audit. What is a strong content type? Fundamentally, a strong content type makes explicit what the content is supposed to say and how it is supposed to say it. Or, to put it another way, a strong content type captures, enforces, or factors out the major constraints of the content, including its major rhetorical constraints. A strong content type constrains the interpretation of content as well as its composition, and the more reliably content can be interpreted, the more reliably it can be audited. Strong content types are almost always in the {subject domain}+(index "subject domain;information typing in the")+(index "information typing").

        You can create content that conforms to its rhetorical constraints without using structured writing techniques. But as we saw in [#chapter.conformance], strong content types provide explicit guidance to the writer and facilitate the use of {conformance}+(index "conformance;auditing") algorithms. Thus, you design content structures to support conformance. The same holds true for auditing; strong content types make your content easier to audit, so you design content structures to help you meet your auditing goals. 

    section: Facilitating human review
        
        Auditing is sometimes not as straightforward as conformance, even with structured writing techniques in place. Auditing often requires human review, not only to make sure that all subjects have been covered but also to discover new issues or subjects that need to be addressed. Human review of a large content set is difficult due to the sheer volume of content. An algorithm can simplify this work by creating different views of the content set that humans can review more easily. This is an application of the {content generation algorithm} for internal purposes.

        Suppose, for instance, that you are using subject-domain annotations to drive linking, as described in [#chapter.linking]. Every topic in the collection is supposed to be annotated to state the type and name of each subject it covers. Every mention of a significant subject is supposed be annotated with its type. The {linking algorithm}+(index "linking;annotations and") uses these annotations to link the content without requiring you to create or manage links in the source text. But that does not guarantee that all the right links get made. There could be errors in annotation that are impossible to detect when {conformance}+(index "conformance;auditing") testing individual topics. But you can do an lot to catch these kinds of errors when you audit the content set as a whole.

        These are some of the audit functions you can perform based on annotations and index entries:

        * Create a sorted list of all annotated phrases and see if they are annotated consistently. This will tell you a lot about your subject types, including how well they are understood and what instances of each type you should cover.
        
        * Create a list of all annotated topics and check it against your content plan, using a {taxonomy}+(index "taxonomy;auditing and a") if you have one. This will tell you a lot about whether your coverage is complete, whether your writers are on track, and whether your {content plan} or taxonomy matches reality.
        
        * Create a sorted list of all the annotated terms and check it against the list of annotated phrases to find phrases that are indexed but not annotated or annotated but not indexed. This can identify subjects that are not covered, content that covers extraneous subjects, and topics that are not being indexed or annotated properly.

        You can also use the same technique for project management purposes. Early in the content development phase, the list of annotations on phrases that don't match the annotations on any topic will inevitably grow, as writers annotate subjects that have not been documented yet. Over time, however, new topics will start to fill in those gaps, and those new topics will contain fewer references to subjects that have not yet been documented. The trend line of the growth of new subjects being annotated versus annotated topics being created will rise and then fall, allowing you to track how close a content set is to completion, even in cases where defining the boundaries in advance is difficult.

        Content is one of the hardest assets to audit and inventory. Structured writing, particularly subject-domain structured writing, can greatly aid in establishing an effective audit function for your content. An effective content audit process, in turn, can help avoid the gradual decline of order and reliability that affects so many managed content sets and leads to the slow death by strangulation of so many content management systems. 

        
    # FIXME: Needs an example audit report. Generate one for the book to illustrate the idea.
        
