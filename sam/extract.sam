!smart-quotes: on
chapter:(#chapter.extract) Extract

    <<<(annotations.sam)
    
    subjects:: type, term
        algorithm, extract algorithm

    A great deal of content, particularly technical and business content, is essentially a report in human language on the features of a product, process, or data set. Much of the data that defines those things is contained in some kind of formal data set, such as a database or software source code. 

    In the traditional approach, writers treat these data sets as a research source. They look up information and then write their content. This is complex and requires a large amount of work, which is ongoing because writers must keep their content in sync with the original data set as it changes.
    
    The source data for those systems is, in effect, subject-domain content for those products, processes, and data sets. Rather than researching and recreating that content, you can use structured writing techniques to extract information from those sources and create or validate content. This is the extract algorithm. It partitions and redirects the complexity of dealing with these sources away from writers and towards information architects and content engineers who create algorithms that extract the data and generate subject-domain content from it. 
       
    section: Tapping external sources of content

        subjects:: type, term
            concept, external content

        block-index:
            {external content}+(index "content;external")
    
        I have talked throughout this book about moving content from the media domain to the document domain and from the document domain to the {subject domain}+(index "subject domain;extract algorithm and the"). I have described the advantages of creating content in the subject domain and looked at the processing algorithms that use subject-domain content to produce various kinds of publications in different media. 

        Subject-domain content is created and annotated in structures that are based on the subject matter rather than on the structure of documents or media. Subject-domain structures tell you what the content is about rather than how it should be published. You can, therefore, write algorithms that process content based on what it is about rather than how it is presented. This allows you to transfer and delay decisions about presentation and formatting and to have algorithms make those decisions based on the subject-domain markup.
        
        Any data source that is contained in subject-domain structures is a source of subject-domain content, regardless of whether or not that source was created with the expectation that it would be used to produce content. This includes virtually all {databases}+(index "databases;as source of content") and quite a bit of software code. It also includes all authored content (under an appropriate license) that contains usable and accessible subject-domain structures or annotations. You can potentially extract subject-domain content from all of this material. This means you can use the extract algorithm to feed content to any of the structured writing algorithms that work with subject-domain content.

        As a source of {subject-domain}+(index "subject domain;extract algorithm and the") content, the extract algorithm naturally {separates content from formatting} and contributes to the {differential single sourcing algorithm}.
    
        By tapping existing information to build content, the extract algorithm works hand in hand with the {content reuse}+(index "content reuse;extract algorithm and") algorithm. In fact, it is really the highest expression of reuse, since it reuses content not only in the content system but also from the organization at large or even beyond the organization, which further reduces duplication within the organization.
        
        Because the extract algorithm taps directly into external sources of information, it is a great source of information for content auditing (which I discuss in [#chapter.audit]). At one level, it provides a canonical source of information to validate existing content against. At another level, it factors out part of the conformance problem from the authoring function. It transfers the entire responsibility for maintaining information to the creators of the source you are extracting content from, which is a responsibility they already have.   
        
    section: Extracting information created for other purposes    
        
        There is nothing new, of course, about generating content from database records. Database reporting is an important and sophisticated field in its own right, and it would be entirely correct to characterize it as a type of structured writing. What sets it apart from other structured writing practices is that the databases it reports on serve other business purposes besides being sources of content. An insurance company policy database, for instance, may be used to publish custom benefit booklets for plan participants, but it is also used for processing claims. The design of the structures and data entry interfaces of these systems tends to fall outside the realm (and the notice) of writers and authoring system designers. 

        This is a pity, because organizations often develop separate processes, tools, and repositories for content creation that duplicate information that is already contained in databases that are researched, validated, recorded, and managed independently. Rather than treating code and databases as sources of content, writers treat them as research sources. They look up information in these sources, write content to describe the information from those sources, and then store that content in a separate {repository}. 
        
        The essence of the problem is that many content organizations choose to work in the media domain or the document domain and have neither the tools not the expertise to bridge the gap to all this material already available in the subject domain. But even when content organizations extend their efforts into the subject domain, they often don't realize that the {subject-domain}+(index "subject domain;extract algorithm and the") content they plan to create already exists in the systems of another department. 

        Another drawback is that content produced from these other systems -- for instance, from a database reporting system -- exists in isolation from the rest of the content produced by the organization. Such content can often be quite sophisticated and beautifully formatted and published. But it is the product of an entire structured publishing chain that has to be separately developed and maintained. 

        In the field of software documentation you can see the same pattern with programming language API documentation. Much of the material of an API reference guide is a description of each function, what information is required as input (its parameters or arguments), the information it produces as output, and the errors or exceptions it can generate. All of this information already exists in the code that implements the function. 
        
        {API documentation tools} such as {JavaDoc}+(index "JavaDoc;API documentation using") or {Sphinx} extract this information from code and comments and turn it into publishable content. This is an application of subject-domain structured writing, and these API documentation tools implement an entire structured publishing system that produces final output, often in multiple formats. 

        And here you see the same problems again:

        * An entire publishing chain is maintained separately from the main content {publishing chain}+(index "publishing pipeline"). 

        * The content produced from this publishing chain is isolated from all the other content produced by the organization.

        * Much of the same information gets re-created in the form of programming guides or knowledge base articles and maintained in a separate {repository} using a separate tool chain. 
        
        There are other cases of entirely separate publishing chains producing information that is isolated from the rest of an organization's content. {Technical support}+(index "technical support") organizations create {knowledge bases}+(index "knowledge bases") to answer commonly asked questions. The material in these knowledge bases is technical communication, plain and simple, yet it usually exists in isolation from the product documentation set, even to the extent that users may not be able to search both the documentation and the knowledge base from the same search box. Most users, however, have no way of guessing whether the answers they are looking for are in the documentation, the support knowledge base, or a users' forum (yet another independent publishing system).

        You can address these {redundancies}+(index "redundant publishing pipelines")+(index "publishing pipeline;redundant") and the isolation that goes with them in two ways. One approach is to attempt to unify all content authoring and production in a single enterprise-wide system, often with a single set of content structures intended for use across all departments. However, this is expensive and disruptive, and it tends to create interfaces and structures that are less usable and less specific to your business functions than the ones they replace.

        This approach also ignores the local complexity of individual groups and subject matter, which means that complexity gets shoved downstream, eventually, to the reader. And many of these systems exist for purposes other than generating content, which means they have subject-domain structures specific to, and required for, the database or software functions they were built for. 
        
        The second approach is to leave the subject-domain systems in place (and perhaps create more of them) and feed their output into a common document-domain publishing {tool chain}+(index "tool chain;publishing") through {shared pipes}. The {publishing}+(index "publishing;document-domain") algorithm normally passes {subject-domain}+(index "subject domain;publishing and the") content through the document domain on its way to media-domain publication. Subject-domain content, by its nature, is not strongly tied to a particular document-domain structure, so integrating many sources of subject-domain content into a single publishing chain is not particularly onerous. The management-domain features of some tool chains make things more complicated, but since the subject domain  factors out a lot of the management domain, this is not an insurmountable problem.
        
        Most enterprise-wide content systems are based on document-domain languages. After all, there is no way to create a single enterprise-wide subject-domain system, since an enterprise creates content on many subjects for many audiences. In principle, a document-domain system should be capable of integrating content from domain-specific subject-domain systems.  Unfortunately, neither subject-domain systems nor enterprise content systems are commonly designed with this kind of integration in mind. 

        Because of this, you sometimes have to find ways to extract content from these sources and feed them into a unified publishing chain.
    
        A common example of the {extraction algorithm} is found in API documentation tools such as {JavaDoc}+(index "JavaDoc;API documentation using"). These tools parse application source code to pull out information such as the names of functions, parameters, and return values, which they use to create reference documentation (or at least an outline). These tools generate a human language translation of what the computer language code says. 
        
        How the extraction algorithm works depends on how the source data is structured, but it usually creates output in the subject domain that clearly labels the pieces of information it has culled from the source. For instance, a {Java}+(index "Java;function definitions in") function definition is a piece of structured content in which the role and meaning of each element can be determined from the grammar of the Java language (see [*fig.java-function-example]).

        figure:(*fig.java-function-example) The syntax of a Java function definition
            ```(java)
                boolean isValidMove(int theFromFile, 
                                    int theFromRank, 
                                    int theToFile, 
                                    int theToRank) {
                        // ...body
                    }
        
        The same information can be extracted by an algorithm that knows the grammar of {Java}+(index "Java;function definitions in") to produce something that looks more like subject-domain content (see [*fig.java-function-subject-domain]):

        figure:(*fig.java-function-subject-domain) Subject-domain rendering of a Java function definition
            ```(sam)
                java-function:
                    name: isValidMove
                    return-type: boolean
                    parameters:: type, name
                        int, theFromFile
                        int, theFromRank
                        int, theToFile
                        int, theToRank
                    
        [*fig.java-function-subject-domain] contains the same information as [*fig.java-function-example], but in a different structure. In this structure, however, the information can be easily accessed and processed through the publishing {tool chain}+(index "tool chain;publishing") like any other content.
        
        Any structured data source that consistently expresses the semantics of its data can be a source of subject-domain content. You just need to find a way to get at it. 
        
    section: The diversity of sources

        When it comes to drawing content from diverse sources, the term {single sourcing} can be misleading, because it can lead you to think that all source content is kept in a single place. Some vendors of content management systems would like to encourage this interpretation, but a better definition is that each piece of information comes from a single source. That is, there may be many sources, each containing different pieces of information, but each piece of information comes from only one source.
        
        Ensuring that you store content only once means making sure that both the information and the {repository} that contains it meet an appropriate set of constraints. The {constraints}+(index "constraints;uniqueness") that establish the uniqueness of a piece of content are different for different types of content and different subject matter, so such constraints are best expressed and enforced by systems that are designed for a particular type of content and subject matter.
                
        This is not to say that consolidation and standardization of sources is never appropriate. Often two departments will store substantially the same information with trivial differences in how it is structured and expressed. Combining the information from both departments into one source, or at least standardizing the way each department structures and expresses its data, makes data exchange simpler and more reliable. Most organizations have all kinds of isolated and ad hoc information that could potentially be managed much more efficiently and be more easily accessible with a degree of rationalization and centralization. But it does not follow that absolute {centralization}+(index "data centralization") of all data into a single system or a single data model is appropriate, useful, or even possible. 
        
        The best way to ensure that information is stored only once is to store each type of information in a system with the right constraints and the right processes for the people who create and manage that information. By tightly constraining each source, you prevent it from accidentally accepting information that should be stored in another system. By contrast, a single central system that is loosely constrained may end up accepting many copies of the same information because neither the system nor the users can detect the duplication. 

        Preserving many tightly constrained information systems that serve the needs of different departments may mean that an integrated publishing system will have to draw from diverse sources of information and content. Therefore, the ability to extract content from these sources and merge it with other content for publication is central to effectively partitioning complexity and eliminating duplication and error.
        
        Of course, a system that relies on drawing content from many different sources, even if all those sources are well-constrained and well-managed  introduces a lot of integration and maintenance complexity. Remember that the point of the exercise is to minimize the overall amount of unmanaged complexity in the system. Integrated systems can manage {complexity}+(index "complexity;integrated systems and") in sophisticated ways, but they also introduce complexity that has to be factored into the calculation. Sometimes it may be less complex and less costly to allow some {duplication}+(index "duplicate content;complexity of avoiding") of information between systems rather than try to manage the complex relationships required to eliminate all duplication. Sometimes the optimal solution is less than total integration. 
