!smart-quotes: on
chapter:(#chapter.extensible) Extensible and Constrainable Languages

    <<<(annotations.sam)
    index::type, term
        concept, extension
        concept, constraint

    The languages we have looked at to this point are publicly specified and have existing tool chains. Some are more constrained than others, and some support different structured writing algorithms and different ways of partitioning and redirecting complexity. Choosing one of them makes sense if the constraints they express and the algorithms they support partition content complexity in a way that is right for your organization. If not, you need to create your own structures to improve how complexity is partitioned and distributed in your organization.

    You have three options for doing this:
    
    * Create your own language entirely from scratch, creating both the syntax and the semantics. (This is what {John Gruber}(person "Gruber, John") did when he created {Markdown}.)
    
    * Use an existing abstract markup syntax, such as XML or SAM, and create your own semantics by defining named structures using that syntax (as described in [#chapter.markup]).
    
    * Take an existing markup language with extensible and/or constrainable semantics, such as DITA or DocBook, and extend and/or constrain it to meet your needs. 
    
    Each of these approaches has merits and drawbacks. For instance, creating a new language may enable you to achieve exceptional {functional lucidity} for a particular type of information, extending/constraining an existing language may save tool development costs, or defining your own semantics based on an existing syntax may enable you to find the right balance between functional lucidity and development costs. 

    This chapter looks at extensible and constrainable markup languages.     

    section: XML
    
        The X in {XML} stands for eXtensible, but, as noted in [#chapter.markup], XML is an abstract language that does not define any document structures itself. Therefore, with XML, you start from zero. Syntactically, everything is defined for you. Semantically you start from scratch. 
        
        You can define the structure of a new XML markup language using one of several available {schema languages}. I look at schema languages in [#chapter.constraints], but the mechanics of defining a markup language in XML are out of scope for this book. 


    section: ReStructuredText    
    
        index:: type, term
            language, reStructuredText
    
        ReStructuredText defines a base set of structures -- such as paragraphs, titles, and lists -- with a concrete syntax. It defines other blocks using directives. [*fig.restructuredtext-1] shows a directive that includes an image and a set of attributes for rendering that image. 

        figure:(*fig.restructuredtext-1) reStructuredText directive
            ```(reStructuredText)
                .. image:: images/biohazard.png
                   :height: 100
                   :width: 200
                   :scale: 50
                   :alt: alternate text
        
        You extend reStructuredText by adding new directives. However, there is no {schema} language for reStructuredText. To create a new directive, you must create code to process it. 
        
        There is an important distinction between languages that are extensible by schema and those that are extensible only by writing code to process the extension.

        If a language is extended by writing processing code, the only way to know if the input is valid is by processing it. If it raises a processing error, it is invalid. If you have only one processor for a language, you can treat that processor as normative. That is, the definition of a correct file is any file that can be successfully processed by the normative processor. The language is defined by the processor. But if you have multiple processors, how do you determine who is at fault when one of those processors fails to process a given input file? Is the processor incorrect or the source file?
        
        A schema creates a language definition that is independent of any processor. That is, it partitions and redirects the complexity of validation. The schema is normative, not any of the processors. If the source file is valid per the schema, the processor is at fault if it does not process that file correctly. If the source file is not valid per the schema, the blame lies with the source file. 
        
        In the case of reStructuredText, the capacity of the processor to be extended using directives is built into the processor architecture. There is a specific and well-documented way to extend the language. But although reStructuredText allows you to extend the language by adding new directives, it does not have a constraint mechanism. There is no mechanism (other than hacking the code) to restrict the use of existing or new directives and structures. 

    section: TeX
    
        index:: type, term
            language, TeX
            
        TeX (pronounced "Tek") is a typesetting system invented by {Donald Knuth} in 1978. As a typesetting language it is a concrete {media-domain} language. But Knuth also included a macro language in TeX, which allows users to define new commands in terms of existing commands. (I say commands because that is the term used in TeX. Markup in the media domain tends to be much more imperative than markup in the subject domain, which is entirely descriptive, so "commands" is an appropriate name for TeX's tags.) This macro language has been used to extend TeX, most notably in the form of {LaTeX}, a {document-domain} language that I described in [#chapter.lightweight].
        
        As noted with {reStructuredText}, the ability to extend is not the same thing as the ability to constrain. Introducing new commands does not create a constraint mechanism.
        
    section: SAM
    
        index:: type, term
            language, SAM
    
        Although lightweight languages provide great {functional lucidity}, they suffer from limited extensibility (which generally requires writing code) and a lack of constraint mechanisms. I believe that a fully extensible, fully constrainable lightweight markup language would be a valuable addition to the structured writing toolkit. This is why I developed {SAM}, the markup language used for most of the examples in this book and for writing the book itself. 
        
        As described in [#chapter.markup], SAM is a hybrid markup language that combines implicit syntax similar to {Markdown} with an explicit syntax for defining abstract structures called blocks, record sets, and annotations. It also has concrete markup for common features such as insertions, citations, and variable definitions. 
        
        SAM, like {XML}, is for defining specific markup languages. However, unlike XML, all languages defined in SAM share a small common base set of text structures for which SAM provides concrete syntax. This allows SAM to combine lightweight syntax for the most common text structures with the ability to define constrained markup languages for specific purposes, particularly {subject-domain} languages. In other words, SAM represents a different partitioning of the markup design process from both the common lightweight languages and XML.
                
        SAM is designed to be extensible and constrainable through a {schema} language (this is not complete at time of writing, but hopefully will be available by the time you read this). My intent is that the schema language should be able to define and constrain new block structures, constrain the use of existing concrete structures, and constrain the values of fields using patterns.
        
        SAM is not designed to be as general as XML in its applications. As a result, its syntax is simple and more {functionally lucid}, and its schema language should also be simpler and make it much easier for writers to develop their own SAM-based markup languages. 
        
        I use SAM for the majority of the examples in this book because SAM is designed to make structure clear. All of the examples could have been expressed in XML, but that would have made them harder to follow. Naturally, to write in SAM you need to know more about the rules of the language, but you should be able to read a typical SAM document and understand its structure with little or no instruction (see [*fig.SAM-structures]).
        
        figure:(*fig.SAM-structures) Basic SAM structures
            ```(SAM)
                examples: Basic SAM structures

                    example: Paragraphs
                        The is a sample paragraph. It is inside
                        the {block}(structure) called `example`.
                        It contains two {annotations}(structure),
                        including this one. It ends with a blank
                        line.

                        This is another paragraph.

                    example: Lists

                        Then there is a list:

                        1. First item.
                        2. Second item.
                        3. Third item.

                    example: Block quote

                        Next is a block quote with a {citation}(structure).

                        """[Mother Goose]
                            Humpty Dumpty sat on a wall.
                        
        This objective is similar, but not identical, to the aim of mainstream concrete and hybrid languages such as {Markdown}(language) and {reStructuredText}(language), which is to have the source file be readable as a document. Those languages are document-domain languages, and they strive to make the document structure clear and readable from the markup. SAM has the same goal, except that SAM was designed primarily for creating {subject-domain} languages. As such, SAM is designed to make the subject-domain structure of a document clear to the reader. 
        
        A SAM document may not look as much like a finished document as a {Markdown}(language) or {reStructuredText}(language) document. For example, it does not use underlines to visually denote different levels of header. Instead, it focuses on creating a hierarchy of named blocks and fields. In doing so, it uses the kind of markup people commonly use to create named blocks of text and to express a hierarchical relationship between them. As you can see in [*fig.SAM-structures], blocks are introduced with a name followed by a colon, and hierarchy is expressed through indentation. 

        SAM is an open source project. A description of the language and a set of associated tools are available from {https://github.com/mbakeranalecta/sam}(url).

    section: DocBook
    
        index:: type, term
            language, DocBook
    
        {DocBook} is not really extensible in the same sense as the other languages mentioned here, but it still deserves a mention. What it does provide is a deliberately modular construction that makes it easy to create new {schemas} that include elements from DocBook. DocBook takes full advantage of the extensibility features built into XML schema languages. 
               
        DocBook has a large tag set, so if you want a small constrained {document-domain} markup language, you can often create one by sub-setting DocBook. DocBook provides just about any document structure out there, so if you are building a document-domain language, chances DocBook has the pieces you need. You can even add additional constraints.
        
        The great advantage of creating a new language as a subset of DocBook is that the result is also a valid DocBook document and can therefore be published by the DocBook tool chain. You don't have to write any new publishing algorithms if you take this approach (other than to customize formatting to your needs, which is required with any system). Creating a subset of DocBook allows you to impose more constraints and improve {functional lucidity} significantly compared to standard DocBook without having to write any processing code at all.
        
        Technically speaking, any {XML}-based markup language is extensible in the same way. However, DocBook's structure, and the implementation of its schemas, was designed to support both extension and sub-setting, something which is not true for many markup languages.
        
    section: DITA
    
        index:: type, term
            language, DITA
    
        {DITA}'s approach to extension is unique among markup languages. In fact, it is something of a misnomer to call DITA a markup language. The DITA standard calls it an information typing architecture. What is an information typing architecture?  
        
        The conventional way to define a document type is to use a {schema language}. That is how XML and SAM do it. Schema languages describe constraints on markup structures. (I look at schema languages in [#chapter.constraints]). So what does an information typing architecture provide over and above what a schema language provides?

        An architecture is a set of principles for addressing a certain kind of problem. For instance, there are several different architectures for building bridges, including the beam bridge, the truss bridge, the arch bridge, and the suspension bridge. As [#figure.bridgearchitectures] shows, we can recognize the same basic architecture in hundreds of different bridges of each type. 

        figure:(#figure.bridgearchitectures)
            caption:
                A comparison of different bridge architectures. Based on: 
                By Themightyquill - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=24825433
                By Themightyquill - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=24825439
                By Themightyquill - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=24825432
                By Themightyquill - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=24825437
            >>>(image ../graphics/bridges.xml)

        An architecture, by itself, is just a set of ideas. The virtue of an architecture is that it puts several elements together in a way that can be tested and that can support the development of tools and/or prefabricated components than can be used to build specific bridges. If you decide to build an arch bridge, therefore, there are lots of architectural tools, practices, and data that you can draw on to make the design and construction of your bridge faster and more reliable.

        Architectures are a way of partitioning the design problem for a structure. They divide the design space into pieces and provide design and construction guidance within each piece. It is not a case of a single architecture addressing all cases. The suspension bridge is a great solution for crossing some gaps, but a beam or truss bridge is a much simpler and more economical architecture for most common gaps. An architecture does not tell you what type of bridge to build; it helps you build a particular type of bridge once you have determined which type is appropriate. 

        At the heart of each architecture you will usually find one fairly simple idea or principle. The arch bridge architecture depends on the strength of the arch shape to support the load. The truss bridge depends of the rigidity of the triangle shape to provide strength, the suspension bridge architecture uses cables to transfer weight to towers. At the heart of the DITA architecture is a similarly simple idea, which we might call the block and map architecture. That is, DITA treats information products are being constructed of blocks of information (which it calls "topics") according to a hierarchical map. The {Assemble from Pieces} {reuse algorithm} that I discussed in [#chapter.reuse] is an example of a block and map architecture. (But note that DITA is not the only block and map architecture, and maps are not the only way to assemble blocks.) 

        figure:(#block-and-map) A block and map architecture use to perform reuse.
            >>>(image ../graphics/assemble.xml)


        An architecture does not solve the entire design problem for a bridge or for a content system. It provides a starting point, both for design and for tools and implementation, but there is still design, build, and integration work to do to create a bridge or a system to address a specific need. As I noted in describing out-of-the-box DITA, even out-of-the-box systems require some customization to make them fit your needs. When you choose an out-of-the-box system, therefore, you should choose the one that comes closest to meeting your needs, so that you can live with the fewest limitations and do the least customization. 

        If you choose an architecture as the basis for building a custom system, you will have more work to do, and will be able to eliminate many more limitations (partition your system to more closely meet your needs), but the principle remains the same: you start with the architecture that comes closest to meeting your needs. Choosing an existing architecture should mean you have to do less work that if you designed and built from scratch, but this is only true is the architecture is actually a good fit for your organization. If not, trying to adapt the architecture to meet a need it was not designed for can be more work than designing and building something from scratch, or from lower level components.

        Just as every out-of-the-box tool has passionate advocates who will tell you it is right for every problem, so too that are passionate advocates of architectures like DITA who will tell you that it is right for all content system. But the very nature of an architecture is that make it easier to address a particular kind of problem by implementing a particular kind of solution. A universal architecture would have no content. Architectures get their usefulness by being specific and limited, not by being general or all-encompassing.

        While DITA is almost the only thing in the content space that explicitly calls itself an architecture, there are, in fact, other content architectures. FrameMaker, for instance, is based on an architecture that builds a document out of a set of nested frames. Wikis are based on what we can call the wiki architecture, which is an architecture of pages connected to each other by references contained in the pages themselves, in the form of wikiwords and categories. (In other words it is a block architecture where assembly is not based on maps.) Blog platforms implement a block-based architecture based on temporal sequence supplemented by categorization and tagging. But while all these are architecture, they have not be described and formalized as architectures outside of the tools the implement them. The only other architecture I know of that is being formalized in this way is the one I am developing myself, {SPFE}, which I will describe briefly later in this chapter. 
        
        Of course, there is more to an architecture than just its core idea. In fact, each of the bridge architectures can be divided into different sub architectures. For example, the Bailey Bridge ([#fig-Bailey]) is an sub-architecture of the truss bridge architecture. Sub architectures come with a lot more specification of details which both makes them easier to implement and further limits their range of applicability. 

        figure:(#fig-Bailey) The Bailey bridge architecture
            caption: By Fmiser (Own work) [CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons
            >>>(image ../graphics/Bailey-truss.xml)

        As suggested above, DITA is a sub-architecture of the more general block and map architecture, or which there are several other examples. As such it comes with a lot of specific design and implementation detail. As with the Bailey Bridge, this additional specification serves both to make it easier to implement in applicable cases and more limited in its scope of applicability. Of course, this added ease of implementation only applies if your use it within its scope of applicability. If you try to build something outside of that scope, you may well succeed (architectures are, in the end, just a set of ideas), but the difficulty of implementation will rise the further you depart from the sweet spot of that particular architecture. 

        {DITA} has grown into a large and complex architecture over the years, and it is out of scope for this book to describe its architecture in full.[*1] Instead, I attempt to map key features of the DITA architecture to the structured writing concepts explored in this book.  

        footnote:(*1)
            Obviously I am not a fan of the DITA architecture or I would not be developing my own architecture in {SPFE}. For a much fuller and more sympathetic treatment of the DITA architecture, see {DITA for Practitioners Volume 1: Architecture and Technology}(citetitle) by Elliot Kimber, {http://xmlpress.net/publications/dita/practitioners-1/}(url)

        section: Basics of the DITA architecture
        
            The fundamental block and map architecture of DITA consists of these pieces:

            1. A collection of schemas for some basic block types ("topics" in DITA parlance).
            2. A specification for a map mechanism for assembling blocks into information products.
            3. A specification for a specialization mechanism for creating specialized types of blocks from the base types, and a parallel mechanism for specializing the processing code for the base blocks to process the specialized blocks. 

            There is a great deal more in the current DITA specification, but those elements are the core of the architecture. 

            Information typing in DITA, therefore, consists of creating specialized versions of the base information types using the specialization mechanism (described below). We have already looked at several general ways to do information typing. DITA's information typing ability is not unique therefore. What distinguishes it is the particular properties of specialization as a tool for information typing. Like any feature of an architecture, specialization is designed to make a particular class of information typing easier. 

            This approach is to provide base topic types that contain thought-out and tested information type designs. This means you are not starting information typing from scratch. When you specialize from an existing DITA type, you don't have to do the all the design work from scratch, you only have to design the parts that are specific to your specialization. 

            The limitation is that DITA's base types are not universal. They are a specific type of information type designed to work with a specific architecture. Some applications of XML, such as recording transfers between banks or storing the configuration options of an editor, are not instances of DITA topics. DITA lets you design a range of information types more easily, but limits the range of information types you can create. 

            This is the proper role of any architecture. It gives you a head start in its own domain, but it is only appropriate for use in that domain, and only if the choices baked into the architecture are the choices that make sense for your application. These limitations are reflected in the way DITA defines information typing:

            pagination-tweak:
                min-space: 1.5in
            
            The DITA specification defines information typing as follows:  
        
            """[Darwin Information Typing Architecture (DITA) Version 1.3 Part 3: All-Inclusive Edition]
                Information typing is the practice of identifying types of topics, such as concept, reference, and task, to clearly distinguish between different types of information.[*XX]
            
            footnote:(*XX)
                {http://docs.oasis-open.org/dita/dita/v1.3/csd01/part3-all-inclusive/dita-v1.3-csd01-part3-all-inclusive.html#information-typing}(url)

            Unfortunately this definition is largely circular -- information typing defines information types. But it does help establish a scale. Information typing is about defining a unit of information called a {topic}.

            pagination-tweak:
                min-space: 1.5in
            
            The specification goes on to define the purpose of information typing as follows:
        
            """
                Information typing is a practice designed to keep documentation focused and modular, thus making it clearer to readers, easier to search and navigate, and more suitable for reuse.

            DITA, then, is about information typing at a particular scale -- the topic, and it designed to support particular algorithms such as reuse. DITA information typing is not as general as structured writing. That does not make it impossible to work at other scales or implement other algorithms in DITA, it just means that the architecture provides better support for certain areas.
        
            Out-of-the-box DITA is commonly associated with the idea that there are just three information types: task, concept, and reference. The DITA specification makes it clear that this is not the intention of DITA as an information typing architecture. 
        
            """
                DITA currently defines a small set of well-established information types that reflects common practices in certain business domains, for example, technical communication and instruction and assessment. However, the set of possible information types is unbounded. Through the mechanism of specialization, new information types can be defined as specializations of the base topic type (<topic>) or as refinements of existing topics types, for example, <concept>, <task>, <reference>, or <learningContent>.

            As I have noted, many structured writing algorithms partition complexity best when given more specific markup, particularly markup in the subject domain. The ability to create other information types is, therefore, relevant to getting the most out of structured writing. 
        
        section: Specialization

            What is DITA {specialization}? XML syntax defines abstract structures that do not occur in documents: elements, attributes, etc. To create a markup language in XML, you define named elements and attributes for the structures you are creating. This is a type of specialization.

            For example, in {DocBook}, `para` is a type of element. `para` has what is called an "is-a" relationship to elements: `para` is an element, but it is a special type of element. An XML parser will process a `para` generically as an element, reporting its name to another algorithm whose job is to interpret and act on that structure. This algorithm must then supply a rule that processes just this specialized `para` element (and not the also specialized but different `title` element). All of the algorithms in this book act on structures reported to them by a parser.
        
            DITA specialization follows the same principle, but moves it up a level. The base `topic` topic type is the abstract structure. You can create a more specific type, such as `knitting-pattern` or `ingredients-list`, as a specialization of `topic` (or of other topic types that are themselves specializations of `topic`).  Each of these specialized types has an is-a relationship with the type it was specialized from. So `knitting-pattern` is a `topic`.
        
            DITA specialization differs from simply creating new named elements in XML in that the base DITA topic type is not an abstraction like an XML element. You cannot create an XML document without inventing a set of elements and giving them names. The base DITA topic type, on the other hand, is a fully implemented topic type that you can use directly. You can, and people do, write directly in the base topic type without inventing anything new. I noted in [#chapter.rhetorical_structure] that it is sometimes easy to treat what is intended as a meta model as a generic model. This is the case here. All topic types in DITA are derived by specialization from the generic `topic` type. They all have an is-a relationship to this generic type. 
       
            One consequence of this is that a processor can successfully process a specialized element as though it were the base element. The result may not reflect all aspects of the specialized element, but the processing will not fail. This is an attractive quality because it is often easier to modify an existing piece of code than it is to write new code from scratch, particularly if most of the rules in the base code can remain unchanged. 

            To specialize a topic type, you specialize the root element and any child elements or attributes that you need. Each specialized element or attribute should have an is-a relationship to the element it specializes. Thus a procedure element might be a specialization of an ordered-list element and its step elements might be specializations of a list-item element. In this case, processing a procedure as an ordered list would produce meaningful output: a conventionally labeled numbered list. However, you would probably want to specialize the output of steps in a procedure, perhaps by prefixing each step with "Step 1:" rather than just "1." Modifying the ordered-list code to handle procedures would probably take less code than writing a procedure processing algorithm from scratch.
               
            The second way in which DITA specialization differs from giving names to abstract elements is that specialization is recursive. For example, suppose you have created a topic type called `animal-description`, which is a specialization of `topic`. You want to impose additional constraints on the description of certain types of animal, so you create `fish-description` and `mammal-description`, which are specializations of `animal-description` (and which would be processed like an `animal-description` if no other processing is specified for them).

            Then you might decide to impose still more constraints to describe different kinds of mammals, so you create the type `horse-description`, which is a specialization of `mammal-description`. This type would be processed as a `mammal-description` if no specific processing is provided for `horse-description`; as `animal-description` if no specific `mammal-description` processing is provided; and as `topic` if no specific `animal-description` processing is provided. 
        
            The third way in which information typing in DITA differs from creating a language from scratch is that DITA information types share a common approach to processing and to information architecture. In particular, they inherit a common set of {management-domain} structures and their associated management semantics. 

            A DITA topic, then, comes with a lot of built-in functionality and structure that you don't have to reinvent when you create a specialized topic type. But the corollary is that not all content types are specialized instances of a DITA generic topic. This means that while the set of information types that you can create using specialization is unbounded in number, it is not unbounded in type. There are types you can't create by specialization, at least if you want to maintain the is-a relation to the base type (without which specialization isn't specialization at all). 

            When we factored the ingredients of a recipe out of a {document-domain} list into a {subject-domain} record structure it was, in part, to make them independent of the decision to format them as a {table} or as a {list}. 

            figure:(*fig.ingredients-list) Subject-domain ingredients list
                ```(sam)
                    ingredients:: ingredient, quantity, unit
                        eggs, 3, each
                        salt, 1, tsp
                        butter, .5, cup 

            The ingredient record set in [*fig.ingredients-list] no longer has an is-a relationship to a {table} or a {list}. The point was to break that relationship so you could present this content any way you wanted to. Because any structure specialized from a DITA topic should maintain an is-a relationship to a DITA topic, which is a generic document-domain structure, you can use specialization to create subject-domain structures that intersect with the document domain, such as the `introduction`, `ingredients`, and `preparation` sections in a `recipe` document. However, you can't take the next step of factoring out the presentation, as I've done in [*fig.ingredients-list], since factoring out presentation breaks the is-a relationship with the document domain.  

            In other words, the specialization mechanism lets you create any element name you like as a specialization of any other element name you like. As such, it can create any structure you like. But the entire justification of the specialization mechanism rests on the maintenance of the is-a relationship between the specialized element and the base element. Once this is broken, code inheritance and fallback processing no longer work, and specialization simply becomes an unnecessarily complex and error prone way of writing a new and unrelated schema.
    
            Labels are a big part of document-domain content. When we created a subject-domain structure for recording nutritional information for a recipe, we factored out all of the labels by putting the content in named fields. In other words, as shown in [*fig.nutrition], the labels went from being data (in the content) to {metadata} (part of the structure).

            figure:(*fig.nutrition) Subject-domain structure for nutritional information
                ```(sam)
                    nutrition:
                        serving: 1 large (50 g)
                        calories: 78
                        total-fat: 5 g
                        saturated-fat: 0.7 g
                        polyunsaturated-fat: 0.7 g    
                        monounsaturated-fat: 2 g    
                        cholesterol: 186.5 mg    
                        sodium: 62 mg    
                        potassium: 63 mg    
                        total-carbohydrate: 0.6 g    
                        dietary-fiber: 0 g    
                        sugar: 0.6 g    
                        protein: 6 g    

            Although the structure in [*fig.nutrition] looks superficially like a list, it is really a data record. If this structure were created as a specialization of a list and then published using a generic list publishing algorithm, the result would look like [*fig.nutrition-list].

            figure:(*fig.nutrition-list) Generic rendering of the subject-domain structure in [*fig.nutrition]
                """
                    * 1 large (50 g)
                    * 78
                    * 5 g
                    * 0.7 g
                    * 0.7 g    
                    * 2 g    
                    * 186.5 mg    
                    * 62 mg    
                    * 63 mg    
                    * 0.6 g    
                    * 0 g    
                    * 0.6 g    
                    * 6 g    

            In short, there is no is-a relationship between a pure {subject-domain} structure and a generic {document-domain} structure. The subject-domain structures are simply data fields with no presumption about presentation attached to them. 

            Does this mean that you cannot create a {subject-domain} structure using DITA's {specialization} mechanism? No, you can usually create the structure you want, and in a way that meets the syntax requirements of a DITA specialization. However, it won't be an actual {specialization} of its base type. Because it is a {subject-domain} structure it breaks the is-a relationship with the {document domain}. As I noted at the end of [#chapter.linking], {subject-domain} algorithms work completely differently from those of the {document domain}, so inheriting some of the processing of a base {document-domain} structure is moot once you move to the {subject domain}. Your new {subject-domain} structure will be an essentially unrelated structure for which you will have to create completely new processing algorithms, just as you would if you had defined your structure from scratch.
        
            Generally, the fewer pieces of an architecture you use, the less value there is to basing your work on that architecture, both because you have more work to do and because you take less advantage of the infrastructure, tools, and expertise surrounding that architecture. If you get too far away from the intention of architecture, you will create a system that is less understandable to people versed in the architecture. All this betrays a poor fit between the system partitioning you need and the partitioning the architecture provides. All architectures come with overhead, and even if you don't use their features, you have to live with the overhead, which adds cost and complexity to your system. Thus, while you can use DITA and depart from the default DITA way of doing things, the value of using DITA diminishes the further you depart from the DITA way. The same is true of any information typing architecture, just as it is true of and content development system.

        section: Limits on rhetorical constraint
        
            Throughout this book I have stressed the value of constraining rhetoric, not only to improve rhetoric itself, but also to improve process. One of the limits imposed by DITA's topic and map architecture is that it limits the size of unit to which you can apply rhetorical constraints.

            For instance, if you regard a recipe as being made up of one concept, one reference, and one task topic, then DITA will let you constrain the rhetoric of a specialized ingredient list reference topic, or a preparation task topic, but not of the recipe as a whole. DITA will let you write a map to combine a concept topic containing an introduction, a reference topic containing a list of ingredients, and a task topic containing preparation instructions. But it does not let you specify that a recipe topic consists of one concept topic, one reference topic, and one task topic in a particular order. In other words, DITA does not provide any direct way to define larger types or the overall rhetorical structure of documents.[*fn.specialize_map]
            
            footnote:(*fn.specialize_map)

                Actually, it might be technically possible to create a map that was constrained in this way through specialization. That is, you could create a recipe map that is allowed to contain exactly one recipe-intro topic, one ingredients-list topic, and one preparation topic in that order. But there is no higher level way of specifying this kind of constrained map and the {functional lucidity} of such an approach is clearly low. But the real point here is not the technical limitation but the limits of the block and map architecture itself to support effective rhetorical constraint. 
            
            You can define a recipe topic type in DITA, but to do so you need to adopt a different view about how atomic a DITA topic is. Some DITA practitioners might say that a recipe is not a map made up of three information types, but a single task topic. In this view, a task topic is much more than what {Information Mapping} would call a procedure. It allows for the introduction of a task, a list of requirements, and the procedure steps all within the definition of a single topic. However, this approach abandons DITA's principle of segregating information types into separate files.[*fn.specialize-recipe]

            footnote:(*fn.specialize-recipe)
                I have asked a number of DITA practitioners how a recipe should be modeled in DITA and have received each answer from multiple people. 
            
            One of the reasons for this uncertainty about how to define an atomic topic in DITA is DITA's focus on {content reuse}. DITA topics are not only units of information typing, they are units of reuse. Making a recipe a single topic leaves you with fewer, larger units of content, which makes individual topics harder to reuse. The atomic unit of content that is small enough to maximize potential reuse is much smaller than the atomic unit of  content that contains a complete {rhetorical pattern}. The atomic unit of reuse is smaller than the atomic unit of use.
            
            Because DITA has no direct mechanism for describing models larger than a topic, a DITA practitioner must choose between modeling for maximum reuse and modeling to constrain a topic type to rhetorical structure. In practice, DITA users make different decisions about how atomic their topic types should be based on their business needs. 

    section: SPFE
    
        index:: type, term
            tool, SPFE
    
        {SPFE} is another project of mine. It is an architecture for implementing structured writing algorithms. Its structure follows the model I laid out in [#chapter.publishing]. As I noted there, every structured writing publishing chain is something like this, since every publishing chain has to get content from the {subject domain or the {document domain} into the {media domain}. But how that processing is partitioned is key to which structured writing algorithms you can implement in the publishing process and how cleanly you can insert them. SPFE specifies a division of responsibilities in a publishing tool chain that is intended to allow for the efficient partitioning of the content process and of the development of structured writing algorithms. 

        However, that is a little too general a description. As I said, no architecture can be optimized for all cases. The SPFE architecture, just like the DITA architecture, is optimized to favor certain algorithms and certain information designs. For instance, DITA is optimized for the {reuse algorithm} and {hierarchical information architectures}, while SPFE is optimized for the {linking algorithm} and a {hypertext} or {bottom-up information architecture}. This does not mean you can't do reuse in SPFE or linking in DITA. It simply means that these things are at the heart of the design of each. Architectures are just a starting point for building systems. They don't determine what the final capabilities of any system will be, just how easy it will be to create them. 

        SPFE and DITA are also optimized for working in different structured writing domains. DITA is based in the {document domain}, since its base topic types are document domain types, and like all document domain systems, it makes heavy use of management domain structures. SPFE does not include any base types in the architecture, so it is not tied to one domain in the same way DITA is, but it is intended mainly to support the {subject domain}. (The subject domain requires an additional processing stage compared to document domain systems and the SPFE architecture support that additional stage.)

        Another big contrast between DITA and SPFE is that DITA is a {block and map architecture}, whereas SPFE is designed to support automatic collection and linking of content based on metadata, meaning that no maps are required. Of course, this means that information architects or content engineers have to write the algorithms to do the desired collection and linking for a specific information set, as opposed to using a generic mechanism as in DITA. The SPFE architecture is designed to support this work. 

        One of the features of the {subject domain} is that it allows you to greatly reduce the amount of {management domain} features you need. Generating information architecture from metadata rather than specifying it with maps, means writers don't need to know how to link or assemble the pieces they write. Together, these features mean that authors don't need to know much if anything about the publishing system. While writers working in {DITA} typically require extensive DITA training, individual writers working in a {subject-domain} SPFE system should have to know little or nothing about how SPFE works, as long as they follow the constraints of the markup language they are using.

        We have noted that the DITA architecture involves some ambiguity about whether a {DITA} topic is a {rhetorical block} or merely a {semantic block}, and that one of the limits of DITA is that it cannot constrain any unit larger than a topic. SPFE places no inherent constrain on the size of units processed by a SPFE system, so it does not place limits on the rhetorical constraints you can apply. (At the same time, we should note that the real source of the problem for DITA is the nature of the block and map architecture itself. These issues will occur for any system, including a SPFE system, if it tries to implement block and map reuse below the level of a rhetorical block. As I have noted, however, there are forms of reuse, particularly in the subject domain, that avoid these issues, though they are not as general as block and map reuse in the document domain.)
        
        SPFE does not define a base set of content structures the way DITA does, and it does not develop information types by specialization the way DITA does. In SPFE, you can do information typing from scratch, as long as you follow certain guidelines. But SPFE also supports a higher level of information typing that uses a technique call composition.

        As with {DITA} {specialization}, SPFE composition includes both content structures and the code that processes them. But rather than creating one topic type from another by specialization, SPFE lets you assemble an information type, of any scale, by assembling existing definitions of {semantic blocks} and their associated algorithms, like building a model out of Lego blocks. Because a semantic block can also serve as a {structural block} in assembling a larger semantic block, this process also works iteratively. The result is that you can take advantage of existing design work and coding in the creation of your information types without the restriction on size and type that come with DITA specialization. 

        To create a {subject-domain} {markup language} in {SPFE}, therefore, all you have to define for yourself are the key subject-domain fields and blocks that are essential to your business. All the other {semantic blocks} you need, such as paragraphs, lists, tables, and common annotations, you can include from the pre-built components, along with their default processing code. 

        By strictly segregating the presentation and formatting layers, SPFE reduces the effort required to process custom markup formats. Custom format are processed to a common {document-domain} markup language which is then processed to all required {media-domain} output formats. The {SPFE Open Tool Kit} includes a basic {document-domain} language for this purpose, but you can also use {DocBook} or {DITA} in this role, allowing you to take advantage of their existing publishing capabilities. This also allows you to install SPFE as an authoring layer on top of an existing DITA or DocBook tool chain.      

        I developed SPFE because I am not a fan of the block and map architecture, of which DITA is simply the most widely known example today. Block and map architectures provide good support for ad hoc {reuse}, but at the expense of poor support for many other structured writing algorithms, for the {subject domain}, and for {functional lucidity}. I believe that many organizations would befit from a different partitioning of their content systems supported by a different architecture. But picking a architecture is all about meeting your business goals efficiently and if extensive ad hoc reuse is the overwhelming driver of your content system redesign, then a block and map architecture like DITA (but not just DITA) is something you should look at.   
        
        Both {SAM} and {XML} are supported as markup syntax for {SPFE}, and you can freely mix and match SAM and XML content. 
        
        SPFE is an open source project available from {https://github.com/mbakeranalecta/spfe-open-toolkit}(url).

     


