!smart-quotes: on
chapter:(#chapter.extensible) Extensible and Constrainable Languages

    <<<(annotations.sam)
    index::type, term
        concept, extension
        concept, constraint

    The languages we have looked at to this point are publicly specified and have existing tool chains. Some are more constrained than others, and some support different structured writing algorithms and different ways of partitioning and redirecting complexity. Choosing one of them makes sense if the constraints they express and the algorithms they support partition content complexity in a way that is right for your organization. If not, you need to create your own structures to improve how complexity is partitioned and distributed in your organization.

    You have three options for doing this:
    
    * Create your own language entirely from scratch, creating both the syntax and the semantics. (This is what {John Gruber}(person) did when he created {MarkDown}.)
    
    * Use an existing abstract markup syntax, such as XML or SAM, and create your own semantics by defining named structures using that syntax as described in [#chapter.markup].
    
    * Take an existing markup language with extensible and/or constrainable semantics, such as DITA or DocBook, and extend and/or constrain it to meet your needs. 
    
    Each of these approaches has merits and drawbacks. For instance, creating a new language may enable you to achieve exceptional {functional lucidity} for a particular type of information; extending/constraining an existing language may save you a lot of tool development costs; while defining your own semantics based on an existing syntax may enable you to find the right balance between functional lucidity and development costs. 

    This chapter looks at extensible and constrainable markup languages.     

    section: XML
    
        The X in {XML} stands for eXtensible, but, as noted in [#chapter.markup], XML is an abstract language that does not define any document structures itself. Therefore, with XML, you start from zero. Syntactically, everything is defined for you. Semantically you start from scratch. 
        
        You can define the structure of a new XML markup language using one of the several available {schema languages}. We will look at schema languages in [#chapter.constraints], but the mechanics of defining a markup language in XML are out of scope for this book. 

    section: DITA
    
        index:: type, term
            language, DITA
    
        {DITA} is unique among markup languages because it was designed for extension from the beginning. In fact, it is something of a misnomer to call DITA a markup language. DITA actually calls itself an information typing architecture. What is an information typing architecture? DITA is really the only thing that calls itself by this name, so we have to derive the definition from the properties of this one example. 
        
        The conventional way to define a document type is using a {schema language}. Schema languages are simply languages for describing constraints on markup structures. (We will look at them in [#chapter.constraints]). So what does an information typing architecture provide over and above what a schema language provides?
        
        The programming world provides one useful example. Programming environments use architectures and frameworks to abstract certain types of operations to a higher level. For example, the environment may implement commonly used functions that you could program from scratch, but having those functions available moves complexity from the local developer to the algorithms supplied in the architecture, saving time and possibly avoiding errors. 

        The usefulness of an architecture depends on whether it partitions and distributes the problem space in substantially the same way you would have done it if you were designing the system from scratch. (Of course, you might not have come up with all the good ideas in a good architecture yourself; the point is that once you have been exposed to those ideas and understand them, would you then partition and distribute complexity according to those ideas or not?)

        So the question to ask about DITA is, does its architecture partition and distribute content complexity the way that you would naturally wish it to be done to achieve the best outcome for your organization? 
        
        The DITA architecture consists of a set of predefined markup languages that you can extend using a mechanism called {specialization}, a facility for assembling information products (maps), a facility for managing metadata, an extensive set of {management-domain} markup, and specifications for the behaviors associated with all of these capabilities.
        
        In other words, it predefines a range of structures, semantics, and operations that you might need in establishing an {information architecture} and then provides a way for you to build from there. 
        
        As with any other architecture, DITA's usefulness depends on how well the predefined structures, semantics, and operations suit your needs; how easy the extension mechanism is to use; and how reliable the available implementations are. 
        
        It is common in the software world for there to be many competing architectures with different sweet spots. Because an architecture is essentially a series of guesses about what a variety of systems may have in common, different architectures may be constructed differently to cover different sets of commonalities among diverse projects, and you may not see equivalent architectural features from one system to another. 
        
        There are not a lot of information typing architectures. The only other one I am aware of is the one I am developing myself, which is called {SPFE}. {SPFE}, however, has a very different architecture from {DITA}. {DITA} has grown into a very large and complex architecture over the years, and it is out of scope for this book to describe its architecture in full.[*1] All I shall attempt to do here is to do a basic mapping of some key features of the DITA architecture to the structured writing concepts explored in this book.  
        
        footnote:(*1)
            Obviously I am not a fan of the DITA architecture or I would not be developing my own architecture in {SPFE}. For a much fuller and more sympathetic treatment of the DITA architecture, see {DITA for Practitioners Volume 1: Architecture and Technology}(citetitle) by Elliot Kimber, {http://xmlpress.net/publications/dita/practitioners-1/}(url)

        Inherent in the process of constructing an architecture is that you partition the field in certain ways. Architectures move functionality to a higher level by choosing some options and rejecting others. An {XML schema} is an information typing language that can define a markup language for any purpose at all. However, some applications of XML, such as recording transfers between banks or storing the configuration options of an editor, are not within the scope of the information typing that the DITA architecture was designed for. Therefore, DITA has a more restricted definition of information typing. The DITA specification defines information typing this way:    
        
        """[http://docs.oasis-open.org/dita/dita/v1.3/csd01/part3-all-inclusive/archSpec/base/information-typing.html]
            Information typing is the practice of identifying types of topics, such as concept, reference, and task, to clearly distinguish between different types of information.
            
        Unfortunately this definition is largely circular -- information typing defines information types. But it does help establish a scale. Information typing is about defining {topic types}. The spec goes on to define the purpose of information typing:
        
        """
            Information typing is a practice designed to keep documentation focused and modular, thus making it clearer to readers, easier to search and navigate, and more suitable for reuse.
        
        DITA information typing is not as general as structured writing. It focuses on information at a particular scale and on a subset of the structured writing algorithms. That does not make it impossible to work at other scales or implement other algorithms, it just means that the architecture provides better support for certain areas.
        
        Out-of-the-box DITA is commonly associated with the idea that there are just three information types, task, concept, and reference. The DITA spec makes it clear that this is not the intention of DITA as an information typing architecture. 
        
        """
            DITA currently defines a small set of well-established information types that reflects common practices in certain business domains, for example, technical communication and instruction and assessment. However, the set of possible information types is unbounded. Through the mechanism of specialization, new information types can be defined as specializations of the base topic type (<topic>) or as refinements of existing topics types, for example, <concept>, <task>, <reference>, or <learningContent>.

        As I have noted, many structured writing algorithms partition complexity best with more specific markup, particularly markup in the subject domain. The ability to create an other information types is therefore relevant to getting the most out of structured writing. 
        
        Clearly, though, one does not need an information typing architecture to define an information type. You can, as {John Gruber} did with {MarkDown}, sit down and sketch out a set of structures and a syntax to represent them, and then write a program to process them. With an {abstract language} like {XML} or {SAM}, you can create a new information type by defining a set of named elements and attributes using a schema language. How does using a higher level information typing architecture like DITA change this process? How does it partition the design problem differently?
        
        First and foremost, it means that you don't start from scratch. All topic types in DITA are derived from a base topic type called `topic` through specialization. 
        
        What is specialization? XML syntax defines abstract structures that do not occur in documents: elements, attributes, etc. To create a markup language in XML, you define named elements and attributes for the structures you are creating. This is a type of specialization. For example, in {DocBook} `para` is a type of element. `para` has what is called an "is-a" relationship to elements: `para` "is-an" element, but it is a special type of element. An XML parser will process a `para` generically as an element, reporting its name to the application layer. The application layer must then supply a rule that processes just this specialized `para` element (and not the also specialized but different `title` element).
        
        DITA specialization follows the same principle, but moves it up a level. The base `topic` topic type is the abstract structure. More specific types like `knitting-pattern` or `ingredients-list` are specializations of `topic` (or of other topic types that are specializations of `topic`). A generic DITA processor can process them as a instance of `topic`, but it requires additional code to process them specifically as `knitting-pattern` or `ingredient-list`. Each of these specialized types has an "is-a" relationship with the type it was specialized from. So `knitting-pattern` "is-a" topic. 
        
        DITA specialization differs from simply creating new named elements in XML in that the base DITA topic type is not an abstraction like an XML element. You cannot create an XML element without inventing a name for it. The base DITA topic type, on the other hand, is a fully implemented topic type that you can use directly. You can, and people do, write directly in the base topic type without inventing anything new. We noted in [#chapter.rhetorical_structure] that it is sometimes easy to treat what is intended as a meta model as a generic model. This is the case here. All topic types in DITA are derived by specialization from the generic `topic` type. They all have an "is-a" relationship to this generic type. 
       
        One consequence of this is that a processor can process a specialized element as though it were the base element without failure. The result may not reflect all aspects of the specialized element, but the processing will not fail. This is an attractive quality because it is often easier to modify an existing piece of code than it is to write new code from scratch, particularly if most of the rules in the base code can remain unchanged. 

        To specialize a topic type, you specialize the root element and any child elements or attributes that you need to define your new topic type. Each specialized element or attribute should have an is-a relationship to the element it specializes. Thus a procedure element might be a specialization of an ordered list element and its step elements might be specializations of a list item element. In this case, processing a procedure as an ordered list would produce meaningful output: a conventionally labeled numbered list. However, you would probably want to specialize the output of steps in a procedure, perhaps by prefixing each step with "Step 1:" rather than just "1." This would probably be less code to write than writing a procedure processing algorithms from scratch.
               
        The second way in which DITA specialization differs from giving names to abstract elements is that specialization is recursive. That is, suppose you have a topic type `animal-description`, which is a specialization of `topic`. You want to impose additional constraints on the description of different types of animal, so you create more specialized types `fish-description` and `mammal-description`, which are specializations of `animal-description` (and could be processed like an `animal-description` if no other processing were specified for them). Then you might decide to impose still more constraints on the description of different kinds of mammals, so you create the type `horse-description`, which is a specialization of `mammal-description`. This type would be processed as a `mammal-description` if no specific processing is provided for `horse-description`; as `animal-description` if no specific `mammal-description` processing is provided; and as `topic` if no specific `animal-description` processing is provided. 
        
        The third way in which information typing in DITA differs from creating a language from scratch is that DITA information types share a common approach to processing and to information architecture. In particular, they inherit a common set of {management-domain} structures and their associated management semantics. 

        A DITA topic, then, comes with a lot of built in functionality and structure that you don't have to invent for yourself when you create a specialized version of that topic type. But the corollary is that not all content types are specialized instances of a DITA generic topic, nor are all the structures they contain instances of generic document domain structures. This means that while the set of information types that you can create using specialization is unbounded in number, it is not unbounded in type. There are types you can't create by specialization, at least if you want to maintain the is-a relation to the base type (without which specialization isn't specialization at all). 

        When we factored the ingredients of a recipe out of a {document domain} list into a {subject domain} record structure it was, in part, to make them independent of the decision to format them as a {table} as a {list}. 

        ```(sam)
            ingredients:: ingredient, quantity, unit
                eggs, 3, each
                salt, 1, tsp
                butter, .5, cup 

        The ingredient record set no longer has an is-a relationship to a {table} or a {list}. The whole point was to break that relationship so that we could present that content any way we wanted to. The restriction to maintain an is-a relationship to a generic document domain structure means that you can use specialization to create the kind of subject domain structure that are part of the intersection with the document domain, such as specifying that a `recipe` document consists of sections for `introduction`, `ingredients`, and `preparation`, but you can't take the next step of factoring out the presentation, since factoring out presentation means precisely that you are breaking the is-a relationship with the document domain.[*fn.specialization]

        footnote:(*fn.specialization)
            To be completely clear on this, the specialization mechanism will let you create any elements name you like as specializations of any other element name you like. As such, it can create any structure you like. But the entire justification of the specialization mechanism rests on the maintenance of the is-a relationship between the base and specialized element. Once this is broken, code inheritance and fallback processing no longer work, and it simply becomes an unnecessarily complex and error prone way of writing a new and unrelated schema.

        Labels are a big part of document domain content. When we created a subject domain structure for recording the nutritional information for a recipe, we factor out all of the labels by putting the content in named fields. In other words, the labels went from being data (in the content) to {metadata} (part of the structure):

        ```(sam)
            nutrition:
                serving: 1 large (50 g)
                calories: 78
                total-fat: 5 g
                saturated-fat: 0.7 g
                polyunsaturated-fat: 0.7 g    
                monounsaturated-fat: 2 g    
                cholesterol: 186.5 mg    
                sodium: 62 mg    
                potassium: 63 mg    
                total-carbohydrate: 0.6 g    
                dietary-fiber: 0 g    
                sugar: 0.6 g    
                protein: 6 g    

        Though this structure looks superficially like a list, it is really a data record. If this structure were created as a specialization of a list and then published using a generic list publishing algorithm, the result would be:

        """
            * 1 large (50 g)
            * 78
            * 5 g
            * 0.7 g
            * 0.7 g    
            * 2 g    
            * 186.5 mg    
            * 62 mg    
            * 63 mg    
            * 0.6 g    
            * 0 g    
            * 0.6 g    
            * 6 g    

        In short, there is no "is-a" relationship between generic {document domain} structure and the structures of pure {subject domain} content. These are simply data fields with no presumption about presentation attached to them. 

        Does this mean that you cannot create a {subject domain} structure using DITA's {specialization} mechanism? No, you can usually create the structure you want, but it won't be an actual {specialization} of its base type. By the very fact that it is a {subject domain} structure it has broken the "is-a" relationship with the {document domain}. As I noted at the end of [#chapter.linking], it is characteristic of the {subject domain} that its algorithms work completely differently from those of the {document domain}, so inheriting some of the processing of a base {document domain} structure is moot once you move to the {subject domain}. You new {subject domain} structure will be a new and essentially unrelated structure for which you will have to create completely new processing algorithms, just as you would if you has started defining your structure from scratch.
        
        Generally, the fewer pieces of an architecture you use, the less value there is to basing your work on that architecture, both because you have more work to do and because you take less advantage of the infrastructure or tools and expertise surrounding that architecture, creating a system that is less understandable to people versed in the architecture. All this betrays a poor fit between the system partitioning you are creating and the partitioning that the architecture provides. All architectures come with overheads and if you don't use their features, you still have to live with their overhead, which adds cost and complexity to your system. Thus, while you can use DITA and depart from the default DITA way of doing things, the value of using DITA diminishes the further you depart from the DITA way. The same would be true of any other information typing architecture.

        section: Limits on rhetorical constraint
        
            Throughout this book I have stressed the value of constraining rhetoric. One of the limits imposed by DITA's topic and map architecture is that it limits the size of unit to which rhetorical constraints can be applied.

            For instance, if you regard a recipe as being made up of one concept, one reference, and one task topic, then DITA will let you constrain the rhetoric of specialized ingredient list reference topic, or a preparation task topic, but not of the recipe as a whole.  DITA will let you write a map to combine a concept topic containing an introduction, a reference topic containing a list of ingredients, and a task topic containing preparation instructions. But it does not give you a way to specify that a recipe topic consists of one concept topic, one reference topic, and one task topic in a specific order. In other words, DITA does not provide any direct way to define larger types or the overall rhetorical structure of documents.[*fn.specialize_map]
            
            footnote:(*fn.specialize_map)

                Actually, it might be technically possible to create a map that was constrained in this way through specialization. In other words, to create a recipe map that is only allowed to contain one recipe-intro topic, one ingredients-list topic, and one preparation topic in that order. But there is no higher level way of specifying this kind of constrained map and the {functional lucidity} or such an approach is clearly very low. But the real point here is not the technical limitation but the limits of the block and map architecture itself to support effective rhetorical constraint. 
            
            Actually, it is possible to define a recipe topic types in DITA, but this involves having a different idea about how atomic a DITA topic is. Some DITA practitioners might say that a recipe is not a map made up of three information types, but a single task topic. In this view, a task topic is much more than what Information Mapping would call a procedure. It allows for the introduction of a task, a list of requirements, and the procedure steps all within the definition of a single topic. This mean effectively abandoning DITA's principle of segregating information types into separate files. (I have asked a number of DITA practitioners how a recipe should be modeled in DITA and have received each answer from multiple people.) 
            
            One of the reasons for this uncertainty about what an atomic topic is in DITA is DITA's focus on {content reuse}. DITA topics are not only units of information typing, they are units of reuse. The approach in which a recipe is a single topic leaves you with fewer, larger units of content, which makes individual topics harder to reuse. The atomic unit of  content that is small enough to maximize potential reuse is much smaller than the atomic unit of  content that contains a complete {rhetorical pattern}. The atomic unit of reuse is smaller than the atomic unit of use.
            
            Because DITA has no direct mechanism for describing model larger than a topic, a DITA practitioner is left with a choice between modeling for maximum reuse and modeling to constrain a topic type to rhetorical structure. In practice, it seems that different DITA users make different decisions about how atomic their topic types should be, based on their business needs. 


    section: DocBook
    
        index:: type, term
            language, DocBook
    
        {DocBook} is not really extensible in the same sense as the other languages mentioned here, but it still deserves a mention. DocBook does not provide an extension mechanism like {DITA}'s specialization. What it does provide is a deliberately modular construction that makes it easy to create new {schemas} that include elements from DocBook. DocBook takes full advantage of the extensibility features built into XML schema languages. 
        
        Does the fact that DocBook does not invent its own extension mechanism means that it is not as extensible as DITA? No. By relying on XML's own extensibility features, which are both more comprehensive and lower level than DITA's specialization mechanism, DocBook is as extensible as it is possible for any XML vocabulary to be. (And thus it is more extensible than DITA since it is not limited by the restriction to creating new elements that have an is-a relationship to existing elements.)
        
        Where it differs from DITA specialization is that there is no fall-back processing.  Extensions of DocBook are not DocBook. They are new languages that incorporate DocBook structures. The extensions cannot be processed by standard DocBook tool chains, though the incorporated DocBook structures obviously can. DITA's specialization mechanism means that a specialized topic will always pass through the DITA publication process, though whether it will be presented in a useful or comprehensible way very much depends on how well the is-a relationship between specialization and base was maintained. If you would rather ensure that topics always pass through the publication process, even if the results are gibberish, DITA will support that. If you want to ensure that errors are raised if any structure is not recognized by the publishing tool chain (thus avoiding accidental gibberish) then DocBook's extension mechanism will give you that.
              
        Another aspect of DocBook customization deserves to be mentioned here even though it is not strictly speaking extension. DocBook has a huge tag set and it is quite conceivable that if you want a small constrained {document domain} markup languages that you can create one by sub-setting DocBook. DocBook provides for just about every document structure out there, so if you are building a document domain language, chances are the pieces you need are in there. You can even add additional constraints.
        
        The great advantage of creating a new language as a subset of DocBook is that the result is also a valid DocBook document and can therefore be published by the DocBook tool chain. You will not have to write any new publishing algorithms if you take this approach (other than to customize formatting to your needs, which is required with any system). Creating a subset of DocBook can therefore allow you to impose more constraints and improve {functional lucidity} significantly compared to standard DocBook without having to write any processing code at all.
        
        Technically speaking, any {XML}-based markup language is extensible in the same way that DocBook is. However, DocBook's structure, and the implementation of its schemas, was designed deliberately to support both extension and sub-setting of DocBook, something which is not true for many markup languages.
        
    section: RestructuredText    
    
        index:: type, term
            language, RestructuredText
    
        RestructuredText has a number of blocks for things like paragraphs, titles, and lists that are defined with a concrete syntax. It defines other blocks using directives: 
        
        ```(reStructuredText)
            .. image:: images/biohazard.png
               :height: 100
               :width: 200
               :scale: 50
               :alt: alternate text

        
        It is extensible by adding new directives to the language. However, there is no {schema} language for RestructuredText. To create a new directive, you have to create the code that processes it. 
        
        There is an important distinction to be made between languages that are extensible by schema and those that are extensible by writing code to process the extension. If a language is extended by writing processing code for the extension, the only way to know if the input is valid is by processing it. If it raises a processing error, it is invalid. 
                
        If you have only one processor for a language, you can treat that processor as normative. That is, the definition of a correct file is any file that can be successfully processed by the normative processor. The language, in other words, is defined by the processor. But if you have multiple processors, how do you determine who is at fault when of of those processor fails to process a given input file? Is the processor incorrect or the source file? 
        
        A schema creates a language definition that is independent of any processor. (In other words, it partitions and redirects the complexity of validation in language design.) It is the schema that is normative, not any of the processors. If the source file is valid per the schema, the processor is at fault if it does not process that file correctly. If the source file is not valid per the schema, the blame lies with the source file. 
        
        In the case of RestructuredText, the capacity of the processor to be extended in this way is built into the processor architecture. It is not like you have to hack around in the code to add your extensions. There is a specific and well documented way to do it. But while RestructuredText allows you to extend it by adding new directives, it does not have a constraint mechanism. There is no mechanism (other than by hacking into the code) to restrict the use either of new directives or the existing directives and structures. 

        
    section: TeX
    
        index:: type, term
            language, TeX
            
        TeX (pronounced "Tek") is a typesetting system invented by {Donald Knuth} in 1978. As a typesetting language it is a concrete {media domain} language. But Knuth also included a macro language in TeX which allows users to define new commands in terms of existing commands. (I say commands because that is the term used in TeX. Markup in the media domain tends to be much more imperative than markup in the subject domain, which is entirely descriptive, so "commands" is an appropriate name for TeX's tags.) This macro language has been used to extend TeX, most notably in the form of {LaTeX}, a {document-domain} language that we looked at in [#chapter.lightweight].
        
        As we noted with {RestructuredText}, the ability to extend is not the same thing as the ability to constrain. Introducing new commands does not create a constraint mechanism.  
        
    section: SAM
    
        index:: type, term
            language, SAM
    
        While lightweight languages provide great {functional lucidity}, they suffer from limited extensibility (which generally requires writing code) and a general lack of constraint mechanisms. I believe that a fully extensible, fully constrainable lightweight markup language would be a valuable addition to the structured writing toolkit. This is why I have developed {SAM}, the markup language used for most of the examples in this book and for writing the book itself. 
        
        As described in [#chapter.markup], SAM is a hybrid markup language which combines implicit syntax similar to {MarkDown} with an explicit syntax for defining abstract structures called blocks, record sets, and annotations, and with specific concrete markup for common features such as insertions, citations, and variable definitions. 
        
        SAM, like {XML}, is for defining specific markup languages. However, all languages defined in SAM share a small common base set of text structures for which SAM provides concrete syntax. This allows SAM to combine lightweight syntax for the most common text structures with the ability to define specific constrained markup languages for particular purposes, particularly {subject domain} languages. In other words, SAM represents a different partitioning of the markup design process from both the common lightweight languages and from XML.
                
        SAM is designed to be extensible and constrainable through a {schema} language (this is not complete at time of writing, but hopefully will be available by the time you read this). The intent is that the schema language should be able not only to define and constrain new block structures, but to constrain the use of the concrete structures as well, and to constrain the values of fields using patterns. 
        
        SAM is not designed to be nearly as general as XML in its applications. As a result, its syntax is simple and more {functionally lucid} and its schema language should also be simpler and make it much easier for writers to develop their own SAM-based markup languages. 
        
        I use SAM for the majority of the examples in this book because SAM is designed to make structure clear. All the examples could be expressed in XML as well. Using XML would just have made them harder to follow. Naturally, to write in SAM you would need to know more about the rules of the language, but you should be able to read a typical SAM document and understand its structure with little or no instruction. 
        
        This is similar, but not identical, to the aim of mainstream concrete and hybrid languages such as {Markdown}(language) and {Restructured Text}(language), which is to have the source file be readable as a document. In other words, they strive to make the document structure clear from the markup. They are document domain languages, and they strive to make sure that the markup expresses the document structure they create in a way that is readable. SAM has the same goal, except that SAM was designed primarily for creating {subject domain} languages. As such, it is designed to make the subject domain structure of the document clear to the reader. 
        
        A SAM document may not look as much like a finished document as a {Markdown}(language) or {reStructuredText}(language) document. For example, it does not use underlines to visually denote different levels of header. Instead, it focuses on creating a hierarchy of named blocks and fields. In doing so, it uses the kind of markup people commonly use to create named blocks of text and to express a hierarchical relationship between them. Blocks are introduced with a name followed by a colon, and hierarchy is expressed through indentation. 
        
        ```(SAM)
            examples: Basic SAM structures

                example: Paragraphs
                    The is a sample paragraph. It is inside
                    the {block}(structure) called `example`.
                    It contains two {annotations}(structure),
                    including this one. It ends with a blank
                    line.

                    This is another paragraph.

                example: Lists

                    Then there is a list:

                    1. First item.
                    2. Second item.
                    3. Third item.

                example: Block quote

                    Next is a block quote with a {citation}(structure).

                    """[Mother Goose]
                        Humpty Dumpty sat on a wall.
                        
        SAM is an open source project. A description of the language and a set of associated tools are available from {https://github.com/mbakeranalecta/sam}(url).

    section: SPFE
    
        index:: type, term
            tool, SPFE
    
        {SPFE}(tool) is another project of mine. It is designed to be a framework for implementing structured writing algorithms and its structure follows the model I laid out in [#chapter.publishing]. It is tempting to compare it to {DITA} as an {information typing architecture}, but as I commented before, architectures are not necessarily parallel to each other and often differ in their emphasis. SPFE takes a different approach to the partitioning and distribution of content complexity, with a major emphasis on directing content management and information architecture complexity away from writers. Individual writers working in a SPFE system should have to know little or nothing about how SPFE works, as long as they follow the constraints of the markup language they are using.
        
        SPFE is principally designed for {subject domain} markup. As such, it does not start with a generic document domain topic type like DITA. SPFE does not require any particular schema, though it does require that {schema}s meet certain constraints. 

        But SPFE does not leave it entirely to you to develop schemas from scratch. Instead, it supports building schemas from pre-built components. The pre-built components include a collection of semantic blocks and the default processing code for each stage of the publishing algorithm. SPFE also allows you to define your own reusable structured components with processing code. This is, essentially, extensibility through composition, rather than extensibility through specialization (as in {DITA}) or extensibility through processor extension (as in {reStructuredText}). Constraints are supported through normal schema mechanisms and by selecting the minimal required structural components for the individual case. 

        By strictly segregating the presentation and formatting layers, SPFE reduces the effort required to process custom markup formats. Custom format are processed to a common document-domain markup language which it then processed to all required media-domain output formats. The SPFE Open Tool Kit includes a basic {document domain} language for this purpose, but you can also use {DocBook} or {DITA} in this role, allowing you to take advantage of their existing publishing capabilities. This also allows you to install SPFE as an authoring layer on top of an existing DITA or DocBook tool chain.         

        To create a subject-domain markup language in SPFE, therefore, all you have to define for yourself are the key subject-domain fields and blocks that are essential to your business. All the other elements you need, such as paragraphs, lists, tables, and common annotations, you can include from the pre-built components, along with their default processing code. 

        Among its default processing steps, the SPFE process includes support for linking based on {subject annotation} as describe in [#chapter.linking]. The {conformance} and {audit} algorithms described in [#part.management] are well-supported as well. 
        
        While it has support for {reuse}, SPFE is not as focused on content reuse or content management as {DITA}. It deliberately limits some of the forms of reuse that tend to produce unmanageable complexity. While it can produce books and {top-down information architecture}s, its main focus is {hypertext} and {bottom-up information architecture}s. SPFE does not define or require maps as an assembly mechanisms, though you could implement maps in SPFE if you wanted them. SPFE's processing model is modeled on a software build architecture and it is designed to work well with a {version control system} system as a repository rather than a {content management system}. One of its key design objectives is that writers should have to know little or nothing about how SPFE works.  
        
        Both {SAM} and {XML} are supported as markup syntax for SPFE, and you can freely mix and match SAM and XML content. 
        
        SPFE is an open source project available from {https://github.com/mbakeranalecta/spfe-open-toolkit}(url).
