chapter:(#chapter.extensible) Extensible and Constrainable Languages

    <<<(annotations.sam)
    index::type, term
        concept, extension
        concept, constraint

    The languages we have looked at to this point are publicly specified and have existing tool chains. Some are more constrained than others, and some support different structured writing algorithms and different ways of partitioning and redirecting complexity. Choosing one of them makes sense if the constraints they express and the algorithms they support partition content complexity in a way that is right for your organization. If not, you need to create your own structures to improve how complexity is partitioned and distributed in your organization.

    You have three options for doing this:
    
    * Create your own language entirely from scratch, creating both the syntax and the semantics. (This is what {John Gruber}(person) did when he created {MarkDown}.)
    
    * Use an existing abstract markup syntax, such as XML or SAM, and create your own semantics by defining named structures using that syntax as described in [#chapter.markup].
    
    * Take an existing markup language with extensible and/or constrainable semantics, such as DITA or DocBook, and extend and/or constrain it to meet your needs. 
    
    Each of these approaches has merits and drawbacks. For instance, creating a new language may enable you to achieve exceptional functional lucidity for a particular type of information; extending/constraining an existing language may save you a lot of tool development costs; while defining your own semantics based on an existing syntax may enable you to find the right balance between functional lucidity and development costs. 

    This chapter looks at extensible and constrainable markup languages.     

    section: XML
    
        The X in {XML} stands for eXtensible, but, as noted in [#chapter.markup], XML is an abstract language that does not define any document structures itself. Therefore, with XML, you start from zero. Syntactically, everything is defined for you. Semantically you start from scratch. 
        
        You can define the structure of a new XML markup language using one of the several available {schema languages}. We will look at schema languages in [#chapter.constraints], but the mechanics of defining a markup language in XML are out of scope for this book. 

    section: DITA
    
        DITA is unique among markup languages because it was designed for extension from the beginning. In fact, it is something of a misnomer to call DITA a markup language. DITA actually calls itself an information typing architecture. What is an information typing architecture? DITA is really the only thing that calls itself by this name, so we have to derive the definition from the properties of this one example. 
        
        The conventional way to define a document type is using a schema language. Schema languages are simply languages for describing constraints on markup structures. (We will look at them in [#chapter.constraints]). So what does an information typing architecture provide over and above what a schema language provides?
        
        The programming world provides one useful example. Programming environments use architectures and frameworks to abstract certain types of operations to a higher level. For example, the environment may implement commonly used functions that you could program from scratch, but having those functions available moves complexity from the local developer to the algorithms supplied in the architecture, saving time and possibly avoiding errors. The usefulness of an architecture depends on whether it partitions and distributes the problem space in substantially the same way you would have done it if you were designing the system from scratch. (Of course, you might not have come up with all the good ideas in a good architecture yourself; the point is that once you have been exposed to those ideas and understand them, would you then partition and distribute complexity according to those ideas or not?)

        So the question to ask about DITA is, does its architecture partition and distribute content complexity the way that you would naturally wish it to be done to achieve the best outcome for your organization? 
        
        The DITA architecture consists of a set of predefined markup languages that you can extend using a mechanism called {specialization}, a facility for assembling information products (maps), a facility for managing metadata (subject schema), an extensive set of {management-domain} markup, and specifications for the behaviors associated with all of these capabilities.
        
        In other words, it predefines a range of structures, semantics, and operations that you might need in establishing an information architecture and then provides a way for you to build from there. 
        
        As with any other architecture, DITA's usefulness depends on how well the predefined structures, semantics, and operations suit your needs; how easy the extension mechanism is to use; and how reliable the available implementations are. 
        
        It is common in the software world for there to be many competing architectures with different sweet spots. Because an architecture is essentially a series of guesses about what a variety of systems may have in common, different architectures may be constructed differently to cover different sets of commonalities among diverse projects, and you may not see equivalent architectural features from one system to another. 
        
        There are not a lot of information typing architectures. The only other one I am aware of is the one I am developing myself, which is called {SPFE}. {SPFE}, however, has a very different architecture from {DITA}. {DITA} has grown into a very large and complex architecture over the years, and it is out of scope for this book to describe its architecture in full.[*1] All I shall attempt to do here is to do a basic mapping of some key features of the DITA architecture to the structured writing concepts explored in this book.  
        
        footnote:(*1)
            Obviously I am not a fan of the DITA architecture or I would not be developing my own architecture in {SPFE}. For a much fuller and more sympathetic treatment of the DITA architecture, see {DITA for Practitioners Volume 1: Architecture and Technology}(citetitle) by Elliot Kimber, {http://xmlpress.net/publications/dita/practitioners-1/}(url)

        Inherent in the process of constructing an architecture is that you partition the field in certain ways. Architectures move functionality to a higher level by choosing some options and rejecting others. An XML schema is an information typing language that can define a markup language for any purpose at all. However, some applications of XML, such as recording transfers between banks or storing the configuration options of an editor, are not within the scope of the information typing that the DITA architecture was designed for. Therefore, DITA has a more restricted definition of information typing. The DITA specification defines information typing this way:    
        
        """[http://docs.oasis-open.org/dita/dita/v1.3/csd01/part3-all-inclusive/archSpec/base/information-typing.html]
            Information typing is the practice of identifying types of topics, such as concept, reference, and task, to clearly distinguish between different types of information.
            
        Unfortunately this definition is largely circular -- information typing defines information types. But it does help establish a scale. Information typing is about defining topic types. The spec goes on to define the purpose of information typing:
        
        """
            Information typing is a practice designed to keep documentation focused and modular, thus making it clearer to readers, easier to search and navigate, and more suitable for reuse.
        
        DITA information typing is not as general as structured writing. It focuses on information at a particular scale and on a subset of the structured writing algorithms. That does not make it impossible to work at other scales or implement other algorithms, it just means that the archicture provides better support for certain areas.
        
        Out-of-the-box DITA is commonly associated with the idea that there are just three information types, task, concept, and reference. The DITA spec makes it clear that this is not the intention of DITA as an information typing architecture. 
        
        """
            DITA currently defines a small set of well-established information types that reflects common practices in certain business domains, for example, technical communication and instruction and assessment. However, the set of possible information types is unbounded. Through the mechanism of specialization, new information types can be defined as specializations of the base topic type (<topic>) or as refinements of existing topics types, for example, <concept>, <task>, <reference>, or <learningContent>.

        As I have noted, many structured writing algorithms partition complexity best with more specific markup, particularly markup in the subject domain. The ability to create an unbounded set of information types is thereforei relevant to getting the most out of structured writing. 
        
        Clearly, though, one does not need an information typing architecture to define an information type. You can, as {John Gruber} did with {MarkDown}, sit down and sketch out a set of structures and a syntax to represent them, and then write a program to process them. With an {abstract language} like {XML} or {SAM}, you can create a new information type by defining a set of named elements and attributes using a schema language. How does using a higher level information typing architecture like DITA change this process? How does it partition the design problem differently?
        
        First and foremost, it means that you don't start from scratch. All topic types in DITA are derived from a base topic type called `topic` through specialization. 
        
        What is specialization? XML syntax defines abstract structures that do not occur in documents: elements, attributes, etc. To create a markup language in XML, you define named elements and attributes for the structures you are creating. This is a type of specialization. For example, in {DocBook} `para` is a type of element. `para` has what is called an "is-a" relationship to elements: `para` "is-an" element, but it is a special type of element. An XML parser will process a `para` generically as an element, reporting its name to the application layer. The application layer must then supply a rule that processes just this specialized `para` element (and not the also specialized but different `title` element).

        In this case, the "is-a" relationship means that the `para` element must comply with the constraints of XML elements as defined in the XML standard specification.
        
        DITA specialization follows the same principle, but moves it up a level. The base `topic` topic type is the abstract structure. More specific types like `knitting-pattern` or `ingredients-list` are specializations of `topic` (or of other topic types that are specializations of `topic`). A generic DITA processor can process them as a instance of `topic`, but it requires additional code to process them specifically as `knitting-pattern` or `ingredient-list`. Each of these specialized types has an "is-a" relationship with the type it was specialized from. So `knitting-pattern` "is-a" topic. 
        
        DITA specialization differs from simply creating new named elements in XML in the following ways:

        * The base DITA topic type is not an abstraction like an XML element. You cannot create an XML element without inventing a name for it. The base DITA topic type, on the other hand, is a fully implemented topic type that you can use directly. You can, and people do, write directly in the base topic type without inventing anything new. We noted in [#chapter.rhetorical_structure] that it is sometimes easy to treat what is intended as a meta model as a generic model. This is the case here. All topic types in DITA are derived by specialization from the generic `topic` type. They all have an "is-a" relationship to this generic type. 

        * The "is-a" relationship requires the specialized element to follow the constraints of the base topic. If the base topic requires a particular ordering of child elements, the specialized element must follow that ordering. If the base topic includes optional child elements, the specialized element can omit them, but it cannot omit required elements. In general, a specialized element can be more constrained than the element it is specialized from, but not less constrained. 
        
        One consequence of this is that a processor can process a specialized element as though it were the base element without failure. The result may not reflect all aspects of the specialized element, but the processing will not fail.

        Another consequence of this is that while the set of topic types you can create from the generic DITA topic type may be unbounded, it is not universal. If a specialized topic has an is-a relationship to the generic topic, the structures in the specialized topic must have a corresponding is-a relationship to the parallel structures in the generic type. This means you can create information types that do not have an is-a relationship to a DITA generic topic.

        !!!(RLH)
                I significantly rewrote the section above because I had trouble understanding it. I think I know where you're headed, so I rewrote and added a list item above, but please check to be sure I got things right.
        
        To specialize a topic type, you specialize the root element and any child elements or attributes that you need to define your new topic type. Each specialized element or attribute must have an is-a relationship to the element it specializes. Thus a procedure element might be a specialization of an ordered list element and its step elements might be specializations of a list item element. In this case, processing a procedure as an ordered list would produce meaningful output, which is what you would expect. However, you would probably want to specialize the output of steps in a procedure, perhaps by prefixing each step with "Step 1:" rather than just "1."
               
        The second way in which specialization differs from giving names to abstract elements is that specialization is recursive. That is, suppose you have a topic type `animal-description`, which is a specialization of `topic`. You want to impose additional constraints on the description of different types of animal, so you create more specialized types `fish-description` and `mammal-description`, which are specializations of `animal-description` (and could be processed like an `animal-description` if no other processing were specified for them). Then you might decide to impose still more constraints on the description of different kinds of mammals, so you create the type `horse-description`, which is a specialization of `mammal-description`. This type would be processed as a `mammal-description` if no specific processing is provided for `horse-description`; as `animal-description` if no specific `mammal-description` processing is provided; and as `topic` if no specific `animal-description` processing is provided. 
        
        The third way in which information typing in DITA differs from creating a language from scratch is that DITA information types share a common approach to processing and to information architecture. In particular, they inherit a common set of {management-domain} structures and their associated management semantics. It is possible to ignore all of these things, but if you do so, the value of using DITA for information typing is reduced because you then have to invent your own ways of doing these things that are contrary to the partitioning provide by the architecture. 
        
        Generally, the fewer pieces of an architecture you use, the less value there is to basing your work on that architecture, both because you have more work to do and because you take less advantage of the infrastructure or tools and expertise surrounding that architecture, creating a system that is less understandable to people versed in the architecture. All this betrays a poor fit between the system partitioning you are creating and the partitioning that the architecture provides. All architectures come with overheads and if you don't use their features, you still have to live with their overhead, which adds cost and complexity to your system. Thus, while you can use DITA and depart from the default DITA way of doing things, the value of using DITA diminishes the further you depart from the DITA way. The same would be true of any other information typing architecture.
        
    section: Specializing between domains     
        
        We have talked a lot about the benefits of moving content from the document domain to the subject domain as a means to factor out constraints in the document domain and as a way to enable multiple structured writing algorithms and improve functional lucidity.
        
        DITA's base types clearly belong to the document domain. Clearly you can use specialization to create more specialized document domain structures. But can you use specialization to create subject-domain structures that factor out aspects of the document domain?
        
        In formal terms the answer is no. Subject domain information does not have an is-a relationship to document domain information precisely because it is the document domain structures that you factor out when you move to the subject domain. 
        
        Take the list of ingredients in a recipe. In the document domain, they could be presented as a list or as a table. In subject domain terms they are actually more of a table (database sense) than a list. That is, a set of records with a defined semantic structure:
        
        ```(sam)
            ingredients:: ingredient, quantity, unit
                eggs, 3, each
                salt, 1, tsp
                butter, .5, cup 
                    
        How do you create this record structure by specializing document domain elements? What is the best starting point to specialize from? A table is the most obvious candidate because it is structured like a set of records. However, the more common presentation of an ingredient list is as a list. But there is no structure in a list that supports dividing a list item into three named fields. The ingredient structure above is neither a list nor a table; it is a record set, a data structure whose contents could be presented in many different ways in the document domain, but is not a specialization of any of them. 
        
        We can make a distinction between two main types of {subject domain} languages, the rhetorical subject domain, where the focus in on how to present information on a particular subject, and the data-oriented subject domain, where the focus in on capturing the various pieces of information that are required on a subject while factoring out the presentation. Both present problems for specialization. 
        
        One of the elementary things that the rhetorical subject domain does is to factor out the titles of various sections of a topic, replacing the generic structure of sections and titles with specific names blocks whose titles can be supplied consistently by the {presentation algorithm}. These named sections do have a kind of is-a relationship to generic sections, in that they are sections. But this is not a true is-a relationship because a generic section requires a title, and a rhetorical block cannot have on. It therefore does not have all the characteristics of the thing it is specialized from, and therefore it is not a section. It is a named block of text that, in the document domain, would be expressed as a section, but that is not the same things as falling back to being a section. Processing such a block as a section would result in the text following on from that of the previous section with no title, thus lacking the defining document and media domain characteristic of a section. 
        
        For the data-oriented subject domain, which, let us remember, can include material recorded in databases and source code, there is no necessary relationship between the structure of content in the source and the presentation of content in the document domain. Information that is in separate fields in the subject domain may need to be combined to form sentences by the {presentation algorithm} before it is even readable as a piece of content. Such structures clearly have no is-a relationship to a DITA generic topic.  
        
        More generally, as we have seen, the {subject domain} changes the algorithm for each function so inheritance of the code is of little or no value. Subject domain algorithms work differently. And since all subject domain algorithms work on the same structures, rather than there being separate structures for each algorithm, as in the document and management domains, there is little scope for inheritance of processing code from the document domain to the subject domain. Rather, the document domain is semantically downstream from the subject domain. Subject domain algorithms essentially create document domain structures as part of the publication process, as we saw in [#chapter.publishing].  

    section: DocBook
    
        {DocBook} is not really extensible in the same sense as the other languages mentioned here, but it still deserves a mention. DocBook does not provide an extension mechanism like {DITA}'s specialization. What it does provide is a deliberately modular construction that makes it easy to create new schemas that include elements from DocBook. DocBook takes full advantage of the extensibility features built into XML schema languages. 
        
        Does the fact that DocBook does not invent its own extension mechanism means that it is not as extensible as DITA? No. By relying on XML's own extensibility features, which are both more comprehensive and lower level than DITA's specialization mechanism, DocBook is as extensible as it is possible for any XML vocabulary to be. 
        
        Where it differs from DITA is that there is no fall-back processing.  Extensions DocBook are not DocBook. They are new languages that incorporate DocBook structures. The extensions cannot be processed by standard DocBook tool chains, though the incorporated DocBook structures obviously can. DITA's specialization mechanism means that a specialized topic will always pass through the DITA publication process, though whether it will be presented in a useful or comprehensible way very much depends on how well the is-a relationship between specialization and base was maintained. If you would rather ensure that topics always pass through the publication process, even if the results are gibberish, DITA will support that. If you want to ensure that errors are raised if any structure is not recognized by the publishing tool chain (thus avoiding accidental gibberish) then DocBook's extension mechanism will give you that.
              
        Another aspect of DocBook customization deserves to be mentioned here even though it is not strictly speaking extension. DocBook has a huge tag set and it is quite conceivable that if you want a small constrained document domain markup languages that you can create one by sub-setting DocBook. DocBook provides for just about every document structure out there, so if you are building a document domain language, chances are the pieces you need are in there. 
        
        The great advantage of creating a new language as a subset of DocBook is that the result is also a valid DocBook document and can therefore be published by the DocBook tool chain. You will not have to write any algorithms at all if you take this approach. Creating a subset of DocBook can therefore allow you to impose more constraints and improve {functional lucidity} significantly compared to standard DocBook without having to write any processing code at all.
        
        Technically speaking, any XML-based markup language is extensible in the same way that DocBook is. However, DocBook's structure, and the implementation of its schemas, was designed deliberately to support both extension and sub-setting of DocBook, something which is not true for many markup languages.
        
    section: RestructuredText
    
        RestructuredText has a number of blocks for things like paragraphs, titles, and lists that are defined with a concrete syntax. It defines other blocks using directives: 
        
        ```(reStructuredText)
            .. image:: images/biohazard.png
               :height: 100
               :width: 200
               :scale: 50
               :alt: alternate text

        
        It is extensible by adding new directives to the language. However, there is no schema language for RestructuredText. To create a new directive, you have to create the code that processes it. 
        
        There is an important distinction to be made between languages that are extensible by schema and those that are extensible by writing code to process the extension. If a language is extended by writing processing code for the extension, the only way to know if the input is valid is by processing it. If it raises a processing error, it is invalid. 
                
        If you have only one processor for a language, you can treat that processor as normative. That is, the definition of a correct file is any file that can be successfully processed by the normative processor. The language, in other words, is defined by the processor. But if you have multiple processors, how do you determine who is at fault when of of those processor fails to process a given input file? Is the processor incorrect or the source file? 
        
        A schema creates a language definition that is independent of any processor. (In other words, it partitions and redirects the complexity of validation in language design.) It is the schema that is normative, not any of the processors. If the source file is valid per the schema, the processor is at fault if it does not process that file correctly. If the source file is not valid per the schema, the blame lies with the source file. 
        
        In the case of RestructuredText, the capacity of the processor to be extended in this way is built into the processor architecture. It is not like you have to hack around in the code to add your extensions. There is a specific and well documented way to do it. But while RestructuredText allows you to extend it by adding new directives, it does not have a constraint mechanism. There is no mechanism (other than by hacking into the code) to restrict the use either of new directives or the existing directives and structures. 

        
    section: TeX
    
        TeX (pronounced "Tek") is a typesetting system invented by Donald Knuth in 1978. As a typesetting language it is a concrete {media domain} language. But Knuth also included a macro language in TeX which allows users to define new commands in terms of existing commands. (I say commands because that is the term used in TeX. Markup in the media domain tends to be much more imperative than markup in the subject domain, which is entirely descriptive, so "commands" is an appropriate name for TeX's tags.) This macro language has been used to extend TeX, most notably in the form of {LaTeX}, a {document-domain} language that we looked at in [#chapter.heavyweight].
        
        As we noted with RestructuredText, extension of a language is not the same thing as constraint. Introducing new commands does not create a constraint mechanism.  
        
    section: SAM
    
        While lightweight languages provide great {functional lucidity}, they suffer from limited extensibility (which generally requires writing code) and a general lack of constraint mechanisms. I believe that a fully extensible, fully constrainable lightweight markup language would be a valuable addition to the structured writing toolkit. This is why I have developed {SAM}, the markup language used for most of the examples in this book and for writing the book itself. 
        
        As described in [#chapter.markup], SAM is a hybrid markup language which combines implicit syntax similar to {MarkDown} with an explicit syntax for defining abstract structures called blocks, recordsets, and annotations, and with specific concrete markup for common features such as insertions, citations, and variable definitions. 
        
        SAM, like {XML}, is for defining specific markup languages. However, all languages defined in SAM share a small common base set of text structures for which SAM provides concrete syntax. This allows SAM to combine lightweight syntax for the most common text structures with the ability to define specific constrained markup languages for particular purposes, particularly {subject domain} languages. In other words, SAM represents a different partitioning of the markup design process from both the common lightweight languages and from XML.
                
        SAM is designed to be extensible and constrainable through a schema language (this is not complete at time of writing, but hopefully will be available by the time you read this). The intent is that the schema language should be able not only to define and constrain new block structures, but to constrain the use of the concrete structures as well, and to constrain the values of fields using patterns. 
        
        SAM is not designed to be nearly as general as XML in its applications. As a result, its syntax is simple and more {functionally lucid} and its schema language should also be simpler and make it much easier for writers to develop their own SAM-based markup languages. 
        
        I use SAM for the majority of the examples in this book because SAM is designed to make structure clear and that is all I have needed to do in most examples. All the examples could be expressed in XML as well. Using XML would just have made them harder to follow. Naturally, to write in SAM you would need to know more about the rules of the language, but you should be able to read a typical SAM document and understand its structure with little or no instruction. 
        
        This is similar, but not identical, to the aim of mainstream concrete and hybrid languages such as {Markdown}(language) and {Restructured Text}(language), which is to have the source file be readable as a document. In other words, they strive to make the document structure clear from the markup. They are document domain languages, and they strive to make sure that the markup expresses the document structure they create in a way that is readable. SAM has the same goal, except that SAM was designed primarily for creating subject domain languages. As such, it is designed to make the subject domain structure of the document clear to the reader. 
        
        A SAM document may not look as much like a finished document as a {Markdown}(language) or {reStructuredText}(language) document. For example, it does not use underlines to visually denote different levels of header. Instead, it focuses on creating a hierarchy of named blocks and fields. In doing so, it uses the kind of markup people commonly use to create named blocks of text and to express a hierarchical relationship between them. Blocks are introduced with a name followed by a colon, and hierarchy is expressed through indentation. 
        
        ```(SAM)
            examples: Basic SAM structures

                example: Paragraphs
                    The is a sample paragraph. It is inside
                    the {block}(structure) called `example`.
                    It contains two {annotations}(structure),
                    including this one. It ends with a blank
                    line.

                    This is another paragraph.

                example: Lists

                    Then there is a list:

                    1. First item.
                    2. Second item.
                    3. Third item.

                example: Block quote

                    Next is a block quote with a {citation}(structure).

                    """[Mother Goose]
                        Humpty Dumpty sat on a wall.
                        
        SAM is an open source project. A description of the language and a set of associated tools are available from {https://github.com/mbakeranalecta/sam}(url).

    section: SPFE
    
        {SPFE}(tool) is another project of mine. It is designed to be a framework for implementing structured writing algorithms and its structure follows the model I laid out in [#chapter.publishing]. It is tempting to compare it to {DITA} as an information typing architecture, but as I commented before, architectures are not necessarily parallel to each other and often differ in their emphasis. SPFE takes a different approach to the partitioning and distribution of content complexity, with a major emphasis on directing content management and information architecture complexity away from writers. Individual writers working in a SPFE system should have to know little or nothing about how SPFE works, as long as they follow the constraints of the markup language they are using.
        
        SPFE is principally designed for {subject domain} markup. As such, it does not start with a generic document domain topic type like DITA. SPFE does not require any particular schema, though it does require that schemas meet certain constraints. 

        But SPFE does not leave it entirely to you to develop schemas from scratch. Instead, it supports building schemas from pre-built components. The pre-built components include a collection of semantic blocks and the default processing code for each stage of the publishing algorithm. SPFE also allows you to define your own reusable structured components with processing code. This is, essentially, extensibility through composition, rather than extensibility through specialization (as in {DITA}) or extensibility through processor extension (and in {reStructuredText}). Constraints are supported through normal schema mechanisms and by selecting the minimal required structural components for the individual case. 

        By strictly segregating the presentation and formatting layers, SPFE reduces the effort required to process custom markup formats. Custom format are processed to a common document-domain markup language which it then processed to all required media-domain output formats. The SPFE Open Tool Kit includes a basic document domain language for this purpose, but you can also use DocBook or DITA in this role, allowing you to take advantage of their existing publishing capabilities. This also allows you to install SPFE as an authoring layer on top of an existing DITA or DocBook tool chain.         

        To create a subject-domain markup language in SPFE, therefore, all you have to define for yourself are the key subject-domain fields and blocks that are essential to your business. All the other elements you need, such as paragraphs, lists, tables, and common annotations, you can include from the pre-built components, along with their default processing code. 

        Among its default processing steps, the SPFE process includes the subject-based linking algorithms described in [#chapter.linking] and the subject-based composition and architecture algorithms described in [#chapter.composition] and [#chapter.architecture], including bottom-up information architecture. The {conformance} and {audit} algorithms are well-supported as well. 
        
        While it has support for {reuse}, SPFE is not as focused on content reuse or content management as {DITA}. It deliberately limits some of the forms of reuse that tend to produce unmanageable complexity. While it can produce books and top-down information architectures, its main focus is hypertext and bottom-up information architectures. SPFE does not define or require maps as an assembly mechanisms, though you could implement maps in SPFE if you wanted them. SPFE's processing model is modeled on a software build architecture and it is designed to work well with a {version control system} system as a repository rather than a content management system. One of its key design objectives is that writers should have to know little or nothing about how SPFE works.  
        
        Both {SAM} and {XML} are supported as markup syntax for SPFE, and you can freely mix and match SAM and XML content. 
        
        SPFE is an open source project available from {http://spfeopentoolkit.org}(url).
