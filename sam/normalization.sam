chapter:(#chapter.duplication) Avoiding duplication
    
    <<<(annotations.sam)
    
    index: type, term
        concept, single source of truth
        concept, single sourcing
        concept, duplication
        algorithm, avoiding duplication
        
    A significant source of complexity in any content organization is avoiding the unwanted duplication of content. Writers don't hold the entirety of the organization's content collection in their heads, so when they decide to write something, there is always the possibility that that content already exists somewhere in the content set. 

    Obviously we would prefer writers to look before they write, but if content exists but it is not easy to find, writers may create a duplicate piece of content. Duplicate content can cause a number of problems. Not only is it expensive to create, different versions of the content may not agree with each other, which creates complexity for readers. When the subject matter changes, only one of the duplicates may get updated, causing further drift between the two versions (and updating two versions is twice the work). Duplicate content may also get punished by search engines. The need to detect duplicate content is therefore a major source of complexity in the content system.

    However, detecting duplication in content is not easy. Indeed, even defining what constitutes duplication, let alone detecting that it exists, is not easy. Structured writing techniques can help, to a point, but it is important not to get carried way in the attempt to eliminate duplication, as this can easily do more harm than good, and can inject more complexity into the content system than it removes. 

    The problem of avoiding duplication is essentially the problem of detecting if content that serves a particular purpose already exists. If a writers is to avoid creating duplicate content, they need to determine as quickly as possible whether that content already exists or not. This check takes place countless times in the content systems so even if the cost of a single check is small, the cumulative cost can be large, creating a huge overhead that can significantly slow down your content system. Small costs with his repetition rates are often the hardest to detect and least satisfying to fix, whereas the wins from detecting duplication are often more visible and more satisfying. But it is important to make sure that your efforts to limit content duplication are not actually costing you more than they save.  

    This problem obviously affects any attempt at content reuse, since every time you set out to reuse content, you have to determine if reusable content exists. The longer it takes to determine if a piece of content already exists, the longer, and therefore more expensive, each instance of content reuse becomes. And bear in mind that the cost of determining if reusable content exists is incurred every time the writer looks for it, even if they don't find it, but any saving associated with reuse are realized only when reusable content is found.

    If you are going to implement a system for detecting and eliminating duplicate content, therefore, it is vital to have clear plan that tells you exactly how authors are going to detect duplicate content, or find content to reuse, and to make sure that the overhead of using such a system does not outweigh the benefits it provides. This generally requires are formal system for identifying duplicate and/or reusable content, since only an appropriately formal system can operate reliably, an only a formal system will allow you to measure its costs and benefits accurately.  
    
    Creating a formal system for ensuring that content only exists once is sometimes called establishing a single source or truth. We should be very clear from the beginning that "single source of truth" does not mean that there is only one place or system from which all truths come; it means that for every significant truth you manage, that truth is only stored once. Different truths can certainly be stored in different places. The point is to make sure that the same truth is not stored in two different places, or two times in the same place. 

    A formal system for detecting duplication essentially means establishing a set of constraints by which duplication can be defined and detected. In other words, there needs to be a set of rules that says that if item X matches item Y in aspects A, B, and C, then X is a duplicate of Y. 

    To do this in any kind of formal and consistent way, aspects A, B, and C have to be a defined part of the model of X and Y, or of the way X and Y are stored. This allows an algorithm can detect the duplication. 

    Where no such formal constraints exist, writers also have the option of using a search engine to look for existing content on the subject they want to write about. A search engine can certainly turn up existing content on a subject, but search engines are not precise enough to detect duplication reliably. They may miss an actual duplicate because of a variation in {terminology} while returning several possible matches that are not duplicates. To eliminate the possibility that a duplicate exists in those results, the writer has to assess each one of them. That takes a lot of time, and, as we have noted, this task has to be performed before writing any piece of content, if the possibility of duplication is to be eliminated. This is a tremendous amount of overhead to impose on the content system.   

    How exactly do you construct a set of constraints for detecting duplicate content?   
    
    Consider two movie reviews, written in markdown:
    
    ```(markdown)
    
        Disappointing outing for the Duke
        ================================
        
        After a memorable outing in _Rio Grande_ 
        and _Sands of Iwo Jima_, John Wayne 
        turns in an pedestrian performance 
        in _Rio Bravo_.
        
    and\:
    
    ```(markdown)
    
        Wayne's best yet
        ================
        
        After tiresome performances in _Rio Grande_ 
        and _Sands of Iwo Jima_, the Duke is brilliant 
        in _Rio Bravo_.
        
    Do we have two reviews of _Rio Bravo_ or not? A human reading the text can tell easily enough, but an algorithms would have no way to tell, since nothing in the markup of either review directly identifies which movie is being reviewed. Even if it could recognize the names of movies in the text, it would have no way to tell which one of those titles was the subject of the review. 

    But suppose these same reviews were written in a subject domain movie review language:

    ```(markdown)
    
        movie-review: Disappointing outing for the Duke
            movie-title: Rio Bravo
            review-text:
                After a memorable outing in {Rio Grande}(movie) 
                and {Sands of Iwo Jima}(movie), 
                {John Wayne}(actor) turns in 
                an pedestrian performance 
                in {Rio Bravo}(movie).
        
    and\:
    
    ```(sam)
    
        movie-review: Wayne's best yet
            movie-title: Rio Bravo
            review-text:
                After tiresome performances in {Rio Grande}(movie) 
                and {Sands of Iwo Jima}(movie), 
                {the Duke}(actor, "John Wayne") is brilliant 
                in {Rio Bravo}(movie).
    
    Now it is very easy for an algorithm to tell that these two pieces of content are both movie reviews and that they both review the same movie.[#1]

        footnote:(*1) 
            If the two markdown examples were stored in a CMS that kept movie-review specific {metadata}, including the name of the movie reviewed, an algorithm could do the same check using the CMS metadata. For more on the choice between locating subject domain metadata in the topic vs. in a CSM, see [#chapter.content-management] and [#chapter.metadata]. 

    But is this one criteria sufficient to declare these two reviews duplicates of each other? While they review the same movie, they express two very different opinions about it. The question of whether we want to keep both reviews is therefore a business decision. You will seldom find two independently written pieces of content that are word for word identical. You will seldom find reusable content that is word for word what you would have written yourself. The definition of duplicate content is not based on identical text, but on whether or not the content serves an identical business purpose. The constraints that determine if two pieces of content are duplicates of each other, therefore, are business rules.
    
    For some types of content, coming up with a business rule for determining if two pieces of information are duplicates is easy enough. What is the customer's birthday? A person can only have one birthday, so there is no difficulty creating a clear policy that says that a customer's birthday may only be recorded once across the organization and should be accessed from that single source whenever it is needed.[*2] 
    
    footnote:(*2)
        Stating the policy is straightforward; implementing and enforcing it may be more difficult, since it means every system or document that wants to include the customer's birthday has to be capable of retrieving it dynamically from the central data store. That requirement is also a source of complexity that has to be weighed in the balance. 
    
    Let's say we are willing to have multiple reviews of the same movie in our collection as long as they gave different opinions. To accommodate this we can change our business rule for detecting duplicate movie reviews by adding a grading system to our review structure:

    ```(sam)
    
        movie-review: Disappointing outing for the Duke
            movie-title: Rio Bravo
            5-star-rating: 2
            review-text:
                After a memorable outing in {Rio Grande}(movie) 
                and {Sands of Iwo Jima}(movie), 
                {John Wayne}(actor) turns in 
                an pedestrian performance 
                in {Rio Bravo}(movie).
        
    and\:
    
    ```(sam)
    
        movie-review: Wayne's best yet
            movie-title: Rio Bravo
            5-star-rating: 5
            review-text:
                After tiresome performances in {Rio Grande}(movie) 
                and {Sands of Iwo Jima}(movie), 
                {the Duke}(actor, "John Wayne") is brilliant 
                in {Rio Bravo}(movie).
    
    Now we can rewrite our duplicate detection rule for movie reviews to say that two reviews duplicate each other if the `movie-title` fields have the same value and the `5-star-rating` fields have the same value. By that rule, these two reviews are not duplicates because they have different '5-star-rating' values. 

    But in other cases, the business rules for determining duplication may be more difficult to express. Take a recipe for guacamole, for instance. Is guacamole a single dish for which there can only be one recipe? Then detecting duplication is easy enough. If the type of the item is "recipe" and the value of the dish field is "guacamole", then the content is duplicate.
    
    But there are many different ways in which you can prepare guacamole, some differing only slightly from one another and some presenting welcome variations that different people might like to try. Clearly a recipe site would not want eight essentially identical guacamole recipes, but nor would they want to pick one variation to the exclusion of all others. So then the question becomes, how do you decide when a recipe is an effective duplicate of another recipe and when it is a welcome variation? If you decide the variation is welcome, how do you differentiate it from other guacamole recipes in your collection? Merely adding an additional data field to the duplication detection rule may not suffice for deciding if a variant of guacamole is interesting enough to add to our collection. In the end, this may have to be a human editorial decision. 
    
    Clearly, therefore, the business rules of detecting duplication are not universal. The way you decide these questions for recipes are not the same way you decide them for API reference topics, used car reviews, movie reviews, or conceptual discussions of ideas. Duplication detection happens in the subject domain and is specific to a particular type of content about a particular type of subject. To do any kind of algorithmic duplication detection you either need content in the subject domain or in a content management system that applies subject domain metadata to the content. (More on this in [#chapter.content-management]). Whatever constraint you decide upon, the business processes and systems that ensure that these constraints are followed are not universal, but specific to each function and organization. 
    
    section: The scale of duplication

        So far we have looked at detecting duplication of whole documents. But what if the duplication we want to avoid occurs below the level of a document? In [#chapter.management] we looked at the example of a warning that was to be attached to all dangerous procedures. The duplication of that warning occurred at a much smaller scale. It was just a single element of a procedure, and could not only occur in many different documents, it could also occur multiple times within a single document. 

        Note that while the duplication of whole documents is something we generally want to eliminate, the duplication of parts of documents is often something we deliberately want to encourage. We want the identical warning to occur in every dangerous procedure so that readers are duly warned when attempting that procedure. All forms of {content reuse} are, in face, methods for deliberately duplication in the content set. With {reuse}, we are trying to eliminate duplication on the writing and content management side of the content system, while creating duplication on the publishing side. 

        When we are dealing with duplication below the scale of the document, we have to look at ways of factoring the content out of each of the documents it occurs in and inserting it back into those documents at publication time. We looked a a variety of methods for doing this in [#chapter.reuse]. But having a method for abstracting and reinserting the duplicate content is not enough. We need also need a reliable method determining when the content we need in a particular document is a duplicate of existing content, and of identifying that content. In other words, we need to determine when duplication is occurring and what the role and scope of the duplicate content is so that we can abstract it out. 

        It helps immeasurably is the duplicate content plays a wholly consistent role in the documents it belongs to. That is the case for examples such as the warning for dangerous procedures. That warning has a clear and distinct purpose and a clear anchor point in any document it appears in: a procedure that is dangerous. Without such a clear purpose and anchor point, the chances of authors remembering to include the existing content rather than create a duplicate goes down substantially. As we saw in [#chapter.management], the use of subject domain structures in the form of a compulsory `is-it-dangerous` field in a procedure structure, can fully define both the purpose and the anchor point, thus making it impossible for authors to forget or neglect to include the warning. 

        Finding a reliable anchor for detecting duplication tends to get more difficult the smaller the content unit you try to apply it to. For example, should you try to remove the duplication of sentences that occur frequently but in different contexts. Should we, for instance, try to eliminate the duplication of the phrase "Press OK." that occurs so many times in many technical documents? This phrase occurs frequently and always means the same thing. But would replacing it with a variable actually reduce complexity or make any part of the content system more reliable?  

    section: Duplication of information vs duplication of text.

        Part of the reason for not wanting to factor out "Press OK" is that anything you replace it with would probably be longer than the original, and certainly more abstract. But a big part of the reason lies in the distinction between the duplication of information and the duplication of text. The same text can occur in multiple places without actually being duplicate information. Each instance of "Press OK" in our procedures  refers to a button in a different dialog box. Those buttons all have the same name, and thus the instruction to press them is an identical piece of text, but they are still different buttons. It is entirely possible that the redesign of one of those dialog boxes could result in OK button being renamed something more specific to the function of the dialog box, such as Print or Send. Thus each instance of "Press OK" is actually a different piece of information, though it is expressed with the same text. 

        By contrast, the warning for a dangerous procedure is a single piece of information occurring in multiple contexts. It applies with equal force to all procedures that are dangerous. Of course, a procedure could go from being dangerous to not being dangerous. (A new version of the product may include a safer design.) In this case the warning should be removed. But the value of the warning remains the same for all procedures to which it applies, whereas the name of the make-it-go button for a dialog box can change independently of other dialog boxes, all of which still have make-it-go buttons. 

        If this distinction seems a little hard to get your head round, that is a good indication of how difficult it can be to detect true duplication in content. And we should reflect that when we eliminate duplicated text that is not actually duplicated information, we are actually introducing complexity into our content set -- complexity that will either make {change management} more difficult down the road, or that will get missed and end up falling through to the customer. 

        It is probably better, therefore, to stick to cases where you are very certain that the duplication you are detecting is genuinely duplication of information and not merely duplication of text. 

    section: Duplication and the level of detail 
        
        Another problem with eliminating duplication deals with the level of detail in which a subject is treated. For examples, in most Wikipedia articles on countries, there is a section on the economy of that country, and at the beginning of that section there is a link to an entire article describing the economy of that country, followed by a summary coverage of that country's economy presumably much briefer and less detailed than that provided by the main article. There may also be a brief mention of the highlights of the country's economy in the four or five context-setting paragraphs that lead most Wikipedia articles. These different levels of detail serve different user needs, and so each is a valuable contribution to the content set. 

        Content is essentially narrative, and the same fact may be mentioned in many different narratives for many different purposes. It may be elaborated on in one place, explained briefly in another, and merely mentioned in a third. But how do you remove the duplicate statement of any of the facts that appear in more than one of these places? They each present the same fact in a different way for a different purpose and audience. 
        
        Content is always designed for a particular audience, both to serve a particular need and to suit a particular background and level of knowledge. Everything we know about effective content tells us that we need to address different audiences and different tasks differently. Taking a piece of content designed for one audience and using it for all other audiences, or attempting to write generic content that takes no account of any audience's needs or tasks is certain to produce content that is significantly less effective. This is a classic case of dumping complexity on the user, though in this case it is more a case of doing it by deliberate action rather than failure to properly manage the natural complexity of content creation. 
        

    section: Duplication in less structured contexts
    
        Since duplication detection rules define when a piece of content is unique, they make it easy to determine if a piece of content exists. If you know which fields of a proposed content item define it as unique, you can query for a topic that has the same values in those fields. If you find one, you are confident the content already exists; if you don't, you can be confident that it does not exist and needs to be written. 
        
        At least, in theory you can. The problem is that not all content can be structured to the same degree. If you have a set of tightly-constrained subject-domain content types, you can be reasonably sure that if you don't find an existing item in the set of items of its type, you also won't find it in any of the other tightly-constrained types, because such content would not fit those type definitions. But what about your less constrained content? All content sets, even those that have been as tightly constrained as possible, will have some unconstrained or loosely constrained content. Some subjects just don't lend themselves to tightly constrained content types, while others might only occur once or twice in your content set, making it pointless, or, at least, expensive, to tightly constrain that information.   
        
        For example, while we know with a high degree of certainty that there is only one API reference entry on the `hello()` function of the `greetings` library, it is much harder to detect if a writer decides to insert a full description of the `hello()` function into a topic in the programmer's guide. Programmer's guides typically deal with the relationships between different APIs and other parts of the system, and on how to accomplish certain real world tasks with the system as a whole. This focus may lend itself to a reasonably strict content type for programming topics, but it does not lend itself to strict duplication detection through a few unambiguous fields like library name and function name. Detecting that the author of a programming topic has duplicated information provided by the API reference may therefore be difficult. 
        
        And it is also possible that while the programming guide author has indeed duplicated information from the API guide, it may also be the case that they needed to do so. If, for instance, they are explaining the reasons one might choose to use a function from the `salutations` library rather than the `greetings` library, then explaining the differences between the `hello()` function in each library is necessary, and necessarily involves repeating some of the information in each libraries API reference. Simply referring the reader to each API reference to compare and contrast for themselves would eliminate the duplication, but at the expense of dumping the complexity of detecting and understanding the differences onto the reader. 

        Content, by its nature, deals with the complex and irregular aspects of the world, and we cannot expect to fully remove all duplication, or everything that might or might not be duplication, depending on how you look at it, from the overall content set without creating far more complexity in the content system than one has redirected. However good it looks on paper, such attempts are likely to leave you with more unhandled complexity than they remove.

        But while this is a reason to be cautions, it is not a reason to throw up your hands an abandon the attempt to tackle duplication in your content set. There are certainly things that can be done effectively on a smaller scale. 
        
    section: Localizing duplication detection

        If detecting duplication in the general case is likely to introduce more unhandled complexity than it removes, most of the duplication that really matters occurs much more locally. The risk that your movie review collection will accidentally duplicate content in your recipe collection is pretty low. It is duplication within each collection you need to worry about, not duplication between them. 

        There are other ways to localize duplication. For instance, some organizations try to minimize duplication between the technical, training, and marketing content for each individual product. While these three types of content obviously don't all have the same content models, their individual content models can sometimes include the key fields that are part of your duplication detection rule.  

        Finally, of course, the more local the content set you are dealing with, the more likely it is that everyone involved in creating and maintain the content set will know, or at least be able to make a good guess, as to what content exists. 

    section: Reducing duplication through consistent content models

        Using highly specific content models can also help reduce duplication of content. A tightly constrained subject-domain content model, in particular, makes sure that there is a place for every piece of information and every piece of information stays in its place. There is far less scope for incidental duplication between different content types if each content types is appropriately constrained. (And this works equally well to combat the opposite of duplication, which is omission.)

        Strongly defined content types tend to be more cohesive -- meaning they cover the same piece of ground and cover it more consistently for each instance of the subject matter. Without strong types, different authors may chunk up ideas and information differently, so that topics from two different authors may partially overlap each other. Not only is partial overlap harder to detect, since there will be fewer points of similarity between the items, it is also harder to fix because each item contains different information that the user needs. Eliminating one of the duplicates means finding a place for all of the extra information it contains, a process that could potentially affect several other content items and perhaps raise other duplication detection questions. 

        Duplication can also occur even when writers know that a piece of content on the same subject and with the same purpose already exists. The writer may not think the existing content is good enough, but may not be willing to or able to track down the author of the current content to discuss it, or figure out what might be affected if they edited the current content to bring is up to standard. 

        There are two basic issue here. The first is that the writer may be correct, that the existing content is simply not good enough. In this case, a focus on creating consistent quality across the content system will go a long way towards avoiding this kind of duplication, because it will ensure that when someone does find some content they would like to reuse, it will actually be good enough to use again. 

        The second is that the writer may simply have a different view of the appropriate style or information content from the original writer. Differences of opinion about how things should be written are common, but they can be avoided if the content organization sets up well-defined constrains that define what the organization has decided is the appropriate style and the appropriate set of information to include. Here again, structured writing can help enormously by setting quality standards that are clear to writers through the constraints built into the structured writing languages they are using. Once again, this remove disputes about quality as an argument for duplicating content.  

        The use of consistent content types also ensures that when authors have a question about whether a certain subject had been covered or not, they have a much better sense about where to look for it. 

    section: Reducing the incentive to create duplicate content

        Of course, merely knowing that content already already exists is no guarantee against duplicate content being created. Even if there are down-the-road benefits to avoiding duplication, it may still be easier for an individual writer on a deadline to create duplicate content if the means for reusing the existing content are cumbersome or difficult to use and understand. In other words, if creating duplicate content is less complex than reusing existing content, it is likely that you will get lots of duplication, even if you have solved all the technical challenges of identifying and reusing content. Solving the technical challenge alone is never the point. The point is always to remove complexity from key players in the system so that it is easier for them to do the job the right way. 

        A technically less sophisticated solution that is simpler for people to use will almost always win out in the end over a more sophisticated systems that is more complex to use. Or, to put it another way, real technical sophistication in content systems is always about partitioning and directing complexity to the person with the skills, time, and resources to handle it. Capability always takes a back seat to usability, whether that is manifest in how the system is designed or in how it is used. In other words, {functional lucidity} is key to getting consistently structured data. Duplication detection rules and content reuse methods that reduce functional lucidity are likely to be self-defeating. 

    section: Less formal forms of duplication detection

        Having hard and fast rules that define duplication as two pieces of content having the same values in the same set of fields work well when the subject matter lends itself to that degree of structure. But not all content can be structured this precisely, and structured writing techniques can also be used to implement some less formal approaches to detecting duplication. These approaches are more probabilistic than certain, and may be more appropriate as {audit} tools than as something that you expect authors to do before the write anything. Still, they can be useful, and can detect cases of duplication that might not be detectable by other methods.
    
        For instance, if your content contains the kind of index entries that we looked at in [#chapter.linking], which list the subjects it covers but type and term, you might have an algorithm compare the indexing of topics across you topic set and flag any pairs of topics whose index entries contain the same type/term combinations. Often it is perfectly appropriate for topics of different types, such as a programming topic and an API reference topic, to have the same terms indexed, since these topics treat the same API functions, but in different ways. But if you find you have topics of the same type with matching or near-matching sets of index terms, that is a good clue that there may be content duplication going on.  

        You could do a similar check based on the list of subject annotations that topics contain. This is less precise than the index approach because subject annotations annotate subject that the topic refers to, rather then the subject is is about, but it is still true that if a two topics refer to many of the same subjects, they may contain duplicate content.      

        It is worth mentioning here that there are tools out there that claim to do the duplication of detection using natural language methods. I'm not going to comment on the specific capabilities of such tools, but there are a few obvious downsides to this method. The first is that it can only compare two completed texts to see if they are duplicates. They can't detect that the text you are thinking of creating already exists. The second is that they are looking for the same or similar texts, which is not the same a same or similar information.   
  
        
    section: Reducing duplication by aggregating content

        Sometimes you want to deliberately duplicate content with variations for different publications. But while you want duplication on the output side, you don't want it on the input side. 

        One way to reduce duplication in content is to aggregate information at source that will be output to different documents. We looked at an example of this in [#chapter.management-domain] when we combined alcoholic and non-alcoholic beverage matches for a recipe into a single subject-domain recipe document, allowing us to produce variations of the same recipe for _Wine Weenie_ and _The Teetotaler's Trumpet_. 

        Because a true subject domain document is simply a collection of information about a subject from which you can select items to create many different documents, you can use the subject domain to eliminate duplication on the input side of a project. For instance, if you have a product with multiple versions, you can aggregate the information about all of the versions into a single source file and then publish only the blocks that apply to a particular version. (This is an applications of the common with conditions {reuse algorithm} that we looked at in [#chapter.reuse].) 
        

    section: Duplication and the structure of content

        As we have seen, placing content in well defined structures help with duplication detection because it allows algorithms to compare values in a structure to determine if two items are duplicates or not. 

        But using consistent well-defined structures has a far deeper importance for the detection and avoidance of duplication. By dividing content into consistent blocks, it makes items of content more comparable. It is very difficult to do anything about duplication that occurs when two differently structured pieces of content overlap in the subject matter they describe. You can't simply replace one with the other because they don't cover the same ground. Only units of the same size, shape, and scope can be effectively compared to see if they are duplicates. Using well defined content structures ensures that each piece of content on a particular subject -- each movie review, recipe, API reference, feature description, or configuration task -- has the same shape, size, and scope, meaning they are comparable.

        But the effect is not simply that the units are comparable. If content is written in well defined structures, each of which has a specific job to do, the chances of creating duplicate content, even by accident, are greatly reduced. Authors and managers have a good sense of what has been created and what has not, and the structures ensure that different approaches to describing a subject don't result in partial overlaps (or omissions).  


    section: Detecting duplication before and after authoring

        Detecting and removing duplication is an important part of keeping your information collection tidy. However, detecting duplication after it already is exists still means that you have paid writers to write the same content twice. Ideally you would like to to detect the duplication before the writer does all the work of creating the duplicate content. 

        The extend to which you can do this depends greatly on how structured your content types are. Let's look briefly at some scenarios.

        * The ideal scenario is that the content model factors out the repeated content altogether. If our procedure structure contains a compulsory `is-it`dangerous` field, then the text of any warning you might want to add has been completely factored out. All the author has to do is correctly fill out the `is-it-dangerous` field. Aggregating material from different output documents into a single input-side document is also an example of this approach. 

        * The next best scenario is that the authoring or content management system refuses to let the writer create the content record in the first place. For instance, if in order to create a movie review in your CMS, the writer must supply values for the movie name and five star rating fields first, before entering any content, then the CMS can reject the attempt before the author wastes any time writing a new review. However, while this kind of interface is common in the database world, it is not common for content, in large part because of the variety of different content types that you would have to create this kind of interface for, and also because this is simply not the way writers are used to working. 

        * The next best thing after that is that the system allows the writer to query the current collection to determine if there is already a piece of content with those values in those fields. The main difference here is that the writer has to voluntarily initiate the query, rather than the CMS doing it as a condition of accepting the content. This obviously requires some kind of interface for allowing writers to run these queries. Most writers don't have to learn how to write them, though, as a content engineer of information architect can provide a predefined set of queries that match the organization's business rules for determining duplication. This approach can be implemented using a much wider variety of tools and does not require any form of database or content management system. 

        * After this the next best thing is to use a search engine to try to find duplicates. This obviously is time consuming and uncertain, so it should be supplemented by regular auditing of the content set, aided by various structured writing techniques, to catch and remove accidental duplication. 

        * Finally, it may be appropriate to accept that certain types of duplication are just to hard to define and detect and your content system will be simpler overall if you simply tolerate them. 

        

    section: Optimize the whole, not the parts

        Overall, then, the elimination of all duplication from a content set is not possible. The variations on how, when, and why you mentions particular facts in content, and the various purposes and audiences for which you describe different subjects make it impossible to have hard and fast rules about what is an is not a duplicate. Too zealous a pursuit of a single source of truth can result in the elimination of valuable differences in information and presentation -- dumping the complexity of managing all the instances of a description of a subject on the head of the reader in the form of excessively generic content that is harder for them to understand and may not give them all the information they need. 

        However, moving as much of your content as possible to the subject domain with give you a fair amount of useful duplication removal without any additional effort -- the subject domain tends to enable additional algorithms from the same structures and without additional effort or expense.         
   
     