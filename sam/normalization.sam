chapter:(#chapter.normalization) Normalization
    
    <<<(annotations.sam)
    
    index: type, term
        concept, single source of truth
        concept, single sourcing
        
    A significant source of complexity in any content organization is simply determining if a piece of content already exists or not. This problem obviously affects any attempt at content reuse, since every time you consciously reuse content you have to determine if reusable content exists. The longer it takes to determine if a piece of content already exists, the longer, and therefore more expensive, each instance of content reuse becomes. (And bear in mind that the cost of determining if reusable content exists is incurred every time the writer looks for it, even if they don't find it, but any saving associated with reuse are realized only when reusable content is found.) 
    
    Even if reuse is not a major part of your process, however, determining if content already exists is still important, as it helps you avoid recreating content you already have, and all of the complexity that goes with maintaining two copies of the same thing. 
    
    Creating a formal system for ensuring that content only exists once is called normalization. Normalization is not just about eliminating duplicates from an information set -- that is merely housekeeping. It is about creating a set of constraints by which duplication can be defined, detected, and eliminated. Normalization is not something that happens after the fact, after your data types are defined. It is something that is integral to the design process itself. It is about designing each data type in your system so that you can tell at once if one piece of data is a duplicate of another. Essentially is says that if item X matches item Y in aspects A, B, and C, then it is a duplicate. And this means that aspects A, B, and C have to be formalized as part of the model of X and Y if we are going to implement any algorithmic support for normalization, and even if we are going to have clear criteria for human decision making on what is or is not a duplicate. 
    
    Consider two movie reviews written in markdown:
    
    ```(markdown)
    
        Disappointing outing for the Duke
        ================================
        
        After a memorable outing in _Rio Grande_ 
        and _Sands of Iwo Jima_, John Wayne 
        turns in an pedestrian performance 
        in _Rio Bravo_.
        
    and:
    
    ```(markdown)
    
        Wayne's best yet
        ================
        
        After tiresome performances in _Rio Grande_ 
        and _Sands of Iwo Jima_, the Duke is brilliant 
        in _Rio Bravo_.
        
    Do we have two reviews of _Rio Bravo_ or not? A human reading the text can tell easily enough, but an algorithms would have no way to tell. But suppose these same reviews were written in a subject domain movie review language:

    ```(markdown)
    
        movie-review: Disappointing outing for the Duke
            movie: Rio Bravo
            review-text:
                After a memorable outing in {Rio Grande}(movie) 
                and {Sands of Iwo Jima}(movie), 
                {John Wayne}(actor) turns in 
                an pedestrian performance 
                in {Rio Bravo}(movie).
        
    and:
    
    ```(markdown)
    
        movie-review: Wayne's best yet
            movie-title: Rio Bravo
            review-text:
                After tiresome performances in {Rio Grande}(movie) 
                and {Sands of Iwo Jima}(movie), 
                {the Duke}(actor, "John Wayne") is brilliant 
                in {Rio Bravo}(movie).
    
    Now it is very easy to tell that these two pieces of content are both movie reviews and that they both review the same movie. However, since they each take a very different view of the movie, it is fair to ask if the fact that they are both movie reviews about the same film is enough to consider them duplicates. That is a business decision that the content owner has to make. 
    
    
    You can do content reuse without the assurance that your content set is free from duplication, and organizations frequently do. The task of finding some content that meets your needs and can be reused is very different from the task of assuring that there is only one piece of content in existence that can meet a specific need. Finding content to reuse is a task for the writer doing the reusing. Ensuring that there is only one source for that content is a task for the writer of every piece of original content that is created. They have to make sure when they create and store some piece of content that there really is no other piece of content serving the same purpose. 
    
    There is a clear partitioning of responsibility here. Normalization partitions the problem of ensuring that only one copy of something exists from the problem of finding content that can be reused. Unfortunately, normalizing your content set is a much more difficult task than simply doing ad hoc reuse. Many attempts at creating a reuse system founder because people focus on establishing ad hoc reuse capability without giving any thought to normalization. That neglected complexity has to do somewhere, and it tends to accumulate in the system until it brings it to a halt. 
    
    But the problem with establishing and maintaining a normalized content set is defining exactly when one piece of content is the duplicate of another.  For some types of content, this question is easy to answer. What is the customer's birthday? A person can only have one birthday, so there is no difficulty creating a clear policy that says that a customer's birthday may only be recorded once across the organization and should be accessed from that single source whenever it is needed.[*1] 
    
     footnote:(*1)
        Stating the policy is straightforward; implementing and enforcing it may be more difficult, since it means every system or document that wants to include the customer's birthday has to be capable of retrieving it dynamically from the central data story.
    
    Normalization rules are constraints, and the set of constraints that defines a piece of information as unique are not universal. The definition of what constitutes unique for different pieces of information is complex and specific to the subject matter at hand. Take a recipe for guacamole, for instance. Is guacamole a single dish for which there can only be one recipe? Then normalization is easy enough. If the type of the item is "recipe" and the value of the dish field is "guacamole", then the content is duplicate.
    
    But there are many different ways in which you can prepare guacamole, some differing only slightly from one another and some presenting welcome variations that different people might like to try. Clearly a recipe site would not want eight essentially identical guacamole recipes, but nor would they want to pick one variation to the exclusion of all others. So then the question becomes, how do you decide when a recipe is an effective duplicate of another text and when the subject is a welcome variation? If you decide the variation is welcome, how do you differentiate it from other guacamole recipes in your collection? 
    
    If we decided that we were willing to have multiple reviews of the same film in out collection as long as they gave different opinions, we might add a grading system to our review structure, and allow more than one review of the same movie to be created as long as each gave the film a different rating:

    ```(markdown)
    
        movie-review: Disappointing outing for the Duke
            movie: Rio Bravo
            5star-rating: 2
            review-text:
                After a memorable outing in {Rio Grande}(movie) 
                and {Sands of Iwo Jima}(movie), 
                {John Wayne}(actor) turns in 
                an pedestrian performance 
                in {Rio Bravo}(movie).
        
    and:
    
    ```(markdown)
    
        movie-review: Wayne's best yet
            movie-title: Rio Bravo
            5star-rating: 5
            review-text:
                After tiresome performances in {Rio Grande}(movie) 
                and {Sands of Iwo Jima}(movie), 
                {the Duke}(actor, "John Wayne") is brilliant 
                in {Rio Bravo}(movie).
    
    Now if our normalization rule is that a piece is a duplicate if it is a movie-review and the movie-title is the same and the 5star-rating is the same, then these two reviews are not duplicates because they have different 5star-ratings. (Note that you don't have to include every field in your type definition in your normalization rule, just those fields that determine if a piece of content is a duplicate or not according to your business rules.)

    Clearly the answers to these questions are not universal. The way you decide these questions for recipes are not the same way you decide them for API reference topics, used car reviews, movie reviews, or conceptual discussions of ideas. Normalization happens in the subject domain and is specific to a particular type of content about a particular type of subject. To do any kind of content normalization you either need content in the subject domain or in a content management system that applies subject domain metadata to the content. (More on this in [#chapter.content-management]). Whatever constraint you decide upon, the business processes and systems that ensure that these constraints are followed are not universal, but specific to each function and organization. 
    
    section: The limits of content normalization

        Content, by its nature, deals with those subject that do not fit neatly into rows and columns, and thus cannot be formally normalized according to database rules. Where databases describe and normalize relationships formally by use of records and keys, content describes relationships informally in prose. In particular, content deals with complex, unique, and potentially ambiguous relationships that could not be reduced to rows and tables at any reasonable cost. 
        
        Content is not a granular as typical database data. Content is essentially narrative, and the same fact may be mentioned in many different narratives for many different purposes. It may be elaborated on in one place, explained briefly in another, and merely mentioned in a third. For examples, in most Wikipedia articles on countries, there is a section on the economy of that country, and at the beginning of that section there is a link to an entire article describing the economy of that country, followed by a summary coverage of that country's economy presumably much briefer and less detailed than that provided by the main article. There may also be a brief mention of the highlights of the country's economy in the four or five context-setting paragraphs that lead most Wikipedia articles. These different levels of detail serve different user needs, and so each is valuable contributions to the content set. But how do you normalize the statement of any of the facts that appear in more than one of these places? They each present the same fact in a different way for a different purpose and audience. 
        
        Content is always designed for a particular audience, both to serve a particular need and to suit a particular background and level of knowledge. Everything we know about effective content tells us that we need to address different audiences and different tasks differently. Taking a piece of content designed for one audience and using it for all other audiences, or attempting to write generic content that takes no account of any audience's needs or tasks is certain to produce content that is significantly less effective. This is a classic case of dumping complexity on the user, though in this case it is more a case of doing it by deliberate action rather than failure to properly manage the natural complexity of content creation. 
        
    section: Examples of content normalization

        The way we achieve the normalization of a fact is by abstracting it out of all of the expressions of that fact. Abstracting variants from invariants is, of course, what {structured writing}(concept) is about. A simple case of abstracting out a fact is the replacement of certain pieces of text by variables. In technical communication, it is a frequent practice to replace things like company names and product names by variables:
        
        ```(sam)
            Thank you for buying >($company-name)'s >($product-name). 
            We hope you enjoy it for many years to come.
            
        There are a couple of reasons for doing this. One is that company names and product names often have a precise formal variant that the marketing department wants everyone to use, and one or more informal variants that people actually use. There is constraint here: use the formal name for company and product not the common name. But because authors are more used to using the common name (like everyone else) they are likely to slip it in without thinking. And if they do remember that they are supposed to use the formal name, they may not remember it correctly and so use the wrong form. Using the variable instead ensures that the correct version of the name is used.
        
        The second reason is that products and companies are sometimes re-branded. The names change. If the names are written into the source in a hundred places, you will need to find those hundred instances and change them all. If a variable is used, you just need to change the definition of the variable and you don't have to touch the source content at all. For more on this approach and its pitfalls, see [#chapter.change].
        
        This is clearly a very tactical approach to normalization. We have a single source of truth for the name of the company and the name of the product. We don't have a single source of truth for every name of every subject we mention. We only do it for certain names we consider critical or subject to change.      
        
        Other examples of normalization concern the normalization of entire content units. Obvious and natural examples come from reference material. For example, an API reference entry is normalized based on the name of the library it belongs to and the name of the function it is describing. If you have an entry for the `hello()` function of the `greetings` library, then it should be easy to detect if someone tries to create another entry for that function. With some repository systems, it might be impossible to create the duplicate at all.   

        A great deal of your ability to normalize your content comes down to your ability to clearly name the things you are writing about. API library and functions are clearly and unambiguously named. Is the set of your product's feature names as clear and unambiguous? Sometimes features correspond to buttons you can press on the interface, but sometimes they are more marketing terms that refer to business problems solved. Can you agree across the company on what such feature names are? Is that even desirable, or would one list of feature names market the product better to one consumer group and a different list to a different group?   

        The ability to name things consistently is a compounding factor in any attempt to normalize content, or indeed to detect or limit duplication by any means. Duplication can be hidden by the same fact being referred to by different names. The elimination of duplication can be made more difficult when the same word refers to more than one truth. But merely standardizing terms is not a solution, because different readers frequently use different terms to refer to the same ideas, or use the same term to refer to different ideas. Structured writing techniques can help here by putting such ambiguous terms into contexts where their meaning may be easier to delineate. I will look at this in more detail in [#chapter.taxonomy].


    section: Normalization and discovery
    
        Since normalization rules define when a piece of content is unique, they make it easy to determine if a piece of content exists. If you know which fields of a proposed content item are use to normalize it in the content set, you can query for a topic that has the same values in those fields. If you find one, you are confident the content already exists; if you don't, you can be confident that it does not exist and needs to be written. 
        
        At least, in theory you can. The problem is that not all content can be normalized to the same degree. If you have a set of tightly-constrained subject-domain content types, you can be reasonably sure that if you don't find an existing item in the set of items of its type, you also won't find it in any of the other tightly-constrained types, because such content would not fit those type definitions. But what about your less constrained content? All content sets, even those that have been as tightly constrained as possible, will have some unconstrained or loosely constrained content. Some subjects just don't lend themselves to tightly constrained content types, while others might only occur once or twice in your content set, making it pointless, or, at least, expensive, to tightly constrain that information.   
        
        For example, while we know with a high degree of certainty that there is only one API reference entry on the `hello()` function of the `greetings` library, it is much harder to detect if a writer decides to insert a full description of the `hello()` function into a topic in the programmer's guide. Programmer's guides typically deal with the relationships between different APIs and other parts of the system, and on how to accomplish certain real world tasks with the system as a whole. This focus may lend it self to a reasonably strict content type for programming topics, but it does not lend itself to strict normalization through a few unambiguous fields like library name and function name. Detecting that the author of a programming topic has duplicated information provided by the API reference may therefore be difficult. 
        
        And it is also possible that while the programming guide author has indeed duplicated information from the API guide, it may also be the case that they needed to do so. If, for instance, they are explaining the reasons one might choose to use a function from the `salutations` library rather than the `greetings` library, then explaining the differences between the `hello()` function in each library is necessary, and necessarily involves repeating some of the information in each libraries API reference. Simply referring the reader to each API reference to compare and contrast for themselves would eliminate the duplication, but at the expense of dumping the complexity of detecting and understanding the differences onto the reader. 
        
    section: Proximity detection
    
        A less rigorous approach to detecting potentially duplicate content is what we might call "proximity detection" -- pieces of content with several points of similarity which might indicate that they are covering the same subject, or at least that their coverage overlaps. 
        
        For instance, you might look at the list of terms indexed in each topic, or the list of subject annotations they contain (pretty much the same thing, in the document and subject domains respectively) and if there are matches above a certain threshold you might inspect them to see if there are duplications. 
        
        Proximity detection is much easier when all of your content items are as constrained to subject-domain content types as is reasonable for their subject matter. Strongly defined content types tend to be more cohesive -- meaning they cover the same piece of ground and cover it more consistently for each instance of the subject matter. Without strong types, different authors may chunk up ideas and information differently, so that topics from two different authors may partially overlap each other. Not only is partial overlap harder to detect, since there will be fewer points of similarity between the items, it is also harder to fix because each item contains different information that the user needs. Eliminating one of the duplicates means find a place for all of the extra information it contains, a process that could potentially affect several other content items and perhaps raise other normalization questions. 
        
    section: Normalization via aggregation

        One way to normalize content is to aggregate larger sets of information into source content units. For instance, if you have a product with multiple versions, you can aggregate the information about all of the versions into a single source file and then publish only the blocks that apply to a particular version. (This is an applications of the common with conditions {reuse algorithm} that we looked at in [#chapter.reuse].) 
        
        Doing this reliably is greatly aided by moving content to the subject domain where you can effectively create a small database of product facts in a subject-domain source file and have algorithms select and publish those that belong to each version of the product. 

        Overall, then, a full normalization of a content set is not possible. The variations on how, when, and why you mentions particular facts in content, and the various purposes and audiences for which you describe different subjects make it impossible to have hard and fast rules about what is an is not a duplicate. Too zealous a pursuit of normalization can result in the elimination of valuable differences in information and presentation -- dumping the complexity of managing all the instances of a description of a subject on the head of the reader in the form of excessively generic content that is harder for them to understand and may not give them all the information they need. 

        However, moving as much of your content as possible to the subject domain with give you a fair amount of useful normalization without any additional effort -- once again, the subject domain enables additional algorithms from the same structures and without additional effort or expense.         


        
        
        
    
     