<?xml version="1.0"?>
<db:chapter xmlns:db="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:strings="http://exslt.org/strings" version="5.0" xml:id="chapter.duplication"><db:title>Avoiding duplication</db:title>




<db:para>Duplicate content is a significant source of complexity in any content organization. Not only is it expensive, but different versions of the content may not agree with each other, which creates complexity for readers. When the subject matter changes, only one of the duplicates may get updated, causing further drift between the two versions (and updating two versions is twice the work). Duplicate and near-duplicate content may also affect search results.<db:footnote>
<db:para>The issue of how search engines treat duplicate content is complex. A lot of that complexity has to do with how duplicate is defined and how the search engine interprets the intent of the duplication it finds. For example, a search engine may filter duplicate or near duplicate content and list only one of the duplicated pages, which may not be the page your reader is looking for. For more information see <db:link xlink:href="https://www.hobo-web.co.uk/duplicate-content-problems/">https://www.hobo-web.co.uk/duplicate-content-problems/</db:link></db:para>
</db:footnote> The need to detect duplicate content is therefore a major source of complexity in the content system.</db:para>

<db:para>Writers don’t hold the entirety of the organization’s content collection in their heads, so when they decide to write something, they may not know if that content already exists somewhere in the content set. Obviously, writers should look for duplicate content before they write, but it can be difficult and time consuming to find out if content on a particular subject for a particular audience already exists.</db:para>
<db:para>Indeed, even defining what constitutes duplication, let alone detecting that it exists, is not easy. Structured writing techniques can help, to a point, but don’t get carried way in the attempt to eliminate duplication; you can easily do more harm than good and inject more complexity into the content system than you remove.</db:para>
<db:para>To avoid creating duplicate content, writers need to determine as quickly as possible whether a piece of content already exists or not. This check takes place countless times, so even if the cost of a single check is small, the cumulative cost can be large, significantly slowing down your content system. Small costs with high repetition rates are often the hardest to detect and least satisfying to fix, whereas the wins from detecting duplication are often more visible and more satisfying. But it is important to make sure that your efforts to limit content duplication don’t cost more than they save.</db:para>
<db:para>This problem obviously affects any attempt at <db:indexterm><db:primary>content reuse algorithm</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>content reuse algorithm</db:secondary></db:indexterm>content reuse, since every time you set out to reuse content, you have to determine if reusable content exists. The longer that effort takes, the longer, and therefore more expensive, each instance of content reuse becomes. And bear in mind that you incur this cost every time a writer looks for duplicate content, whether it exists or not, but you realize any savings only when reusable content is found. Indeed, failed attempts are often more expensive, since the writer has to exhaust all possibilities, whereas successful attempts end as soon as relevant content is found.</db:para>
<db:para>To implement a system for detecting and eliminating duplicate content, you need a clear plan for exactly how writers are going to detect duplicate content or find content to <db:indexterm><db:primary>reuse</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>reuse</db:secondary></db:indexterm>reuse, and you need to ensure that the overhead of using such a system does not outweigh the benefits it provides.</db:para>
<db:para>Given the cost of finding duplicate content by hand, handing the task over to algorithms is highly desirable. But to hand the task over to algorithms, you need to define what constitutes duplication in precise terms and preserve the information that reflects those terms.</db:para>
<db:para>Creating a formal system for ensuring that content only exists once is sometimes called establishing a “single source of truth.” Having a “single source of truth” does not mean that there is only one place or system from which all truths come; it means that every significant truth you manage is stored only once. Different truths can certainly be stored in different places, but you need to make sure that the same truth is not stored in two different places or two times in the same place.</db:para>
<db:para>A formal system for detecting duplication essentially means establishing a set of constraints that allow you to define and detect duplication. In other words, you need a set of rules that says that if item X matches item Y in aspects A, B, and C, then X is a duplicate of Y. (These are rhetorical rules – rhetoric is at the heart of process here as elsewhere.) These rules constitute an algorithm for detecting duplication. However, for a machine to execute this algorithm, aspects A, B, and C must either be a defined part of the content model for X and Y or be stored with X and Y. The content needs to retain this information in explicit rhetorical structures.</db:para>
<db:para>Where no such formal constraints exist, writers can use a search engine to look for existing content. Although a search engine can find existing content on a subject, search engines are not precise enough to detect duplication reliably. They may miss an actual duplicate because of a variation in <db:indexterm><db:primary>terminology</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>terminology</db:secondary></db:indexterm>terminology or return possible matches that are not duplicates. The writer must assess each match to determine if a duplicate exists. That takes a lot of time, and to ensure there is no duplication, the writer must perform this task before writing anything. This is a tremendous amount of overhead to impose on the content system.</db:para>

<?dbfo-need height="2in"?>

<db:para>How exactly do you construct a set of constraints for detecting duplicate content? First you need to define duplicate content. A reasonable general definition is the following:</db:para>
<db:blockquote>
<db:para>Duplicate content is content that describes the same subject matter to the same audience for the same purpose.</db:para>
</db:blockquote>
<db:para>All three aspects of this definition matter. Subject matter obviously matters. You communicate to achieve an objective, so purpose matters. You communicate with a variety of people, so audience matters. Ignoring any one of these three aspects could result in serious quality problems.</db:para>
<db:para>Let’s consider two potentially duplicate pieces of content and look at how to establish a formal system for deciding if they are duplicates. Consider two movie reviews, written in Markdown:</db:para>
<db:programlisting language="markdown">
Disappointing outing for the Duke
================================

After a memorable outing in _Rio Grande_ 
and _Sands of Iwo Jima_, John Wayne 
turns in a pedestrian performance 
in _Rio Bravo_.
</db:programlisting>
<db:para>and:</db:para>
<db:programlisting language="markdown">
Wayne's best yet
================

After tiresome performances in _Rio Grande_ 
and _Sands of Iwo Jima_, the Duke is brilliant 
in _Rio Bravo_.
</db:programlisting>
<db:para>Let’s examine these according to our three criteria: First, do we have two pieces on the same subject? The subject of each is a movie. But is it the same movie? A human reading the text can easily tell that the subject of both pieces is the movie <db:emphasis>Rio Bravo</db:emphasis>, but an algorithm would have no way to tell, since nothing in the markup of either review directly identifies which movie is being reviewed.<db:footnote>
<db:para>I am talking here, and throughout this book, about conventional algorithms. The question of whether an AI algorithm could tell the difference is out of scope here. When and if AIs become sophisticated enough to read and write content effectively in human language, the partitioning of the content system and the role of structured writing in that system will become very different. So, algorithm here means the kind of algorithm that a content engineer or information architect with the appropriate skill set could design and code in a reasonable amount of time with ordinary programming tools.</db:para>
</db:footnote> Even if it could recognize the names of movies in the text, it would have no way to tell which one was the subject of the review.</db:para>

<db:para>But suppose these same reviews were written in <db:indexterm><db:primary>SAM</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>SAM</db:secondary></db:indexterm>SAM in a subject-domain movie review language:</db:para>
<db:programlisting language="sam">
movie-review: Disappointing outing for the Duke
    movie-title: Rio Bravo
    review-text:
        After a memorable outing in {Rio Grande}(movie) 
        and {Sands of Iwo Jima}(movie), 
        {John Wayne}(actor) turns in 
        a pedestrian performance 
        in {Rio Bravo}(movie).
</db:programlisting>
<db:para>and:</db:para>
<db:programlisting language="sam">
movie-review: Wayne's best yet
    movie-title: Rio Bravo
    review-text:
        After tiresome performances in {Rio Grande}(movie) 
        and {Sands of Iwo Jima}(movie), 
        {the Duke}(actor, "John Wayne") is brilliant 
        in {Rio Bravo}(movie).
</db:programlisting>
<db:para>Now it is easy for an algorithm to tell that these two pieces of content are both movie reviews and that they are both reviews of <db:emphasis>Rio Bravo</db:emphasis>.<db:footnote>
<db:para>If the two Markdown examples were stored in a CMS that kept movie-review specific metadata, including the name of the movie reviewed, an algorithm could do the same check using the CMS metadata. For more on the choice between locating subject-domain metadata in the document vs. in a CMS, see <db:xref linkend="chapter.content-management"/> and <db:xref linkend="chapter.metadata"/>.</db:para>
</db:footnote></db:para>

<db:para>What about audience? There are multiple audiences for movie criticism. There are moviegoers and Netflix subscribers who simply want to decide what movie to watch on a Saturday night, but there are also academic film students who may want a detailed analysis of <db:emphasis>Rio Bravo</db:emphasis> according to some school of film criticism.</db:para>
<db:para>With the two Markdown versions, it is impossible for an algorithm to tell whether one of these reviews is written for moviegoers and the other for film students. The structured versions do identify the audience, but not in the form of an <db:code>audience</db:code> field in the markup. Instead, the entire <db:code>movie-review</db:code> <db:indexterm><db:primary>markup language</db:primary></db:indexterm><db:indexterm><db:primary>tool</db:primary><db:secondary>markup language</db:secondary></db:indexterm>markup language is intended for writing reviews for moviegoers. To write a review for film students, you would not use the same markup language, you would create a separate <db:indexterm><db:primary>document type</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>document type</db:secondary></db:indexterm>document type, which you might call <db:code>film-study</db:code>. Thus, you can tell from the document type alone whether two reviews are meant for the same audience.</db:para>
<db:para>The <db:indexterm><db:primary>subject domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject domain</db:secondary></db:indexterm>subject domain is concerned with the rhetoric of a piece of content: which pieces of information need to be presented to the reader in order to achieve its purpose. Since the purpose of any piece of content is to serve the needs of a particular reader seeking a particular goal, the definition of the reader is inherent in the definition of the <db:indexterm><db:primary>subject-domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject-domain</db:secondary></db:indexterm>subject-domain document type itself. It is the type of document that supports this person achieving this goal. If you were recording your content in the document domain, however, you might want to include a field to identify the intended audience of the piece, since audience is not implied by a <db:indexterm><db:primary>document-domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>document-domain</db:secondary></db:indexterm>document-domain structure.</db:para>
<db:para>But what about purpose? Although they review the same movie, the two reviews express two different opinions. Does that constitute a difference in purpose? You could argue that one review encourages readers to see the movie and the other encourages them not to. Or you could argue that having the two reviews together gives readers a more complete picture and more options to inform their viewing choices.</db:para>
<db:para>Ultimately, the decision comes down to whether keeping both versions serves a purpose for the organization that is publishing them. For example, does the organization want to present itself as having a single firm opinion on every movie or does it want to present itself as a neutral arbiter that presents a variety of opinions?</db:para>
<db:para>Therefore, whether or not you keep both reviews is a business decision. You will seldom find two independently written pieces of content that are word-for-word identical, and you will seldom find reusable content that is exactly what you would have written yourself.</db:para>
<db:para>The definition of duplicate content is not based on identical text; it is based on whether or not the content serves an identical rhetorical purpose, which is to say an identical business purpose. The constraints that determine if two pieces of content are duplicates of each other, therefore, are business rules.</db:para>
<db:para>Let’s say you are willing to have multiple reviews of the same movie in your collection as long as they give different opinions. To accommodate this you can change your business rule for detecting duplicate movie reviews by adding a grading system to your review structure. Now you can rewrite your duplicate detection rule for movie reviews to say that two reviews duplicate each other if the <db:code>movie-title</db:code> fields have the same value and the <db:code>5-star-rating</db:code> fields have the same value.</db:para>

<?dbfo-need height="4in"?>

<db:para>By that rule, these two reviews are not duplicates, because their <db:code>5-star-rating</db:code> values differ:</db:para>
<db:programlisting language="sam">
movie-review: Disappointing outing for the Duke
    movie-title: Rio Bravo
    5-star-rating: 2
    review-text:
        After a memorable outing in {Rio Grande}(movie) and
        {Sands of Iwo Jima}(movie), {John Wayne}(actor) turns
        in a pedestrian performance in {Rio Bravo}(movie).
</db:programlisting>
<db:para>and:</db:para>
<db:programlisting language="sam">
movie-review: Wayne's best yet
    movie-title: Rio Bravo
    5-star-rating: 5
    review-text:
        After tiresome performances in {Rio Grande}(movie) and
        {Sands of Iwo Jima}(movie), {the Duke}(actor, "John Wayne")
        is brilliant  in {Rio Bravo}(movie).
</db:programlisting>
<db:para>However, in other cases, the business rules for determining duplication can be more difficult to express. For example, consider a recipe for guacamole. Is guacamole a single dish for which there can only be one recipe? If so, then detecting duplication is easy. If the type of the item is <db:code>recipe</db:code> and the value of the dish field is <db:code>guacamole</db:code>, then the content is duplicate.</db:para>
<db:para>But there are many different ways to prepare guacamole. Some differ only slightly from one another, but some present welcome variations that people might like to try. Clearly a recipe site would not want eight essentially identical guacamole recipes, but neither would they want to pick one variation to the exclusion of all others. So the question becomes, how do you decide when a recipe is an effective duplicate of an existing recipe and when it is a welcome variation? If you decide that a particular recipe is a welcome variation, how do you differentiate it from other guacamole recipes in your collection? In some cases, for example spicy versus mild, you could add another data field, but in other cases, for example different secondary ingredients, the choice may have to be a human editorial decision.</db:para>
<db:para>Clearly, the business rules for detecting duplication are not universal. The method you use to detect duplication for recipes is not the same method you would use for API reference topics, used car reviews, movie reviews, or conceptual discussions. Duplication detection happens in the <db:indexterm><db:primary>subject domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject domain</db:secondary></db:indexterm>subject domain and is specific to a particular type of content about a particular subject serving a specific business purpose. Whatever constraints you choose, the business processes and systems that ensure that those constraints are followed are specific to each function and organization.</db:para>
<db:section><db:title>The scale of duplication</db:title>


<db:para>So far, we have looked at detecting duplication of whole documents. But what if you want to avoid duplication that occurs below the level of a document? In <db:xref linkend="chapter.management-domain"/> I looked at the example of a warning that was to be attached to all dangerous procedures. The duplication of that warning occurred at a much smaller scale. It was just a single structure within a procedure. Not only could it occur in many different documents, it could also occur multiple times within a single document.</db:para>
<db:para>Although you generally want to eliminate duplication of whole documents, you often need to duplicate parts of documents. For example, you want the identical warning to occur in every dangerous procedure so that readers are duly warned when attempting that procedure. In fact, all forms of <db:indexterm><db:primary>content reuse algorithm</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>content reuse algorithm</db:secondary></db:indexterm>content reuse are methods for deliberately duplicating content in multiple places in the content set.</db:para>
<db:para><db:indexterm><db:primary>Reuse</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>Reuse</db:secondary></db:indexterm>Reuse techniques are about eliminating duplication on the writing and content management side of the content system, while creating duplication on the publishing side. However, although reuse techniques give you a mechanism for inserting duplicate content in various locations, these techniques can easily go off the rails if you don’t clearly define what is and is not duplicate content.</db:para>
<db:para>It helps greatly if the duplicate content plays a consistent role. That is the case for the warning for dangerous procedures described in <db:xref linkend="chapter.management-domain"/>. That warning has a clear purpose and a clear anchor point in any document it appears in.<db:footnote>
<db:para>An <db:emphasis>anchor point</db:emphasis> is a defined location in your content where reused text is inserted or referenced. Using the warning example, in a document-domain structure, the anchor point is the position in the text where you would place markup that identifies where the warning should be inserted, and in a subject-domain structure, the anchor point would be the location of the <db:code>is-it-dangerous</db:code> field.</db:para>
</db:footnote> Without a clear purpose and anchor point, the chances of writers remembering to include the existing content, rather than creating a duplicate, goes down substantially. A subject-domain structure – the mandatory <db:code>is-it-dangerous</db:code> field – can fully define both the purpose and the anchor point, thus making it impossible for writers to forget or neglect to include the warning. This is a clear example of rhetorical structure supporting reliable process.</db:para>

<db:para>Finding a reliable anchor for detecting duplication gets more difficult the smaller the content unit you try to apply it to. For example, should you try to remove duplicate sentences that occur frequently but in different contexts? The phrase “Press OK” occurs frequently in technical document, and it always means the same thing. However, would replacing it with a variable reduce complexity or make any part of the content system more reliable? The number of repeated words, phrases, and sentences in the average technical document is very high and treating them all as duplicate content is obviously not feasible, nor would it solve an obvious problem. So, when should you regard a piece of content below the document level as duplicate?</db:para>
</db:section>
<db:section><db:title>Duplication of information vs duplication of text.</db:title>


<db:para>One reason for not wanting to factor out “Press OK” is that anything you replace it with would probably be longer and certainly more abstract. But the most important reason lies in the distinction between duplicating <db:emphasis>information</db:emphasis> and duplicating <db:emphasis>text</db:emphasis>. The same text can occur in multiple places without actually being duplicate information. Each instance of “Press OK” refers to a button in a different dialog box. Those buttons all have the same name, and thus, the instruction to press them is identical. However, they are different buttons. It is entirely possible that a redesign of one of those dialog boxes could result in the OK button being renamed to something more specific to the function of the dialog box, such as Print or Send. Thus, each instance of “Press OK” is a different piece of information, even though it is expressed with the same text. (See <db:xref linkend="chapter.modeling"/> for an approach that eliminates the duplication of “Press OK” without running into this issue.)</db:para>
<db:para>By contrast, the warning for a dangerous procedure is a single piece of information occurring in multiple contexts.<db:footnote>
<db:para>In the example we have been looking at. Of course, some procedures are dangerous in unique ways and require unique warnings.</db:para>
</db:footnote> It applies with equal force to all procedures that are dangerous. Of course, a procedure could go from being dangerous to not being dangerous. (A new version of the product may include a safer design.) In this case the warning should be removed. But the value of the warning remains the same for all procedures to which it applies, whereas the name of the make-it-go button for a dialog box can change independently of other dialog boxes, all of which still have make-it-go buttons. That is, it is a difference in applicability, not a difference in content.</db:para>

<db:para>If this distinction seems a little hard to get your head around, that is a good indication of how difficult it can be to detect true duplication in content. And when you eliminate duplicated text that is not duplicated information, you introduce complexity that will either make <db:indexterm><db:primary>change management</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>change management</db:secondary></db:indexterm>change management more difficult down the road or get missed and damage rhetoric.</db:para>
<db:para>It is probably better, therefore, to stick to cases where you are certain that the duplication you are detecting is genuinely duplication of information and not merely duplication of text.</db:para>
</db:section>
<db:section><db:title>Duplication and the level of detail</db:title>


<db:para>Another problem with identifying duplication deals with the level of detail in which a subject is treated. For example, most Wikipedia articles on countries contain a section on the economy of that country. At the beginning of that section there is a link to an entire article describing the economy of that country followed by a brief summary, which is less detailed. There may also be a brief mention of the highlights of the country’s economy in the four or five context-setting paragraphs that lead most Wikipedia articles. These different levels of detail serve different user needs, and therefore, each is a valuable contribution to the content set. In other words, they are not duplicates, because, although they address the same subject, they serve different audiences and different purposes.</db:para>
<db:para>Everything we know about effective rhetoric tells us that you need to address different audiences and different tasks differently. Taking a piece of content designed for one audience and using it for all other audiences – or attempting to write generic content that takes no account of any audience’s needs or tasks – is certain to produce content that is significantly less effective.</db:para>
<db:para>The question, then, is whether differences in level of detail reflect different audiences and purposes. While it may seem like this is a distinction that any writer should be able to make via inspection, it is often quite a difficult distinction, because it relies on the writer understanding the audience and purpose of the content being examined.</db:para>
<db:para>A writer who thoroughly understands one audience and purpose may look at a piece of content designed for a different audience and purpose and fail to recognize the difference. The writer may not even know that the audience and purpose for which the document was written even exist and, therefore, may not understand its rhetorical purpose and context. To that writer, the document may look badly written, unnecessarily verbose, incorrectly ordered, or too brief. In other words, it can be difficult for a writer to tell whether a potentially reusable piece of information is badly written and, therefore, in need of editing to be reused or an excellent piece written for a different audience and purpose and therefore not reusable at all.</db:para>
<db:para>Here again, <db:indexterm><db:primary>subject-domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject-domain</db:secondary></db:indexterm>subject-domain structured writing can come to the rescue by making the rhetorical purpose and context of the content explicit. As we saw with movie reviews, treatments of the same subject for different audiences and purposes have different structures. If you find a piece of content on a similar subject, but with a completely different structure, you can then look up the documentation for the document structure to determine what its audience and purpose is. This will help prevent accidental editing and inappropriate <db:indexterm><db:primary>reuse</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>reuse</db:secondary></db:indexterm>reuse.</db:para>
</db:section>
<db:section><db:title>Duplication in less structured contexts</db:title>


<db:para>Since duplication detection rules define when a piece of content is unique, they make it easy to determine if a piece of content exists. If you know which fields of a proposed content item define it as unique, you can query for a topic that has the same values in those fields. If you find one, you can be confident that the content already exists; if you don’t, you can be confident that it does not exist and needs to be written.</db:para>
<db:para>At least in theory you can. The problem is that not all content can be structured to the same degree. For example, while we can determine with a high degree of certainty whether the API reference contains only one entry for the <db:code>hello()</db:code> function in the <db:code>greetings</db:code> library, it is much harder to detect if a writer has inappropriately inserted a full description of the <db:code>hello()</db:code> function into the programmer’s guide.</db:para>
<db:para>API references typically contain a clearly identified entry for each API they document. Programmer’s guides typically deal with the relationships between different APIs and other parts of the system and how to accomplish real-world tasks that require programmers to use several APIs together. This focus lends itself to a reasonably strict content type for programming topics, but that does not help you detect duplication of material on related subjects such as API functions and libraries. Therefore, detecting that the writer of a programming topic has duplicated information provided by the API reference can be difficult.</db:para>
<db:para>Also, the programming guide writer may have had a good reason for duplicating information from the API guide. For example, to explain why someone might choose the <db:code>hello()</db:code> function from the <db:code>salutations</db:code> library rather than from the <db:code>greetings</db:code> library, the writer may have needed to explain the differences between the <db:code>hello()</db:code> functions in each library, using information taken from each library’s API reference. Simply referring readers to the two API reference entries to compare and contrast for themselves would eliminate the duplication, but at the expense of dumping the complexity of detecting and understanding the differences onto the reader.</db:para>
<db:para>Content, by its nature, deals with the complex and irregular aspects of the world, and you cannot expect to fully remove all duplication or everything that might be duplication from your content set without creating far more complexity in the content system than you have redirected. However good reuse looks on paper, attempting to remove all duplication is likely to leave you with more unhandled complexity than it removes, which always results in compromised rhetoric.</db:para>
<db:para>But while you should be cautious, don’t throw up your hands and abandon the attempt to tackle duplication in your content set. There are effective strategies, particularly on a smaller scale.</db:para>
</db:section>
<db:section><db:title>Localizing duplication detection</db:title>


<db:para>If detecting duplication in the general case often introduces more unhandled complexity than it removes, most of the duplication that really matters occurs locally. The risk that your movie review collection will accidentally duplicate content in your recipe collection is low. It is duplication within each collection you need to worry about, not duplication between them.</db:para>
<db:para>Therefore, subdividing your total content set can make for a much more practical approach to duplication detection. You can subdivide your content set in several different ways. Dividing it by document type is the most obvious. However, there are cases where you may want to detect and remove duplication between different document types covering the same subject.</db:para>
<db:para>For instance, some organizations try to minimize duplication between technical, training, and marketing content for each product. Although these three types of content obviously have different content models, their content models may include key fields that can be used to define a duplication detection rule (they address the same subject, so logically they will have certain subject-domain structures in common). Where the purpose of the content is similar, therefore, you can define anchor points for inserting the same information in multiple places. Of course, you have to keep in mind that technical, training, and marketing content do not always address the same audience for the same purpose, so an over-zealous approach to removing duplication could do more harm than good.</db:para>
<db:para>Finally, the more local the content set you are dealing with is, the more likely it is that everyone who creates and maintains the content set will know, or be able to guess, what content exists.</db:para>
</db:section>
<db:section><db:title>Reducing duplication through consistent content models</db:title>


<db:para>Using specific content models based on sound rhetorical models can help reduce duplication of content at all scales. A tightly constrained subject-domain content model, in particular, makes sure that there is a place for every piece of information and that every piece of information stays in its place. There is far less scope for incidental duplication between different content types if each content type is appropriately constrained. (And this works equally well to combat the opposite of duplication, which is omission.)</db:para>
<db:para>Strongly defined content types tend to be more cohesive – meaning that each instance of that content type for a different subject still covers the same ground and covers it more consistently. Without strong types, different writers may chunk ideas and information differently, which means that topics from two different writers can partially overlap. Not only is partial overlap hard to detect, since there are fewer points of similarity, it is also hard to fix because each item contains different information that the user needs. Eliminating one of the duplicates means finding a place for all of the extra information it contains, a process that can affect other content items and raise other duplication detection questions.</db:para>
<db:para>Using consistent content types ensures that when writers have a question about whether a certain subject has been covered or not, they know where to look.</db:para>
</db:section>
<db:section><db:title>Reducing duplication through consistent quality</db:title>


<db:para>Duplication can occur even when writers know that a piece of content on the same subject and with the same purpose already exists. A writer may think that the existing content is inadequate but not be willing or able to track down the original writer to discuss the situation or to determine how other uses of the content would be affected by editing it to bring it up to standard. Instead, the writer may simply write a new version.</db:para>
<db:para>There are two basic issues here. First, the quality of the existing content may not be good enough. A focus on creating consistent quality across the content system will go a long way towards avoiding this issue, because it will help ensure that when writers find content they would like to <db:indexterm><db:primary>reuse</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>reuse</db:secondary></db:indexterm>reuse, that content will be good enough to <db:indexterm><db:primary>reuse</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>reuse</db:secondary></db:indexterm>reuse.</db:para>
<db:para>Second, the two writers may have different views about the appropriate style or content to use to describe a particular subject to a particular audience for a particular purpose. Differences of opinion about how things should be written are common and, thus, a common source of complexity in any content system. However, you can manage these differences if you set up well-defined constraints that define the appropriate rhetorical strategy for a particular subject, audience, and purpose. Here again, structured writing can help enormously by allowing you to set rhetorical standards that are clear to writers and enforce those standards through the constraints built into the structured writing languages they use. This helps to avoid content duplication caused by disagreements over rhetoric and style.</db:para>
</db:section>
<db:section><db:title>Reducing the incentive to create duplicate content</db:title>


<db:para>Of course, merely knowing that content already exists is no guarantee against duplicate content being created. Even if there are down-the-road benefits to avoiding duplication, it may still be easier for a writer on a deadline to create duplicate content if the means for finding and reusing the existing content are cumbersome or difficult to use and understand.</db:para>
<db:para>Many of the reasons for avoiding duplication and for reusing content have to do with downstream savings in <db:indexterm><db:primary>change management</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>change management</db:secondary></db:indexterm>change management and <db:indexterm><db:primary>translation</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>translation</db:secondary></db:indexterm>translation. It is not a given that these techniques make life easier for the writer. If the system is too cumbersome or difficult to use, writers will rewrite rather than reuse, despite any downstream problems this causes.</db:para>
<db:para>It is important to understand that content reuse does not always save writers a lot of work. For instance, reuse does not necessarily reduce the amount of research they have to do. After all, how can you determine if a piece of existing content adequately describes your subject to your audience for your purpose if you have not done your research? Only systems that abstract the entire question away from the writer actually remove the need to research the topic. Once the research is done, writing the content can be a fairly straightforward and technically simple operation. There is no particular incentive for writers to undertake the complex tasks of looking for existing content, assessing it, and inserting it into their work, especially when the search may turn up nothing, leaving them with the writing work still to do.</db:para>
<db:para>In other words, if creating duplicate content is less complex and takes less time and energy than reusing existing content, it is likely that you will get lots of duplication, even if you have solved all the technical challenges of identifying and reusing content. Solving the technical challenges alone is never enough. You must remove complexity from key players in the system, so it is easier for them to do the job the right way.</db:para>
<db:para>A technically less sophisticated solution that is simpler for people to use will almost always win out over a more sophisticated system that is more complex to use. Or, to put it another way, complexity is not real sophistication. Real sophistication is not about adding functionality; it’s about directing complexity to where it can best be handled. Real technical sophistication in content systems is always about partitioning and directing complexity to the person with the skills, time, and resources to handle it. A simple <db:indexterm><db:primary>subject-domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject-domain</db:secondary></db:indexterm>subject-domain markup language can provide a simple (and therefore sophisticated) content creation interface that loses none of the functionality you need.</db:para>
</db:section>
<db:section><db:title>Less formal types of duplication detection</db:title>


<db:para>Having hard and fast rules that define duplication as two pieces of content having the same values in the same set of fields works well when the subject matter lends itself to that degree of structure. But not all content can be structured this precisely. Fortunately, structured writing techniques can be used to implement some less formal approaches to detecting duplication. These approaches are more probabilistic than certain and may be more appropriate as <db:indexterm><db:primary>audit</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>audit</db:secondary></db:indexterm>audit tools rather than tools you would expect writers to use before starting to write. Still, they can be useful, and they can detect duplication that other methods might not find.</db:para>
<db:para>For instance, if your content contains index entries – like those we looked at in <db:xref linkend="chapter.linking"/>  -- that list the subjects they cover by type and term, you could have an algorithm compare index entries across your topic set and flag pairs of topics whose index entries contain the same type/term combinations. Often it is appropriate for topics of different types, such as a programming topic and an API reference topic, to have similar or identical index terms, since these topics treat the same API functions in different ways (same subject but different audience or purpose). However, if you find topics of the same type with matching or near-matching sets of index terms, those topics may contain duplicate content.</db:para>
<db:para>You can do a similar check based on <db:indexterm><db:primary>subject annotations</db:primary></db:indexterm><db:indexterm><db:primary>structure</db:primary><db:secondary>subject annotations</db:secondary></db:indexterm>subject annotations. This is less precise than the index approach, because subject annotations annotate subjects the topic refers to rather than subjects the topic describes. However, if two topics contain subject annotations that refer to many of the same subjects, those topics may contain duplicate content.</db:para>
<db:para>There are tools available that claim to detect duplication using natural language methods. I’m not going to comment on the specific capabilities of such tools, but there are a few obvious downsides to these methods. First, they can only compare two completed texts to see if they are duplicates. They can’t detect whether the text you are thinking of creating already exists. Second, these methods look for the same or similar text, which is not the same as looking for the same or similar information. Similar information can be expressed using very different words, and similar words can convey very different information.</db:para>
</db:section>
<db:section><db:title>Reducing duplication by merging sources</db:title>


<db:para>Sometimes you want to deliberately duplicate content with variations for different publications. But while you want duplication on the output side, you don’t want it on the input side.</db:para>
<db:para>One way to reduce duplication in content is to merge information that is intended for different publications into a single structured source file from which the different variations can then be created algorithmically. We looked at an example of this in <db:xref linkend="chapter.management-domain"/> when we combined alcoholic and non-alcoholic beverage matches for a recipe into a single <db:indexterm><db:primary>subject-domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject-domain</db:secondary></db:indexterm>subject-domain recipe document, allowing us to produce variations of the same recipe for <db:emphasis>Wine Weenie</db:emphasis> and <db:emphasis>The Teetotaler's Trumpet</db:emphasis>.</db:para>
<db:para>Because a true <db:indexterm><db:primary>subject-domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject-domain</db:secondary></db:indexterm>subject-domain document is simply a collection of information about a subject from which you can select items to create many different documents, you can use the <db:indexterm><db:primary>subject domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject domain</db:secondary></db:indexterm>subject domain to eliminate duplication on the input side of a project. For instance, if you have a product with multiple versions, you can merge the information about all of the versions into a single source file and then publish only the blocks that apply to a particular version.</db:para>
</db:section>
<db:section><db:title>Duplication and the structure of content</db:title>


<db:para>As we have seen, placing content in well-defined structures makes it easier to detect duplication because it allows algorithms to compare values in a structure. When you can determine the common information requirements of a rhetorical pattern, you can create data points that can be compared to detect duplication before, during, and after writing content.</db:para>
<db:para>But using consistent, well-defined structures has a deeper importance for detecting and avoiding duplication. Dividing content into consistent blocks makes it easier to compare. Reuse is difficult when two pieces of content use different structures to describe the same content. You can’t simply replace one with the other because they don’t cover the same ground. Only units of the same size, shape, and scope can be effectively compared to see if they are duplicates. Using well-defined content structures ensures that each piece of content on a particular subject – each movie review, recipe, API reference, feature description, or configuration task – has the same shape, size, and scope, meaning they are comparable.</db:para>
<db:para>But the benefit of structure is not simply that units are comparable. If content is written in well-defined structures, each of which has a specific job to do, the chances of creating duplicate content, even by accident, are greatly reduced. In addition, writers and managers will have a good sense of what has been created and what has not, because well-defined structures ensure that different approaches to describing a subject won’t result in partial overlaps (or omissions).</db:para>
</db:section>
<db:section><db:title>Detecting duplication before and after authoring</db:title>


<db:para>Detecting and removing duplication is an important part of keeping your information collection tidy and should be a part of your regular <db:indexterm><db:primary>audit</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>audit</db:secondary></db:indexterm>audit practices. However, detecting duplicate content after it has been created means that you have paid writers to write the same content twice. Ideally, you would like to detect the duplication before the writer creates the duplicate content.</db:para>
<db:para>The extent to which you can do this depends on how well-structured your content types are. Let’s look at some scenarios:</db:para>
<db:itemizedlist>
<db:listitem>
<db:para>The ideal scenario is that the content model factors out repeated content altogether. If our <db:code>procedure</db:code> structure contains a compulsory <db:code>is-it-dangerous</db:code> field, then the warning text has been completely factored out. All the writer has to do is correctly fill out the <db:code>is-it-dangerous</db:code> field.</db:para>
</db:listitem>
<db:listitem>
<db:para>Merging sources is also a powerful approach, since the material for both versions is now created by the same writer at the same time.</db:para>
</db:listitem>
<db:listitem>
<db:para>The next best scenario is to have the authoring or content management system refuse to let writers create a duplicate content record in the first place. For instance, if writers must supply values for the movie name and the five-star rating before they enter content for a movie review, the CMS can raise an error before they waste any time writing a new review. However, although this kind of interface is common in the database world, it is not common for content, in large part because of the number of content types that you would have to create this kind of interface for. Also, writers are not used to working in this type of system.</db:para>
</db:listitem>
<db:listitem>
<db:para>The next best thing after that is for the system to allow writers to query the current collection to determine if there is already a piece of content with those values in those fields. The main difference here is that writers must initiate the query, rather than having the CMS check for duplication before it allows content to be created. This approach requires an interface that allows writers to run queries. However, a <db:indexterm><db:primary>content engineer</db:primary></db:indexterm><db:indexterm><db:primary>role</db:primary><db:secondary>content engineer</db:secondary></db:indexterm>content engineer or <db:indexterm><db:primary>information architect</db:primary></db:indexterm><db:indexterm><db:primary>role</db:primary><db:secondary>information architect</db:secondary></db:indexterm>information architect can provide pre-defined queries that match the organization’s business rules for determining duplication, thus relieving writers from having to learn how to create queries. This approach can be implemented using a much wider variety of tools and does not necessarily require any form of database or content management system.</db:para>
</db:listitem>
<db:listitem>
<db:para>After this the next best thing is to use a search engine. This is time consuming and uncertain, so it should be supplemented by using structured writing techniques and regularly <db:indexterm><db:primary>auditing</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>auditing</db:secondary></db:indexterm>auditing the content set to catch and remove accidental duplication.</db:para>
</db:listitem>
<db:listitem>
<db:para>Then there is the case where writers create content outside of the CMS but must complete a metadata record when submitting the content to the CMS. The CMS can detect duplication in the metadata fields and refuse to accept the content. The downside, of course, is that by the time the CMS detects the duplication, you have already paid for creating the content. Technically, though, the mechanism here is exactly the same as the case in which the writer queries before writing. In practice the difference is a matter of discipline rather than the technical implementation.</db:para>
</db:listitem>
<db:listitem>
<db:para>Finally, it may be appropriate to accept that certain types of duplication are just too hard to define and detect and that your content system will be simpler overall if you simply tolerate those types and/or handle them with periodic <db:indexterm><db:primary>auditing</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>auditing</db:secondary></db:indexterm>auditing, perhaps assisted by structured writing techniques.</db:para>
</db:listitem>
</db:itemizedlist>
</db:section>
<db:section><db:title>Optimize the whole, not the parts</db:title>


<db:para>Elimination of all duplication from a content set is not feasible. The variations on how, when, and why you mention particular facts in content, and the various purposes and audiences for which you describe different subjects, make it impossible to have hard and fast rules about what is and is not a duplicate for every type of content. If you pursue a <db:indexterm><db:primary>single source of truth</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>single source of truth</db:secondary></db:indexterm>single source of truth too zealously, you can eliminate valuable differences in information and presentation, which will damage rhetoric by creating excessively generic content that is hard for readers to understand and may not give them the information they need.</db:para>
<db:para>However, as we have seen, there are structured writing techniques that you can use to detect certain kinds of duplication and that make duplication less likely to occur, while reducing the amount of unhandled complexity in your content system.</db:para>
</db:section>
</db:chapter>
