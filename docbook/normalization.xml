<?xml version="1.0"?>
<db:chapter xmlns:db="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:strings="http://exslt.org/strings" version="5.0" xml:id="chapter.duplication"><db:title>Avoiding duplication</db:title>



<db:para>A significant source of complexity in any content organization is the unwanted duplication of content. Duplicate content can cause a number of problems. Not only is it expensive to create, different versions of the content may not agree with each other, which creates complexity for readers. When the subject matter changes, only one of the duplicates may get updated, causing further drift between the two versions (and updating two versions is twice the work). Duplicate content may also get punished by search engines. The need to detect duplicate content is therefore a major source of complexity in the content system.</db:para>
<db:para>Writers don’t hold the entirety of the organization’s content collection in their heads, so when they decide to write something, there is always the possibility that that content already exists somewhere in the content set. Obviously we would prefer writers to look before they write, but it is not always easy to find out if content on a particular subject for a particular audience already exists, so looking before you write every time can become very time consuming.</db:para>
<db:para>Indeed, even defining what constitutes duplication, let alone detecting that it exists, is not easy. Structured writing techniques can help, to a point, but it is important not to get carried way in the attempt to eliminate duplication, as this can easily do more harm than good, and can inject more complexity into the content system than it removes.</db:para>
<db:para>If writers are to avoid creating duplicate content, they need to determine as quickly as possible whether that content already exists or not. This check takes place countless times in the content systems so even if the cost of a single check is small, the cumulative cost can be large, creating an overhead that can significantly slow down your content system. Small costs with high repetition rates are often the hardest to detect and least satisfying to fix, whereas the wins from detecting duplication are often more visible and more satisfying. But it is important to make sure that your efforts to limit content duplication are not actually costing you more than they save.</db:para>
<db:para>This problem obviously affects any attempt at <db:indexterm><db:primary>content reuse algorithm</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>content reuse algorithm</db:secondary></db:indexterm>content reuse, since every time you set out to reuse content, you have to determine if reusable content exists. The longer it takes to determine if a piece of content already exists, the longer, and therefore more expensive, each instance of content reuse becomes. And bear in mind that the cost of determining if reusable content exists is incurred every time the writer looks for it, even if they don’t find it, but any saving associated with reuse are realized only when reusable content is found. (Indeed, failed attempts are often more expensive, since they have to exhaust all possibilities, whereas successful ones end as soon as relevant content is found.)</db:para>
<db:para>If you are going to implement a system for detecting and eliminating duplicate content, therefore, it is vital to have a clear plan that tells you exactly how authors are going to detect duplicate content, or find content to <db:indexterm><db:primary>reuse</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>reuse</db:secondary></db:indexterm>reuse, and to make sure that the overhead of using such a system does not outweigh the benefits it provides.</db:para>
<db:para>Given the cost of doing it by hand, handing the detection of duplication and the finding of existing content over to algorithms is highly desirable. But to hand the task over to algorithms, you will need to define what constitutes duplication and/or the purpose and role of a piece of content in precise terms and preserve the information that reflects those terms.</db:para>
<db:para>Creating a formal system for ensuring that content only exists once is sometimes called establishing a single source or truth. We should be very clear from the beginning that “single source of truth” does not mean that there is only one place or system from which all truths come; it means that for every significant truth you manage, that truth is only stored once. Different truths can certainly be stored in different places. The point is to make sure that the same truth is not stored in two different places, or two times in the same place.</db:para>
<db:para>A formal system for detecting duplication essentially means establishing a set of constraints by which duplication can be defined and detected. In other words, there needs to be a set of rules that says that if item X matches item Y in aspects A, B, and C, then X is a duplicate of Y. (These are rhetorical rules – rhetoric is at the heart of process here as elsewhere.) These rules constitute an algorithm for detecting duplication. For a machine to execute this algorithm, though, aspects A, B, and C have to be a defined part of the model of X and Y, or of the way that X and Y are stored. The content needs to retain this information in explicit rhetorical structures.</db:para>
<db:para>Where no such formal constraints exist, writers also have the option of using a search engine to look for existing content. A search engine can certainly turn up existing content on a subject, but search engines are not precise enough to detect duplication reliably. They may miss an actual duplicate because of a variation in <db:indexterm><db:primary>terminology</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>terminology</db:secondary></db:indexterm>terminology while returning several possible matches that are not duplicates. To eliminate the possibility that a duplicate exists in those results, the writer has to assess each one of them. That takes a lot of time, and, as we have noted, this task has to be performed before writing any piece of content, if the possibility of duplication is to be eliminated. This is a tremendous amount of overhead to impose on the content system.</db:para>
<db:para>How exactly do you construct a set of constraints for detecting duplicate content? First you need to begin with a definition of duplicate content. A reasonable general definition is this: duplicate content is content that describes the same subject matter to the same audience for the same purpose. All three aspects of this definition matter. We communicate to achieve an objective, so purpose matters. We communicate with a variety of people, so audience matters. Ignoring these things in our definition of duplication could result in serious quality problems.</db:para>
<db:para>So let’s examine a case of potentially duplicate content and look at how we can establish a formal system for deciding if another piece of content is a duplicate or not. Consider two movie reviews, written in markdown:</db:para>
<db:programlisting language="markdown">

Disappointing outing for the Duke
================================

After a memorable outing in _Rio Grande_ 
and _Sands of Iwo Jima_, John Wayne 
turns in an pedestrian performance 
in _Rio Bravo_.
</db:programlisting>
<db:para>and:</db:para>
<db:programlisting language="markdown">

Wayne's best yet
================

After tiresome performances in _Rio Grande_ 
and _Sands of Iwo Jima_, the Duke is brilliant 
in _Rio Bravo_.
</db:programlisting>
<db:para>Lets examine these according to our three criteria: First, do we have two pieces on the same subject? The subject of each is a movie. But is it the same movie? A human reading the text can tell easily enough that the subject of both pieces is the movie <db:emphasis>Rio Bravo</db:emphasis>, but an algorithms would have no way to tell, since nothing in the markup of either review directly identifies which movie is being reviewed.<db:footnote>
<db:para>I am talking here, and throughout this book, about conventional algorithms. The question of whether an AI algorithm could tell the difference is out of scope here. When and if AIs become sophisticated enough to read and write content effectively in human language, the partitioning of the content system, and the role of structured writing in that system, will become very different. So, algorithm here means the kind of algorithm that a content engineer or information architect with the appropriate skill set could design and code in a reasonable amount of time with ordinary programming tools.</db:para>
</db:footnote> Even if it could recognize the names of movies in the text, it would have no way to tell which one of those titles was the subject of the review.</db:para>

<db:para>But suppose these same reviews were written in <db:indexterm><db:primary>SAM</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>SAM</db:secondary></db:indexterm>SAM in a subject domain movie review language:</db:para>
<db:programlisting language="sam">

movie-review: Disappointing outing for the Duke
    movie-title: Rio Bravo
    review-text:
        After a memorable outing in {Rio Grande}(movie) 
        and {Sands of Iwo Jima}(movie), 
        {John Wayne}(actor) turns in 
        an pedestrian performance 
        in {Rio Bravo}(movie).
</db:programlisting>
<db:para>and:</db:para>
<db:programlisting language="sam">

movie-review: Wayne's best yet
    movie-title: Rio Bravo
    review-text:
        After tiresome performances in {Rio Grande}(movie) 
        and {Sands of Iwo Jima}(movie), 
        {the Duke}(actor, "John Wayne") is brilliant 
        in {Rio Bravo}(movie).
</db:programlisting>
<db:para>Now it is very easy for an algorithm to tell that these two pieces of content are both movie reviews and that they are both reviews of <db:emphasis>Rio Bravo</db:emphasis>.<db:footnote>
<db:para>If the two markdown examples were stored in a CMS that kept movie-review specific metadata, including the name of the movie reviewed, an algorithm could do the same check using the CMS metadata. For more on the choice between locating subject domain metadata in the document vs. in a CSM, see <db:xref linkend="chapter.content-management"/> and <db:xref linkend="chapter.metadata"/>.</db:para>
</db:footnote></db:para>

<db:para>What about audience? There are multiple audiences for movie criticism. There are moviegoers and Netflix subscribers who simply want to decide what movie to watch on a Saturday night, but there are also academic film students who may want a detailed analysis of <db:emphasis>Rio Bravo</db:emphasis> according to some school of film criticism.</db:para>
<db:para>With the two Markdown versions, it is impossible for an algorithm to tell whether one of these reviews is written for moviegoers and the other for film students. With the structured versions, the identification of audience is present, but it does not take the form of an <db:code>audience</db:code> field within the markup. Instead, the entire <db:code>movie-review</db:code> <db:indexterm><db:primary>markup language</db:primary></db:indexterm><db:indexterm><db:primary>tool</db:primary><db:secondary>markup language</db:secondary></db:indexterm>markup language is intended for writing reviews for moviegoers. If we were recording review for film students, we would not use the same markup language, we would create a separate <db:indexterm><db:primary>document type</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>document type</db:secondary></db:indexterm>document type that we might call <db:code>film-study</db:code>. Thus we can tell from the document type alone that both reviews are meant for the same audience.</db:para>
<db:para>The <db:indexterm><db:primary>subject domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject domain</db:secondary></db:indexterm>subject domain is concerned with the rhetoric of a piece of content: which pieces of information need to be presented to the reader in order to achieve its purpose. Therefore the identification of the reader is usually inherent in the definition of the <db:indexterm><db:primary>subject domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject domain</db:secondary></db:indexterm>subject domain document type itself. If you were recording your content in the document domain, however, you might well want to include a field to identify the intended audience of the piece, since audience is not implied by a <db:indexterm><db:primary>document domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>document domain</db:secondary></db:indexterm>document domain structure.</db:para>
<db:para>But what about purpose? While they review the same movie, the two reviews express two very different opinions about it. Does that constitute a difference in purpose? One could argue that one is encouraging readers to see the movie and the other is encouraging them not to. Or one could argue that the two together serve the purpose of giving the reader a more complete picture, or more options to inform their viewing choices. Ultimately, though it comes down to a question of whether keeping both versions serves a purpose for the organization that is publishing them. Does the organization wish to present itself as having a single firm opinion on every movie, for instance, or does it want to present itself as a neutral arbiter?</db:para>
<db:para>The question of whether we want to keep both reviews is therefore a business decision. You will seldom find two independently written pieces of content that are word for word identical. You will seldom find reusable content that is word for word what you would have written yourself. The definition of duplicate content is not based on identical text, but on whether or not the content serves an identical rhetorical purpose (which is to say, an identical business purpose). The constraints that determine if two pieces of content are duplicates of each other, therefore, are business rules.</db:para>
<db:para>Let’s say you are willing to have multiple reviews of the same movie in your collection as long as they gave different opinions. To accommodate this you can change your business rule for detecting duplicate movie reviews by adding a grading system to your review structure:</db:para>
<db:programlisting language="sam">

movie-review: Disappointing outing for the Duke
    movie-title: Rio Bravo
    5-star-rating: 2
    review-text:
        After a memorable outing in {Rio Grande}(movie) 
        and {Sands of Iwo Jima}(movie), 
        {John Wayne}(actor) turns in 
        an pedestrian performance 
        in {Rio Bravo}(movie).
</db:programlisting>
<db:para>and:</db:para>
<db:programlisting language="sam">

movie-review: Wayne's best yet
    movie-title: Rio Bravo
    5-star-rating: 5
    review-text:
        After tiresome performances in {Rio Grande}(movie) 
        and {Sands of Iwo Jima}(movie), 
        {the Duke}(actor, "John Wayne") is brilliant 
        in {Rio Bravo}(movie).
</db:programlisting>
<db:para>Now we can rewrite our duplicate detection rule for movie reviews to say that two reviews duplicate each other if the <db:code>movie-title</db:code> fields have the same value and the <db:code>5-star-rating</db:code> fields have the same value. By that rule, these two reviews are not duplicates because they have different <db:code>5-star-rating</db:code> values.</db:para>
<db:para>But in other cases, the business rules for determining duplication may be more difficult to express. Take a recipe for guacamole, for instance. Is guacamole a single dish for which there can only be one recipe? Then detecting duplication is easy enough. If the type of the item is “recipe” and the value of the dish field is “guacamole”, then the content is duplicate.</db:para>
<db:para>But there are many different ways in which you can prepare guacamole, some differing only slightly from one another and some presenting welcome variations that different people might like to try. Clearly a recipe site would not want eight essentially identical guacamole recipes, but nor would they want to pick one variation to the exclusion of all others. So then the question becomes, how do you decide when a recipe is an effective duplicate of an existing recipe and when it is a welcome variation? If you decide the variation is welcome, how do you differentiate it from other guacamole recipes in your collection? Merely adding an additional data field to the duplication detection rule may not suffice for deciding if a variant of guacamole is interesting enough to add to our collection. In the end, this may have to be a human editorial decision.</db:para>
<db:para>Clearly, therefore, the business rules for detecting duplication are not universal. The way you decide these questions for recipes are not the same way you decide them for API reference topics, used car reviews, movie reviews, or conceptual discussions of ideas. Duplication detection happens in the <db:indexterm><db:primary>subject domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject domain</db:secondary></db:indexterm>subject domain and is specific to a particular type of content about a particular type of subject serving a specific business purpose. Whatever constraint you decide upon, the business processes and systems that ensure that these constraints are followed are not universal, but specific to each function and organization.</db:para>
<db:section>
<db:title>The scale of duplication</db:title>
<db:para>So far we have looked at detecting duplication of whole documents. But what if the duplication we want to avoid occurs below the level of a document? In <db:xref linkend="chapter.management-domain"/> we looked at the example of a warning that was to be attached to all dangerous procedures. The duplication of that warning occurred at a much smaller scale. It was just a single structure within a procedure. Not only could it occur in many different documents, it could also occur multiple times within a single document.</db:para>
<db:para>Note that while the duplication of whole documents is something we generally want to eliminate, the duplication of parts of documents is often something we deliberately want to encourage. We want the identical warning to occur in every dangerous procedure so that readers are duly warned when attempting that procedure. All forms of <db:indexterm><db:primary>content reuse algorithm</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>content reuse algorithm</db:secondary></db:indexterm>content reuse are, in fact, methods for deliberately duplicating content in multiple places in the content set. <db:indexterm><db:primary>Reuse</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>Reuse</db:secondary></db:indexterm>Reuse techniques are about eliminating duplication on the writing and content management side of the content system, while creating duplication on the publishing side. But while reuse techniques give you a mechanism for inserting duplicate content in various locations, reuse can easily go off the rails is you don’t have a clear way of defining what is and is not duplicate content.</db:para>
<db:para>It helps greatly if the duplicate content plays a wholly consistent role in the documents it belongs to. That is the case for examples such as the warning for dangerous procedures. That warning has a clear and distinct purpose and a clear anchor point in any document it appears in: a procedure that is dangerous. Without such a clear purpose and anchor point, the chances of authors remembering to include the existing content rather than creating a duplicate goes down substantially. As we saw in <db:xref linkend="chapter.management-domain"/>, the use of subject domain structures, in the form of a compulsory <db:code>is-it-dangerous</db:code> field in a procedure structure, can fully define both the purpose and the anchor point, thus making it impossible for authors to forget or neglect to include the warning – a clear example of rhetorical structure supporting reliable process.</db:para>
<db:para>Finding a reliable anchor for detecting duplication tends to get more difficult the smaller the content unit you try to apply it to. For example, should you try to remove the duplication of sentences that occur frequently but in different contexts? Should we, for instance, try to eliminate the duplication of the phrase “Press OK” that occurs so many times in many technical documents? This phrase occurs frequently and always means the same thing. But would replacing it with a variable actually reduce complexity or make any part of the content system more reliable? The number of repeated words, phrases, and sentences in the average technical document is very high and treating them all as duplicate content is obviously not feasible, nor would it solve an obvious problem. So, when should we regard a piece of content below the document level as duplicate?</db:para>
</db:section>
<db:section>
<db:title>Duplication of information vs duplication of text.</db:title>
<db:para>Part of the reason for not wanting to factor out “Press OK” is that anything you replace it with would probably be longer than the original, and certainly more abstract. But a big part of the reason lies in the distinction between the duplication of information and the duplication of text. The same text can occur in multiple places without actually being duplicate information. Each instance of “Press OK” in our procedures refers to a button in a different dialog box. Those buttons all have the same name, and thus the instruction to press them is an identical piece of text, but they are still different buttons. It is entirely possible that the redesign of one of those dialog boxes could result in OK button being renamed something more specific to the function of the dialog box, such as Print or Send. Thus each instance of “Press OK” is actually a different piece of information, though it is expressed with the same text. (But see <db:xref linkend="chapter.modeling"/> for an approach that would eliminate the duplication of “Press OK” without running into this issue.)</db:para>
<db:para>By contrast, the warning for a dangerous procedure is a single piece of information occurring in multiple contexts.<db:footnote>
<db:para>In the example we have been looking at. Of course, some procedures are dangerous in unique ways and require unique warnings.</db:para>
</db:footnote> It applies with equal force to all procedures that are dangerous. Of course, a procedure could go from being dangerous to not being dangerous. (A new version of the product may include a safer design.) In this case the warning should be removed. But the value of the warning remains the same for all procedures to which it applies, whereas the name of the make-it-go button for a dialog box can change independently of other dialog boxes, all of which still have make-it-go buttons. That is, it is a difference in applicability, not a difference in content.</db:para>

<db:para>If this distinction seems a little hard to get your head round, that is a good indication of how difficult it can be to detect true duplication in content. And we should reflect that when we eliminate duplicated text that is not actually duplicated information, we are actually introducing complexity into our content set – complexity that will either make <db:indexterm><db:primary>change management</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>change management</db:secondary></db:indexterm>change management more difficult down the road, or that will get missed and damaging rhetoric.</db:para>
<db:para>It is probably better, therefore, to stick to cases where you are very certain that the duplication you are detecting is genuinely duplication of information and not merely duplication of text.</db:para>
</db:section>
<db:section>
<db:title>Duplication and the level of detail</db:title>
<db:para>Another problem with identifying duplication deals with the level of detail in which a subject is treated. For example, in most Wikipedia articles on countries, there is a section on the economy of that country, and at the beginning of that section there is a link to an entire article describing the economy of that country, followed by a summary coverage of that country’s economy presumably much briefer and less detailed than that provided by the main article. There may also be a brief mention of the highlights of the country’s economy in the four or five context-setting paragraphs that lead most Wikipedia articles. These different levels of detail serve different user needs, and so each is a valuable contribution to the content set. In other words, they are not duplicates because while they address the same subject, they potentially appeal to different audiences, and they serve different purposes.</db:para>
<db:para>Everything we know about effective rhetoric tells us that we need to address different audiences and different tasks differently. Taking a piece of content designed for one audience and using it for all other audiences, or attempting to write generic content that takes no account of any audience’s needs or tasks is certain to produce content that is significantly less effective.</db:para>
<db:para>The question, then, is whether differences in level of detail reflect a difference audience and purpose. While it may seem like this is a distinction that any writer should be able to make via inspection, it is actually quite a difficult distinction to make in many cases, because it relies on understanding the audience and their purpose. A writer who understands one audience and purpose well may look at a piece of content designed for a different audience and purpose and fail to recognize the difference. They may not even know that the audience and purpose for which the document was written even exist. In other words, they may not understand the rhetorical purpose and context of the content. As a result, the document may simply look badly written, or unnecessarily verbose, or ordered incorrectly, or too brief. In other words, it can be very difficult for a writer to be sure whether a potentially reusable pieces of information is badly written and in need of editing to be reused or an excellent piece written for a different audiences and purpose and therefore not reusable at all.</db:para>
<db:para>Here again, <db:indexterm><db:primary>subject-domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject-domain</db:secondary></db:indexterm>subject-domain structured writing can come to the rescue by making the rhetorical purpose and context of the content explicit. As we saw with movie reviews, treatments of the same subject for different audiences and purposes have different structures. If you find a piece of content on a similar subject, but with a completely different structure, you can then look up the documentation for the document structure to determine what its audience and purpose is. This will help prevent accidental editing and inappropriate <db:indexterm><db:primary>reuse</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>reuse</db:secondary></db:indexterm>reuse.</db:para>
</db:section>
<db:section>
<db:title>Duplication in less structured contexts</db:title>
<db:para>Since duplication detection rules define when a piece of content is unique, they make it easy to determine if a piece of content exists. If you know which fields of a proposed content item define it as unique, you can query for a topic that has the same values in those fields. If you find one, you are confident the content already exists; if you don’t, you can be confident that it does not exist and needs to be written.</db:para>
<db:para>At least, in theory you can. The problem is that not all content can be structured to the same degree. For example, while we know with a high degree of certainty that there is only one API reference entry on the <db:code>hello()</db:code> function of the <db:code>greetings</db:code> library, it is much harder to detect if a writer has inappropriately inserted a full description of the <db:code>hello()</db:code> function into the programmer’s guide.</db:para>
<db:para>Programmer’s guides typically deal with the relationships between different APIs and other parts of the system, and on how to accomplish certain real world tasks with the system as a whole. This focus may lend itself to a reasonably strict content type for programming topics, but that does not help you detect duplication of material on related subjects like API functions and libraries. Detecting that the author of a programming topic has duplicated information provided by the API reference may therefore be difficult.</db:para>
<db:para>And while the programming guide author may have duplicated information from the API guide, they may have had a good reason to do so. If, for instance, they are explaining the reasons one might choose to use a function from the <db:code>salutations</db:code> library rather than the <db:code>greetings</db:code> library, then explaining the differences between the <db:code>hello()</db:code> function in each library is necessary, and necessarily involves repeating some of the information in each library’s API reference. Simply referring the reader to each API reference to compare and contrast for themselves would eliminate the duplication, but at the expense of dumping the complexity of detecting and understanding the differences onto the reader.</db:para>
<db:para>Content, by its nature, deals with the complex and irregular aspects of the world, and we cannot expect to fully remove all duplication, or everything that might or might not be duplication, depending on how you look at it, from the overall content set without creating far more complexity in the content system than one has redirected. However good it looks on paper, such an attempt is likely to leave you with more unhandled complexity than it removes and the compromised rhetoric that always results when complexity goes unhandled.</db:para>
<db:para>But while this is a reason to be cautions, it is not a reason to throw up your hands an abandon the attempt to tackle duplication in your content set. There are certainly things that can be done effectively, particularly on a smaller scale.</db:para>
</db:section>
<db:section>
<db:title>Localizing duplication detection</db:title>
<db:para>If detecting duplication in the general case will often introduce more unhandled complexity than it removes, most of the duplication that really matters occurs much more locally. The risk that your movie review collection will accidentally duplicate content in your recipe collection is pretty low. It is duplication within each collection you need to worry about, not duplication between them.</db:para>
<db:para>Subdividing the total content collection, therefore, can make for a much more practical approach to duplication detection. You can subdivide the content set in different ways. Dividing it by document type is the most obvious, but there are worthwhile and feasible ways to detect and remove duplication between different document types in the same subject.</db:para>
<db:para>For instance, some organizations try to minimize duplication between the technical, training, and marketing content for each individual product. While these three types of content obviously don’t all have the same content models, their individual content models can sometimes include the key fields that can be used to define a duplication detection rule, and it is possible to define similar purposes and anchor points for the insertion the the same information in multiple places. Of course, one has to be keep in mind that these three groups do not always address the same audience for the same purpose, so an over-zealous approach to removing duplication could do more harm than good.</db:para>
<db:para>Finally, of course, the more local the content set you are dealing with, the more likely it is that everyone involved in creating and maintain the content set will know, or at least be able to make a good guess, as to what content exists.</db:para>
</db:section>
<db:section>
<db:title>Reducing duplication through consistent content models</db:title>
<db:para>Using highly specific content models based on sound rhetorical models can help reduce duplication of content at all scales. A tightly constrained subject-domain content model, in particular, makes sure that there is a place for every piece of information and every piece of information stays in its place. There is far less scope for incidental duplication between different content types if each content type is appropriately constrained. (And this works equally well to combat the opposite of duplication, which is omission.)</db:para>
<db:para>Strongly defined content types tend to be more cohesive – meaning they cover the same piece of ground and cover it more consistently for each instance of the subject matter. Without strong types, different writers may chunk up ideas and information differently, so that topics from two different writers may partially overlap each other. Not only is partial overlap harder to detect, since there will be fewer points of similarity between the items, it is also harder to fix because each item contains different information that the user needs. Eliminating one of the duplicates means finding a place for all of the extra information it contains, a process that could potentially affect several other content items and perhaps raise other duplication detection questions.</db:para>
<db:para>The use of consistent content types also ensures that when writers have a question about whether a certain subject had been covered or not, they have a much better sense about where to look for it.</db:para>
</db:section>
<db:section>
<db:title>Reducing duplication through consistent quality</db:title>
<db:para>Duplication can also occur even when writers know that a piece of content on the same subject and with the same purpose already exists. The writer may not think the existing content is good enough for their purpose, but may not be willing or able to track down the author of the current content to discuss it, or figure out what might be affected if they edited the current content to bring is up to standard, so they decide to write their own version to meet their own quality standards.</db:para>
<db:para>There are two basic issue here. The first is that the writer may be correct, that the existing content is simply not good enough. In this case, a focus on creating consistent quality across the content system will go a long way towards avoiding this kind of duplication, because it will ensure that when someone does find some content they would like to <db:indexterm><db:primary>reuse</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>reuse</db:secondary></db:indexterm>reuse, it will actually be good enough to <db:indexterm><db:primary>reuse</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>reuse</db:secondary></db:indexterm>reuse.</db:para>
<db:para>The second is that the writer may simply have a different view from the original writer of the appropriate style or content to describe a particular subject to a particular audience for a particular purpose. Differences of opinion about how things should be written are common, and thus a common source of complexity in any content system, but they can be managed if the content organization sets up well-defined constrains that define what the organization has decided is the appropriate rhetorical strategy for a particular subject, audience, and purpose. Here again, structured writing can help enormously by setting rhetorical standards that are clear to writers through the constraints built into the structured writing languages they are using. This then helps to avoid content duplication caused by disagreements over quality.</db:para>
</db:section>
<db:section>
<db:title>Reducing the incentive to create duplicate content</db:title>
<db:para>Of course, merely knowing that content already already exists is no guarantee against duplicate content being created. Even if there are down-the-road benefits to avoiding duplication, it may still be easier for an individual writer on a deadline to create duplicate content if the means for finding and reusing the existing content are cumbersome or difficult to use and understand. Many of the reasons for avoiding duplication and for reusing content have to do with downstream savings in <db:indexterm><db:primary>change management</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>change management</db:secondary></db:indexterm>change management and <db:indexterm><db:primary>translation</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>translation</db:secondary></db:indexterm>translation. It is not a given that these techniques make life easier for the writer. If the system is at all cumbersome or difficult, rewriting may well be easier than reusing in the moment, despite any downstream problems it causes.</db:para>
<db:para>It is important to understand, in this context, that content reuse does not always save the writer a lot of work. For instance, it does not necessarily reduce the amount of research they have to do. After all, how can you determine if a piece of existing content adequately describes your subject to your audience for your purpose if you have not done your research? Only systems that abstract the entire question away from the author actually remove the need to research the topic. Once the research is done, writing the content can be a fairly straightforward and technically simple operation. There is no particular incentive for a writer to undertake the complex tasks of looking for existing content, assessing it, and inserting it into their work, especially when the search may turn up nothing, leaving them with the writing work still to do.</db:para>
<db:para>In other words, if creating duplicate content is less complex and takes less time and energy than reusing existing content, it is likely that you will get lots of duplication, even if you have solved all the technical challenges of identifying and reusing content. Solving the technical challenge alone is never enough. The point is always to remove complexity from key players in the system so that it is easier for them to do the job the right way.</db:para>
<db:para>A technically less sophisticated solution that is simpler for people to use will almost always win out in the end over a more sophisticated systems that is more complex to use. Or, to put it another way, real technical sophistication in content systems is always about partitioning and directing complexity to the person with the skills, time, and resources to handle it. <db:indexterm><db:primary>Functional lucidity</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>Functional lucidity</db:secondary></db:indexterm>Functional lucidity is key to getting consistently structured data. Duplication detection rules and <db:indexterm><db:primary>content reuse algorithm</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>content reuse algorithm</db:secondary></db:indexterm>content reuse methods that reduce <db:indexterm><db:primary>functional lucidity</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>functional lucidity</db:secondary></db:indexterm>functional lucidity are likely to be self-defeating.</db:para>
</db:section>
<db:section>
<db:title>Less formal types of duplication detection</db:title>
<db:para>Having hard and fast rules that define duplication as two pieces of content having the same values in the same set of fields works well when the subject matter lends itself to that degree of structure. But not all content can be structured this precisely, and structured writing techniques can also be used to implement some less formal approaches to detecting duplication. These approaches are more probabilistic than certain, and may be more appropriate as <db:indexterm><db:primary>audit</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>audit</db:secondary></db:indexterm>audit tools than as something that you expect writers to do before the write anything. Still, they can be useful, and can detect cases of duplication that might not be detectable by other methods.</db:para>
<db:para>For instance, if your content contains the kind of index entries that we looked at in <db:xref linkend="chapter.linking"/>, which list the subjects it covers by type and term, you might have an algorithm compare the indexing of topics across your topic set and flag any pairs of topics whose index entries contain the same type/term combinations. Often it is perfectly appropriate for topics of different types, such as a programming topic and an API reference topic, to have the same terms indexed, since these topics treat the same API functions, but in different ways (same subject, different audience or purpose). But if you find you have topics of the same type with matching or near-matching sets of index terms, that is a good clue that there may be content duplication going on.</db:para>
<db:para>You could do a similar check based on the list of <db:indexterm><db:primary>subject annotations</db:primary></db:indexterm><db:indexterm><db:primary>structure</db:primary><db:secondary>subject annotations</db:secondary></db:indexterm>subject annotations that topics contain. This is less precise than the index approach because subject annotations annotate subjects that the topic refers to, rather then the subject is is about, but it is still true that if a two topics refer to many of the same subjects, they may contain duplicate content.</db:para>
<db:para>It is also worth mentioning here that there are tools available that claim to do the duplication of detection using natural language methods. I’m not going to comment on the specific capabilities of such tools, but there are a few obvious downsides to this method. The first is that it can only compare two completed texts to see if they are duplicates. They can’t detect that the text you are thinking of creating already exists. The second is that they are looking for the same or similar texts, which, as we noted, is not the same a same or similar information. Similar information can be expressed using very different words, and similar words can convey very different information.</db:para>
</db:section>
<db:section>
<db:title>Reducing duplication by aggregating content</db:title>
<db:para>Sometimes you want to deliberately duplicate content with variations for different publications. But while you want duplication on the output side, you don’t want it on the input side.</db:para>
<db:para>One way to reduce duplication in content is to aggregate information at source that will be output to different documents. We looked at an example of this in <db:xref linkend="chapter.management-domain"/> when we combined alcoholic and non-alcoholic beverage matches for a recipe into a single <db:indexterm><db:primary>subject-domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject-domain</db:secondary></db:indexterm>subject-domain recipe document, allowing us to produce variations of the same recipe for <db:emphasis>Wine Weenie</db:emphasis> and <db:emphasis>The Teetotaler's Trumpet</db:emphasis>.</db:para>
<db:para>Because a true <db:indexterm><db:primary>subject domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject domain</db:secondary></db:indexterm>subject domain document is simply a collection of information about a subject from which you can select items to create many different documents, you can use the <db:indexterm><db:primary>subject domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject domain</db:secondary></db:indexterm>subject domain to eliminate duplication on the input side of a project. For instance, if you have a product with multiple versions, you can aggregate the information about all of the versions into a single source file and then publish only the blocks that apply to a particular version.</db:para>
</db:section>
<db:section>
<db:title>Duplication and the structure of content</db:title>
<db:para>As we have seen, placing content in well defined structures helps with duplication detection because it allows algorithms to compare values in a structure to determine if two items are duplicates or not. When we can figure out the common information requirements of a rhetorical pattern we can also create data points that we can compare to do duplication detection before, during, and after writing content.</db:para>
<db:para>But using consistent well-defined structures has a deeper importance for the detection and avoidance of duplication. By dividing content into consistent blocks, it makes items of content more comparable. It is very difficult to do anything about duplication that occurs when two differently structured pieces of content overlap in the subject matter they describe. You can’t simply replace one with the other because they don’t cover the same ground. Only units of the same size, shape, and scope can be effectively compared to see if they are duplicates. Using well defined content structures ensures that each piece of content on a particular subject – each movie review, recipe, API reference, feature description, or configuration task – has the same shape, size, and scope, meaning they are comparable.</db:para>
<db:para>But the effect is not simply that the units are comparable. If content is written in well defined structures, each of which has a specific job to do, the chances of creating duplicate content, even by accident, are greatly reduced. Authors and managers have a good sense of what has been created and what has not, and the structures ensure that different approaches to describing a subject don’t result in partial overlaps (or omissions).</db:para>
</db:section>
<db:section>
<db:title>Detecting duplication before and after authoring</db:title>
<db:para>Detecting and removing duplication is an important part of keeping your information collection tidy and should be a part of your regular <db:indexterm><db:primary>audit</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>audit</db:secondary></db:indexterm>audit practices. However, detecting duplication after it already  exists still means that you have paid writers to write the same content twice. Ideally you would like to to detect the duplication before the writer does all the work of creating the duplicate content.</db:para>
<db:para>The extent to which you can do this depends greatly on how structured your content types are. Let’s look briefly at some scenarios.</db:para>
<db:itemizedlist>
<db:listitem>
<db:para>The ideal scenario is that the content model factors out the repeated content altogether. If our <db:code>procedure</db:code> structure contains a compulsory <db:code>is-it-dangerous</db:code> field, then the text of any warning you might want to add has been completely factored out. All the author has to do is correctly fill out the <db:code>is-it-dangerous</db:code> field.</db:para>
</db:listitem>
<db:listitem>
<db:para>Aggregating material from different output documents into a single input-side document is also a powerful approach, since the material for both versions is now created by the same writer at the same time.</db:para>
</db:listitem>
<db:listitem>
<db:para>The next best scenario is that the authoring or content management system refuses to let the writer create a duplicate content record in the first place. For instance, if in order to create a movie review in your CMS, the writer must first supply values for the movie name and five star rating fields, before entering any content, then the CMS can reject the attempt before the author wastes any time writing a new review. However, while this kind of interface is common in the database world, it is not common for content, in large part because of the variety of different content types that you would have to create this kind of interface for, and also because this is simply not the way writers are used to working.</db:para>
</db:listitem>
<db:listitem>
<db:para>The next best thing after that is that the system allows the writer to query the current collection to determine if there is already a piece of content with those values in those fields. The main difference here is that the writer has to voluntarily initiate the query, rather than the CMS doing it as a condition of allowing content to be created. This obviously requires some kind of interface for allowing writers to run these queries. Most writers don’t have to learn how to write them, though, as a <db:indexterm><db:primary>content engineer</db:primary></db:indexterm><db:indexterm><db:primary>role</db:primary><db:secondary>content engineer</db:secondary></db:indexterm>content engineer or <db:indexterm><db:primary>information architect</db:primary></db:indexterm><db:indexterm><db:primary>role</db:primary><db:secondary>information architect</db:secondary></db:indexterm>information architect can provide a predefined set of queries that match the organization’s business rules for determining duplication. This approach can be implemented using a much wider variety of tools and does not necessarily require any form of database or content management system.</db:para>
</db:listitem>
<db:listitem>
<db:para>After this the next best thing is to use a search engine to try to find duplicates. This obviously is time consuming and uncertain, so it should be supplemented by regular <db:indexterm><db:primary>auditing</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>auditing</db:secondary></db:indexterm>auditing of the content set, aided by various structured writing techniques, to catch and remove accidental duplication.</db:para>
</db:listitem>
<db:listitem>
<db:para>Then there is the case where the writer writes the content outside of the CMS but is required to complete a metadata record when submitting the content to the CMS. Here the CMS can detect duplication in the metadata fields and refuse to accept the content. The downside, of course, is that the cost of creating the content has already been paid before the CMS detects the duplication. Technically, though, the mechanism here is exactly the same as the case in which the writer queries first, before, writing. So this becomes more a matter of discipline than a difference in technical solution.</db:para>
</db:listitem>
<db:listitem>
<db:para>Finally, it may be appropriate to accept that certain types of duplication are just too hard to define and detect and your content system will be simpler overall if you simply tolerate them and/or handle them with periodic <db:indexterm><db:primary>auditing</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>auditing</db:secondary></db:indexterm>auditing, perhaps assisted by structured writing techniques.</db:para>
</db:listitem>
</db:itemizedlist>
</db:section>
<db:section>
<db:title>Optimize the whole, not the parts</db:title>
<db:para>Overall, then, the elimination of all duplication from a content set is not really feasible. The variations on how, when, and why you mention particular facts in content, and the various purposes and audiences for which you describe different subjects make it impossible to have hard and fast rules about what is and is not a duplicate for every type of content. Too zealous a pursuit of a <db:indexterm><db:primary>single source of truth</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>single source of truth</db:secondary></db:indexterm>single source of truth can result in the elimination of valuable differences in information and presentation – damaging rhetoric by creating excessively generic content that is harder for readers to understand and may not give them all the information they need.</db:para>
<db:para>However, there are a number of specific structured writing techniques that you can use both to detect certain kinds of duplication and to make duplication less likely to occur. Use them in ways that are consistent with your overall goal of reducing the amount of unhandled complexity in your content system.</db:para>
</db:section>
</db:chapter>
