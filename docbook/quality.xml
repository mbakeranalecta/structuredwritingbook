<?xml version="1.0"?>
<db:chapter xmlns:db="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:strings="http://exslt.org/strings" version="5.0" xml:id="chapter.quality"><db:title>Quality in Structured Writing</db:title><db:indexterm class="startofrange" significance="preferred" xml:id="idp34772068"><db:primary>quality</db:primary></db:indexterm><db:indexterm class="startofrange" xml:id="idp34772068x"><db:primary>concept</db:primary><db:secondary>quality</db:secondary></db:indexterm>



<db:para>When I talk to programmers about what I do, they sometimes ask me why structured writing is important any more. Machines are getting so good at reading human language, they argue, that semantic markup to assist the machine is becoming pointless.</db:para>
<db:section>
<db:title>Robots that read</db:title>
<db:para>Indeed, machines are getting better at understanding human language. An approach called Deep Learning is becoming a key technology for companies like Facebook, Google, and Baidu for both language comprehension<db:footnote><db:para>http://www.technologyreview.com/featuredstory/540001/teaching-machines-to-understand-us/</db:para></db:footnote> and speech recognition.<db:footnote><db:para>http://www.technologyreview.com/news/544651/baidus-deep-learning-system-rivals-people-at-speech-recognition/</db:para></db:footnote></db:para>
<db:para>The semantic web initiative has long sought to create a Web that is not just people talking to people but also machines talking to machines. This has traditionally involved a separate communication channel – semantic markup embedded in text but not presented to the human reader. It has also involved the creation of specialized semantic data stores with query languages to match, to teach computers to understand relationships that humans express in ordinary language.</db:para>
<db:para>But this two-channel approach – one text for humans, another for machines – makes sense only if we assume that machines cannot read human language. If machines and humans can read the same text, then we shouldn’t need two channels. The human Web becomes the semantic Web.</db:para>
<db:para>After all, human text has always been semantic. Semantics is simply the study of meaning. All meaningful texts have <db:indexterm><db:primary>semantics</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>semantics</db:secondary></db:indexterm>semantics. However, it is difficult to build algorithms that can read and understand like humans do. Semantic technologies dumb down the semantics for machines because machines are not bright enough to read the regular semantics.</db:para>
</db:section>
<db:section>
<db:title>Dumbing it down for the robots</db:title>
<db:para>This dumbing down necessarily involves omitting a great deal. Fully expressing the meaning and implications of even the simplest text in a data representation language such as <db:indexterm><db:primary>RDF</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>RDF</db:secondary></db:indexterm>RDF (Resource Description Framework) would be daunting. This creates a problem for semantic technologies: which semantics do you select to dumb down to the machine’s level and for what purpose? This is why there is no universal approach to structured writing that works for all purposes and all subject matter. You can only represent a fraction of the human semantics to the machine, and what you choose to represent depends on what you want the machine to do.</db:para>
<db:para>But if the machine could read the text as well as you can, then these limitations vanish. Deep learning is moving us in that direction.</db:para>
<db:para>Why then should we bother with structured writing? Because while machines may be rapidly learning to read human text, humans still write that text, and most humans are not good writers.</db:para>
</db:section>
<db:section>
<db:title>Making humans better writers</db:title>
<db:para>By that I don’t just mean that they use poor grammar or spelling or that they create run-on sentences or use the passive voice too much, though all those things may be true and annoying. I mean something more fundamental: they don’t say the right things in the right way for the right audience. They leave out stuff that needs to be said, they weigh their text down with stuff that does not need to be said, or they write in a way that is hard to understand.</db:para>
<db:para>We all suffer from a malady called <db:indexterm><db:primary>the Curse of Knowledge</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>the Curse of Knowledge</db:secondary></db:indexterm>the Curse of Knowledge,<db:footnote><db:para>https://en.wikipedia.org/wiki/Curse_of_knowledge</db:para></db:footnote> which makes it difficult for us to understand what it is like not to understand something we know. Harvard psychologiest Steven Pinker regards the curse of knowledge as single best explantion for bad writing.<db:footnote><db:para>https://www.linkedin.com/pulse/single-reason-why-some-people-cant-write-according-glenn-leibowitz</db:para></db:footnote> Because we forget what it is like to not understand, we take shortcuts, we make assumptions, we say things in obscure ways, and we just plain leave stuff out.</db:para>
<db:para>This is not a result of mere carelessness. The efficiency of human communication rests on our ability to assume that the person we are communicating with shares a huge collection of experiences, ideas, and vocabulary in common with us.<db:footnote><db:para>http://everypageispageone.com/2015/08/04/the-economy-of-language-or-why-we-argue-about-words/</db:para></db:footnote> Laboriously stating the obvious is as much a writing fault as omitting the necessary. Yet what is obvious to one reader may be obscure to another. The curse of knowledge is that as soon as something becomes obvious to us, we can no longer imagine it being obscure to someone else.</db:para>
<db:para>Thus, much human-to-human communication fails. The recipient of the communication simply does not understand it or does not receive needed information because the writer left it out. Machines may learn to be better readers than we are, but no machine can learn to read information that isn’t there.</db:para>
</db:section>
<db:section>
<db:title>We write better for robots than we do for humans</db:title>
<db:para>Actually, one of the advantages of the relative stupidity of robots is that it forces us to be very careful in how we create and structure data for machines to act on. The computer science community coined the phrase “garbage in, garbage out” very early, because the machines were, and to a large extent still are, too stupid to identify garbage input, and they did not have the capacity to seek clarification or consult other sources, as a human would. They just spit out garbage.</db:para>
<db:para>Therefore, we had to improve the quality and precision of the data going in. We worked out precise data structures and implemented elaborate audit mechanisms to make sure that data was complete and correct before we fed it to the machine.</db:para>
<db:para>We have never been as diligent in improving the quality of the content we feed human beings. Faced with poor content, human beings do not halt and catch fire; they either lose interest or do more research. Given our adaptability as researchers and our tenacity in pursuing things that really matter to us, we often manage to muddle through bad content, though at considerable economic cost. And writers are often so removed from their readers that they have no notion of what the poor reader is going through. If readers did halt and catch fire, we might put more effort and attention into content quality.</db:para>
<db:para>Even today, with many companies implementing enterprise content management and making the store of corporate knowledge available to all employees, most of the emphasis is on making content easier to find, not on making that content more worth finding. (This despite the fact that the best thing you can do to make content easy to find it to make it more worth finding.) People trying to build the semantic web spend a lot of time trying to make the data they prepare for machines correct, precise, and complete. We don’t do nearly as much for humans. Until we do, deep learning alone may not be enough to make the human web the semantic web.</db:para>
<db:para>Part of the problem is that improving content quality runs up against the <db:indexterm><db:primary>curse of knowledge</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>curse of knowledge</db:secondary></db:indexterm>curse of knowledge. Both writers and the subject matter experts who review their content suffer from the curse, which makes it difficult to audit written content. Style guides and templates can help, but their requirements are difficult to remember and compliance is hard to audit, meaning there is little feedback for an writer who strays.</db:para>
<db:para>The curse of knowledge and the distance separating writers from readers are a major source of complexity. Structured writing provides a way to guide and audit content, specifically, the <db:emphasis>rhetoric</db:emphasis> of our content: what we say and how we say it to achieve a specific end.</db:para>
<db:para>Therefore, when people ask me whether structured writing is relevant, I say “garbage in, garbage out.” Structured writing enables us to write in the subject domain (wholly or partially), which allows us to guide and audit rhetoric in ways not otherwise possible. It also allows us to factor out many constraints, simplifying the writing task and allowing writers to devote more attention to the quality of their content, whether that content is read by people or robots.</db:para>
</db:section>
<db:section>
<db:title>Structure, art, and science</db:title>
<db:para>Many writers reject the idea that imposing constraints can improve quality. They see writing as a uniquely human and individual act, an art, not a science and, therefore, immune to the encroachment of algorithms and robots. But I suggest that structures and algorithms do not diminish the human and artistic aspects of writing. Rather, they supplement and enhance them.</db:para>
<db:para>We see this pattern in all the arts. Music has always depended on making and perfecting instruments as tools of the musician. Similarly the mathematics of musical theory gave us <db:indexterm><db:primary>well-tempered tuning</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>well-tempered tuning</db:secondary></db:indexterm>well-tempered tuning, on which modern Western music is based.</db:para>
<db:para>Computer programming is widely regarded as an art<db:footnote><db:para>http://ruben.verborgh.org/blog/2013/02/21/programming-is-an-art/</db:para></db:footnote> among practitioners, but the use of sound structures is an inseparable part of that art. Art lies not in the rejection of structure but in its mature and creative use. As noted computer scientist Donald Knuth observes in his essay, <db:indexterm><db:primary>Computer Programming as an Art</db:primary></db:indexterm><db:indexterm><db:primary>citetitle</db:primary><db:secondary>Computer Programming as an Art</db:secondary></db:indexterm><db:citetitle>Computer Programming as an Art</db:citetitle>, most fields are not either an art or a science, but a mixture of both.</db:para>
<db:blockquote>
<db:attribution>http://dl.acm.org/citation.cfm?id=361612</db:attribution>
<db:para>Apparently most authors who examine such a question come to this same conclusion, that their subject is both a science and an art, whatever their subject is. I found a book about elementary photography, written in 1893, which stated that “the development of the photographic image is both an art and a science.” In fact, when I first picked up a dictionary in order to study the words “art” and “science,” I happened to glance at the editor’s preface, which began by saying, “The making of a dictionary is both a science and an art.”</db:para>
</db:blockquote>
<db:para>As writers, we can use structures, patterns, and algorithms as aids to art, just like every other profession. Of course, few writers would claim that there is no structure involved in writing. We have long recognized the importance of grammatical structure and rhetorical structure in enhancing communication. The question is, can the type of structures that structured writing proposes improve our writing, and if so, in what areas? Traditional poetry is highly structured, but using an <db:indexterm><db:primary>XML schema</db:primary></db:indexterm><db:indexterm><db:primary>tool</db:primary><db:secondary>XML schema</db:secondary></db:indexterm>XML schema won’t help you write a better sonnet. On the other hand, following the accepted rhetorical pattern of a recipe will help you write a better cookbook, and using structured writing to create your recipes will help you improve the consistency of your recipes, produce them more efficiently, and exploit them as assets in new ways.</db:para>
<db:para>The question then becomes: how much of our work is like recipes and how much is like sonnets? That is, how much of business and technical communication would benefit from structured writing? The answer, I believe, is that a great deal of business and technical communication would benefit. You may not see any obvious structure in much of that communication, but that is not evidence that structure is inappropriate. Rather, it means that structure has not been developed and applied to the content.</db:para>
</db:section>
<db:section>
<db:title>Contra-structured content</db:title>
<db:para>We must also acknowledge that many writers have had a bad experience with structured writing. Often, the structured writing system was not chosen or designed by the writers to enhance their art; it was imposed externally for some other purpose, such as to facilitate the operation of a content management system or to make it easier to reuse content. Too frequently, such systems move complexity away from some other function and dump it on writers without sufficient thought being given to where that complexity should go. Such systems actively interfere with the writer’s task, hindering the production of quality content.</db:para>
<db:para>A badly designed structured writing system forces writers to use structures that are not designed to support their tasks. The result is not merely unstructured for these purposes, it is actually <db:emphasis>contra-structured</db:emphasis>. Such a system enforces structures that actively block writers from doing their best work.</db:para>
<db:para>I talk to writers all the time who show me page designs and layouts that make no sense, lamenting that the system gives them no alternatives. Content structure is not generic, and you cannot expect to simply install the flavor-of-the-month CMS or structured writing system and get a good outcome. The system you use must at least be compatible with, if not actively supportive of, the rhetoric you need to create.</db:para>
<db:para>Properly applied, as a means to guide and enhance the work of writers, structured writing can substantially improve content quality. In upcoming chapters, I will look at the algorithms of structured content, many of which relate directly to content quality.</db:para>
</db:section>
<db:section>
<db:title>Until the robots take over</db:title>
<db:para>Of course, this all presumes that the machines are not becoming better writers than us. Companies like Narrative Science are working to change that, with systems that generate “human-sounding data-driven communications at enterprise scale.”<db:footnote>
<db:para>Narrative Science[http://narrativescience.com]</db:para>
</db:footnote></db:para>

<db:para>Do robots suffer from the <db:indexterm><db:primary>curse of knowledge</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>curse of knowledge</db:secondary></db:indexterm>curse of knowledge? Maybe not. But current writing robots certainly work with highly structured data, so structured writing is still key to quality content even when the robots do come for our keyboards.</db:para>
<db:para>Actually, according to James Bessen’s recent article in The Atlantic, The Automation Paradox,<db:footnote><db:para>http://www.theatlantic.com/business/archive/2016/01/automation-paradox/424437/</db:para></db:footnote> automation does not decimate white collar jobs the way we have been told to fear. By reducing costs, it increases demand, resulting in net growth of jobs, at least for people who learn to use the new technology effectively.</db:para>
<db:para>That said, all the semantic technology and content management in the world won’t make any difference until we improve the quality of content on a consistent basis. Appropriately applied, structured writing, particularly structured writing in the <db:indexterm><db:primary>subject domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject domain</db:secondary></db:indexterm>subject domain, is one of our best tools for doing that.</db:para>
<db:indexterm class="endofrange" startref="idp34772068"/><db:indexterm class="endofrange" startref="idp34772068x"/></db:section>
</db:chapter>
