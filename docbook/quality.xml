<?xml version="1.0"?>
<db:chapter xmlns:db="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:strings="http://exslt.org/strings" version="5.0" xml:id="chapter.quality">
<db:title>Quality in Structured Writing</db:title>

<db:para>When I talk to programmers about what I do, they sometimes ask me why structured content is important any more. Machines are getting so good at reading human language, they argue, that semantic markup to assist the machine is increasingly becoming pointless.</db:para>
<db:section>
<db:title>Robots that read</db:title>
<db:para>Indeed, machines are getting better and better at understanding human language. An approach called Deep Learning is increasingly becoming a key technology for companies like Facebook, Google, and Baidu for both language comprehension<db:footnote><db:para>http://www.technologyreview.com/featuredstory/540001/teaching-machines-to-understand-us/</db:para></db:footnote> and speech recognition<db:footnote><db:para>http://www.technologyreview.com/news/544651/baidus-deep-learning-system-rivals-people-at-speech-recognition/</db:para></db:footnote>.</db:para>
<db:para>The semantic web initiative has long sought to create a Web that is not just people talking to people but also machines talking to machines. This has traditionally involved an essentially separate communication channel – semantic markup embedded in texts but not presented to the human reader. It has also involved the creation of specialized semantic data stores with query languages to match, to teach computers to understand relationships that humans would express in ordinary language.</db:para>
<db:para>But this two-channel approach – one text for the human, another for the machine – only makes sense if we assume that the machine cannot read human language. If machine and human can both read the same text then we shouldn’t need two channels. The human Web becomes the semantic Web.</db:para>
<db:para>After all, the human text always was semantic. Semantics is simply the study of meaning. All meaningful texts have semantics. It is just that it has been difficult to build algorithms that could read and understand like humans do. Semantic technologies are about dumbing the semantics down for the machine because the machine is not bright enough to read the regular semantics.</db:para>
</db:section>
<db:section>
<db:title>Dumbing it down for the robots</db:title>
<db:para>This dumbing down necessarily involves omitting a great deal of the semantics of the text. Fully expressing all the meaning and implications of even the simplest text in RDF triples, for instance, would be daunting. This has always created a problem for semantic technologies: which semantics do you select to dumb down to the machine’s level, and for what purpose? This is why there is no universal approach to structured writing that works for all purposes and all subject matter. You can only represent a fraction of the human semantics to the machine, and which you choose depends on what specific functions you want the machine to perform.</db:para>
<db:para>But if the machine could read the text as well as you can, then these limitation vanish. Deep learning is moving us in that direction.</db:para>
<db:para>Why then should we bother with structured writing? Quite simply because while machines may be rapidly learning to read human text, that text is still written by humans, and most humans are not good writers.</db:para>
</db:section>
<db:section>
<db:title>Making humans better writers</db:title>
<db:para>By that I don’t just mean that they use poor grammar or spelling or that they create run-on sentences or use the passive voice too much, though all those things may be true, and annoying. I mean something more fundamental than that: they don’t say the right things in the right way for the right audience. They leave out stuff that needs to be said, or they weigh it down with stuff that does not need to be said, or they say it in a way that is hard to understand.</db:para>
<db:para>We all suffer from a malady called the Curse of Knowledge<db:footnote><db:para>https://en.wikipedia.org/wiki/Curse_of_knowledge</db:para></db:footnote> which makes it difficult for us to understand what it is like not to understand something we know. We take shortcuts, we make assumptions, we say things in obscure ways, and we just plain leave stuff out.</db:para>
<db:para>This is not a result of mere carelessness. The efficiency of human communication rests on our ability to assume that the person we are communicating with shares a huge collection of experiences, ideas, and vocabulary in common with us.<db:footnote><db:para>http://everypageispageone.com/2015/08/04/the-economy-of-language-or-why-we-argue-about-words/</db:para></db:footnote> Laboriously stating the obvious is as much a writing fault as omitting the necessary. Yet what is obvious to one reader may be obscure to another. The curse of knowledge is that as soon as something becomes obvious to us, we can no longer imagine it being obscure to someone else.</db:para>
<db:para>Thus much of human to human communication fails. The recipient of the communication simply does not understand it or does not receive the information they need because the writer left it out. Machines may learn to be better readers than we are, but even machines are not going to learn to read information that just isn’t there.</db:para>
</db:section>
<db:section>
<db:title>We write better for robots than we do for humans</db:title>
<db:para>Actually, one of the advantages of the relative stupidity of robots is that is forces us to be very careful in how we create and structure data for machines to act on. The computer science community coined the phrase “garbage in, garbage out” very early, because the machines were, and to a large extent still are, too stupid to know when the information they were taking in was garbage, and did not have the capacity, like human beings, to seek clarification or consult other sources. They just spit out garbage.</db:para>
<db:para>This meant that we had to put a huge emphasis on improving the quality and precision of the data going in. We diligently worked out its structures and put elaborate audit mechanisms in place to make sure that it was complete and correct before we fed it to the machine.</db:para>
<db:para>We have never been as diligent in improving the quality of the content that we have fed to human beings. Faced with poor content, human beings do not halt and catch fire; they either lose interest or do more research. Given our adaptability as researchers and our tenacity in pursuing things that really matter to us, we often manage to muddle through bad content, though at considerable economic cost. And the distance that often separates writers from readers means that the writers often have no notion of what the poor reader is going through. If readers did halt and catch fire, we might put more effort and attention into content quality.</db:para>
<db:para>Even today, when a huge emphasis is being placed on enterprise content management and the ability to make the store of corporate knowledge available to all employees, most of the emphasis is on making content easier to find, not on making it more worth finding. (This despite the fact that the best thing you can do to make content easy to find it to make it more worth finding.) People trying to build the semantic web spend a lot of time trying to make the data they prepare for machines correct, precise, and complete. We don’t do nearly as much for humans. Until we do, deep learning alone may not be enough to make the human web the semantic web.</db:para>
<db:para>Part of the problem has always been that improving content quality runs up against the curse of knowledge. Both the authors who create the content and most of the subject matter experts who review it suffer from the curse, meaning that there are few effective ways to audit written content. Style guides and templates can help remind authors of what is needed, but their requirements are difficult to remember and compliance is hard to audit, meaning there is little feedback for an author who strays.</db:para>
<db:para>The curse of knowledge and the distance separating writers from readers are a major source of complexity in the content creation process. Structured writing provides a way to guide and audit content for quality.   My reply to the people who ask me whether structured writing is relevant, therefore, is “garbage in, garbage out”. Structured writing enables us to write in the subject domain (wholly or partially) and this allows us to guide and audit in ways not otherwise possible. It also allows us to factor out many constraints, simplifying the author’s task, and therefore allowing them to give more of their attention to the writing task. These things make content better, whether that content is going to be read by people or by robots.</db:para>
</db:section>
<db:section>
<db:title>Structure, art, and science</db:title>
<db:para>To many writers, the idea that imposing constraints can improve quality is controversial. Many see quality writing as a uniquely human and individual act, an art, not a science, something immune to the encroachment of algorithms and robots. But I would suggest that the use of structures and algorithms as tools does not diminish the human and artistic aspects of writing. Rather, it supplements and enhances them.</db:para>
<db:para>And I would suggest that this is a pattern we see in all the arts. Music has always depended on the making and the perfecting of instruments as tools of the musician. Similarly the mathematics of musical theory gave us well tempered tuning, on which modern Western music is based.</db:para>
<db:para>Computer programming is widely regarded as an art<db:footnote><db:para>http://ruben.verborgh.org/blog/2013/02/21/programming-is-an-art/</db:para></db:footnote> among its practitioners, but the use of sound structures is recognized as an inseparable part of that art. Art lies not in the rejection of structure but in its mature and creative use. As noted computer scientist Donald Knuth observes in his essay, <db:citetitle>Computer Programming as an Art</db:citetitle>, most fields are not either an art or a science, but a mixture of both.</db:para>
<db:blockquote>
<db:attribution>http://dl.acm.org/citation.cfm?id=361612</db:attribution><db:para>Apparently most authors who examine such a question come to this same conclusion, that their subject is both a science and an art, whatever their subject is. I found a book about elementary photography, written in 1893, which stated that “the development of the photographic image is both an art and a science”. In fact, when I first picked up a dictionary in order to study the words “art” and “science,” I happened to glance at the editor’s preface, which began by saying, “The making of a dictionary is both a science and an art.”</db:para>
</db:blockquote>
<db:para>As writers we can use structures, patterns, and algorithms as aids to art, just like every other profession.</db:para>
<db:para>Of course, few writers would claim that there is no structure involved in writing. We have long recognized the importance of grammatical structure and rhetorical structure in enhancing communication. The question is, can the type of structures the structured writing proposes improve our writing, and if so, in what areas? Traditional poetry is highly structured, but it is doubtful that using an XML schema would help you write a better sonnet. On the other hand, it is clear that following the accepted rhetorical pattern of a recipe would help you write a better cookbook, and using structured writing to create your recipes can help you both improve the consistency of your recipes and to produce them more efficiently and exploit them as assets in new ways.</db:para>
<db:para>The question then becomes, how much of our work is like recipes and would benefit from structured writing, and how much is like sonnets and would not. The answer, I believe, is that a great deal of business and technical communication, at least, can benefit greatly. If you look at much of that communication and see no obvious structure, I would suggest that this is not evidence that structure is inappropriate, but that appropriate structure has not been developed and applied to the content.</db:para>
</db:section>
<db:section>
<db:title>Contra-structured content</db:title>
<db:para>We must also acknowledge that many writers have had a bad experience with structured writing. In many of these cases, the structured writing system was not chosen or designed by the writers to enhance their art; it was imposed externally for some other purpose, such to to facilitate the operation of a content management system or make it easier to reuse content. In other words, they were designed to shift complexity away from some other function without sufficient thought about where that complexity would go, and it ended up being dumped on the writers. In some cases, these systems actively interfere with the author’s art and directly hinder the production of quality content.</db:para>
<db:para>Writers who have had bad experiences with structured content have usually been faced with structures that were not designed for the writer’s purpose. Such content is not merely unstructured for these author’s purposes, it is actually contra-structured. It has an enforced structured that actively gets in the way of the author doing their best work.</db:para>
<db:para>I talk to authors all the time who show me page designs and layouts that make no sense, lamenting that the system does not give them any other choices. Content structure is not generic, and you cannot expect to simply install the flavor of the month CMS or structured writing system and get a good outcome.</db:para>
<db:para>Properly applied, however, as a means to guide and enhance the work of authors, structured writing can substantially improve content quality. In upcoming chapters, we will look at the algorithms of structured content, many of which relate directly to the enhancement of content quality.</db:para>
</db:section>
<db:section>
<db:title>Until the robots take over</db:title>
<db:para>Of course, this all supposes that the machines are not becoming better writers than us as well. Companies like Narrative Science are working on that, but I don’t know if they are as far along that path as the deep learning folks are in teaching computers to read.</db:para>
<db:para>Do robots suffer from the curse of knowledge? Maybe not. But current writing robots certainly work with highly structured data, so structured writing is still key to quality content even when the robots do come for our keyboards.</db:para>
<db:para>Actually, according to James Bessen’s recent article in The Atlantic, The Automation Paradox<db:footnote><db:para>http://www.theatlantic.com/business/archive/2016/01/automation-paradox/424437/</db:para></db:footnote>, automation does not decimate white collar jobs the way we have been told to fear. By reducing costs, it increases demand, resulting in net growth of jobs, at least for people who learn to use the new technology effectively.</db:para>
<db:para>That said, all the semantic technology and content management in the world is not going to make the difference it should until we improve the quality of content on a consistent basis. Structured writing, particularly structured writing in the subject domain, is one of our best tools for doing that.</db:para>
<db:para>When we use structured writing to partition the complexity of the content system, we are partitioning much of that complexity to algorithms. This means that functions that used to be performed by people will now be performed by algorithms. This means that the people who are responsible for these functions will no longer be doing them by hand but will instead by responsible for defining, creating, and maintaining the algorithms that do them. This can include writers, who may create structures and algorithms to guide the rhetorical structure of content or to validate and audit content. If they don’t create these algorithms themselves, they will at least need to understand how to commission others to do it for them.</db:para>
<db:para>For this reason, the study of structured writing is first and foremost the study of algorithms. The chapters in this section will explore each of the main structured content algorithms, looking at both the function the algorithm performs and the structures that define the interfaces that supply information to those algorithms. However, these are not chapters on programming. Defining an algorithm for a process and writing software to implement that algorithm are two separate tasks. They may be performed by the same person or by different people. But the aim of this section is to introduce you to the design and capability of structured writing algorithms, not to the coding of software.</db:para>
<db:para>The chapters in this section cover a wide range of subjects, from publishing to reuse, from linking to collaboration, and from authoring to auditing. Some of these go well beyond what we think of as traditional publishing or content management functions. They also consider how each algorithm is performed in each of the structured writing domains. Some algorithms can be implemented in different domains. Some can only be effectively addressed in some of the domains, and the difference in both approach and reliability between implementations in different domains can be substantial.</db:para>
<db:para>As you go through these algorithms, however, be careful not to get too attached to any one domain or any one algorithm. As I noted at the beginning, quality in a modern content system is produced by partitioning the complexity of the whole systems so that every part of that complexity is handled by a person or process with the skills, time, and resources to handle it completely. Implementing any one algorithm in isolation can result in complexity being distributed out to other parts of the content system that are not well equipped to handle it. The aim is not to optimize any one function, but to ensure that complexity is being handled effectively across the whole content system. That will require a combination of algorithms designed to partition the content system appropriately to handle the unique set of factors that contribute complexity to your individual content system.</db:para>
</db:section>
</db:chapter>
