<?xml version="1.0"?>
<db:chapter xmlns:db="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:strings="http://exslt.org/strings" version="5.0" xml:id="chapter.extensible"><db:title>Extensible and Constrainable Languages</db:title><db:indexterm class="startofrange" significance="preferred" xml:id="idm725475484"><db:primary>extension</db:primary></db:indexterm><db:indexterm class="startofrange" xml:id="idm725475484x"><db:primary>concept</db:primary><db:secondary>extension</db:secondary></db:indexterm><db:indexterm class="startofrange" significance="preferred" xml:id="idm725473180"><db:primary>constraint</db:primary></db:indexterm><db:indexterm class="startofrange" xml:id="idm725473180x"><db:primary>concept</db:primary><db:secondary>constraint</db:secondary></db:indexterm>



<db:para>The languages we have looked at to this point are publicly specified and have existing tool chains. Some of them are more constrained than others, and some of them support different structured writing algorithms and different ways of partitioning and redirecting the complexity of the content system. Choosing one of them makes sense if the constraints they express and the algorithms they support are the ones that partition content complexity in a way that is right for your organization. If not, you will need to create your own structures to improve how complexity is partitioned and distributed in your organization.</db:para>
<db:para>There are essentially three options for this:</db:para>
<db:itemizedlist>
<db:listitem>
<db:para>Create your own language entirely from scratch, creating both the syntax and the semantics. (This is what <db:indexterm><db:primary>John Gruber</db:primary></db:indexterm><db:indexterm><db:primary>person</db:primary><db:secondary>John Gruber</db:secondary></db:indexterm>John Gruber did when he created <db:indexterm><db:primary>MarkDown</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>MarkDown</db:secondary></db:indexterm>MarkDown.)</db:para>
</db:listitem>
<db:listitem>
<db:para>Use an existing definition of markup syntax, such as XML or SAM, and create your own semantics by defining named structures using that syntax as described in <db:xref linkend="chapter.markup"/>.</db:para>
</db:listitem>
<db:listitem>
<db:para>Take an existing markup language with extensible and/or constrainable semantics, such as DITA or DocBook and extend and/or constrain it to meet your needs.</db:para>
</db:listitem>
</db:itemizedlist>
<db:para>Each of these approaches has its merits and its drawbacks. For instance, creating a new language entirely from scratch may enable it to achieve exceptional functional lucidity for a particular type of information; extending/constraining an existing language can save you a lot of tool development costs; while defining your own semantics based on an existing syntax may let you find the right balance between functional lucidity and development costs.</db:para>
<db:para>This chapter will look at the various kinds of extensible and constrainable markup languages.</db:para>
<db:section>
<db:title>XML</db:title>
<db:para>The X in <db:indexterm><db:primary>XML</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>XML</db:secondary></db:indexterm><db:indexterm><db:primary>XML</db:primary></db:indexterm><db:indexterm><db:primary>standard</db:primary><db:secondary>XML</db:secondary></db:indexterm>XML stands for eXtensible, but, as we noted in <db:xref linkend="chapter.markup"/> XML is an abstract language that does not define any document structures itself. Extension in XML, therefore, is extension from zero. Syntactically, everything is defined for you. Semantically you are starting from scratch.</db:para>
<db:para>You can define the structure of a new XML markup language using one of the several available <db:indexterm><db:primary>schema languages</db:primary></db:indexterm><db:indexterm><db:primary>tool</db:primary><db:secondary>schema languages</db:secondary></db:indexterm>schema languages. We will look at schema languages in <db:xref linkend="chapter.constraints"/>, but the mechanics of defining a markup language in XML are out of scope for this book.</db:para>
</db:section>
<db:section>
<db:title>DITA</db:title>
<db:para>DITA is somewhat unique among markup languages in that it was designed for extension from the beginning. In fact, it is something of a misnomer to call DITA a markup language. DITA actually calls itself an information typing architecture. What is an information typing architecture? DITA is really the only thing that calls itself by this name, so to a certain extent we have to derive the definition from the properties of this one example.</db:para>
<db:para>The conventional way to define a document type is using a schema language. Schema languages are simply languages for describing constraints on markup structures. (We will look at them in <db:xref linkend="chapter.constraints"/>). So what does an information typing architecture provide over and above what a schema language provides?</db:para>
<db:para>There are plenty of precedents for this distinction. The programming world makes use of architectures and frameworks to abstract certain types of operation to a higher level. You could program these functions from scratch, but the architecture or framework is designed to save time and possibly avoid errors. In other words, an architecture partitions and distributes some aspects of a development project away from the local developer to the developer of the architecture and the algorithms the architecture provides. The usefulness of an architecture depends on whether the partitioning and distribution of the problem space that it provides is a good match for your problem space. In other words, does it partition and distribute the complexity of the system in substantially the same way you would have done it you were designing the system from scratch. (Of course, you might not have come up with all the good ideas in a good architecture yourself; the point is that once you have been exposed to those ideas and understood them, would you then partition and distribute complexity according to those ideas or not?) So the question to ask about DITA is, does its architecture partition and distribute content complexity the way that you would naturally wish it to be done to achieve the best outcome for your organization?</db:para>
<db:para>In DITA’s case, the architecture consists of a set of predefined markup languages that are intended as a basis for extension through a mechanism known as specialization together with an extensive set of <db:indexterm><db:primary>management domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>management domain</db:secondary></db:indexterm>management domain markup and the specification of the behavior it should produce, as well as a facility (maps) for assembling information products, and a facility (subject schema) for managing metadata.</db:para>
<db:para>In other words, it predefines a range of structures, semantics, and operations that you might need in establishing an information architecture and then provides a way for you to build from there.</db:para>
<db:para>As with any other architecture, its usefulness depend on how well the predefined structures, semantics, and operations suit your needs, how easy the extension mechanism is to use, and how reliable the available implementations are.</db:para>
<db:para>It is common in the software world for there to be many competing architectures with different sweet spots – different ways of partitioning and distributing the complexity of the development process or the process the tool you are developing it to serve. Because an architecture is essentially a series of guesses about what a variety of systems may have in common, different architectures may be constructed very differently to cover different sets of commonalities among diverse projects and you may not see equivalent architectural features from one architecture to another.</db:para>
<db:para>There are not a lot of information typing architectures. The only other one I am aware of is the one I am developing myself, which is called <db:indexterm><db:primary>SPFE</db:primary></db:indexterm><db:indexterm><db:primary>tool</db:primary><db:secondary>SPFE</db:secondary></db:indexterm>SPFE. <db:indexterm><db:primary>SPFE</db:primary></db:indexterm><db:indexterm><db:primary>tool</db:primary><db:secondary>SPFE</db:secondary></db:indexterm>SPFE, however, is a very different kind of architecture from <db:indexterm><db:primary>DITA</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>DITA</db:secondary></db:indexterm><db:indexterm><db:primary>DITA</db:primary></db:indexterm><db:indexterm><db:primary>standard</db:primary><db:secondary>DITA</db:secondary></db:indexterm>DITA. <db:indexterm><db:primary>DITA</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>DITA</db:secondary></db:indexterm><db:indexterm><db:primary>DITA</db:primary></db:indexterm><db:indexterm><db:primary>standard</db:primary><db:secondary>DITA</db:secondary></db:indexterm>DITA has grown into a very large and complex architecture over the years, and it is out of scope for this book to attempt to describe its architecture in full.<db:footnote>
<db:para>Obviously I am not a fan of the DITA architecture or I would not be developing my own architecture in SPFE. For a much fuller and more sympathetic treatment of the DITA architecture, see <db:citetitle>DITA for Practitioners Volume 1: Architecture and Technology</db:citetitle> by Elliot Kimber, <db:link xlink:href="http://xmlpress.net/publications/dita/practitioners-1/">http://xmlpress.net/publications/dita/practitioners-1/</db:link></db:para>
</db:footnote> All I shall attempt to do here is to do a basic mapping of some key features of the DITA architecture to the structured writing concepts explored in this book.</db:para>

<db:para>Inherent in the process of constructing an architecture is that you partition the field in certain ways. Architectures move functionality to a higher level by choosing some options and rejecting others. An XML schema is an information typing language. But a schema can define a markup language for any purpose at all, such as recording transfers between banks or storing the configuration options of an editor. Describing banking transactions or storing configuration options is not within the scope of the information typing that the DITA architecture was designed for. DITA therefore has a more restricted definition of “information typing”. The DITA specification defines “information typing” this way:</db:para>
<db:blockquote>
<db:attribution>http://docs.oasis-open.org/dita/dita/v1.3/csd01/part3-all-inclusive/archSpec/base/information-typing.html</db:attribution><db:para>Information typing is the practice of identifying types of topics, such as concept, reference, and task, to clearly distinguish between different types of information.</db:para>
</db:blockquote>
<db:para>Unfortunately this definition is largely circular – information typing defines information types. But it does help establish a scale. Information typing is about defining topic types. The spec goes on to define the purpose of information typing:</db:para>
<db:blockquote>
<db:para>Information typing is a practice designed to keep documentation focused and modular, thus making it clearer to readers, easier to search and navigate, and more suitable for reuse.</db:para>
</db:blockquote>
<db:para>DITA information typing then, is not as general as structured writing. It is focused on information at a particular scale and on a subset of the structured writing algorithms. (That does not mean that it makes it impossible to work at other scales or implement other algorithms, it just means that these are the areas that the architecture supports at a higher level.)</db:para>
<db:para>Out-of-the-box DITA is commonly associated with the idea that there are just three information types, task, concept, and reference. The DITA spec makes it clear that this is not the intention of DITA as an information typing architecture.</db:para>
<db:blockquote>
<db:para>DITA currently defines a small set of well-established information types that reflects common practices in certain business domains, for example, technical communication and instruction and assessment. However, the set of possible information types is unbounded. Through the mechanism of specialization, new information types can be defined as specializations of the base topic type (&lt;topic&gt;) or as refinements of existing topics types, for example, &lt;concept&gt;, &lt;task&gt;, &lt;reference&gt;, or &lt;learningContent&gt;.</db:para>
</db:blockquote>
<db:para>As I have noted many times, many of the structured writing algorithms partition complexity best with more specific markup, particularly markup in the subject domain. The ability to create an unbounded set of information types is therefore very relevant to getting the most out of structured writing.</db:para>
<db:para>Clearly, though, one does not need an information typing architecture to define an information type. You can, as <db:indexterm><db:primary>John Gruber</db:primary></db:indexterm><db:indexterm><db:primary>person</db:primary><db:secondary>John Gruber</db:secondary></db:indexterm>John Gruber did with <db:indexterm><db:primary>MarkDown</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>MarkDown</db:secondary></db:indexterm>MarkDown, sit down and sketch out a set of structures and a syntax to represent them, and then write a program to process them. With an <db:indexterm><db:primary>abstract language</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>abstract language</db:secondary></db:indexterm>abstract language like <db:indexterm><db:primary>XML</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>XML</db:secondary></db:indexterm><db:indexterm><db:primary>XML</db:primary></db:indexterm><db:indexterm><db:primary>standard</db:primary><db:secondary>XML</db:secondary></db:indexterm>XML or <db:indexterm><db:primary>SAM</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>SAM</db:secondary></db:indexterm>SAM, you can create a new information type by defining a set of named elements and attributes using a schema language. How does using a higher level “information typing architecture” like DITA change this process? How does it partition the design problem differently?</db:para>
<db:para>First and foremost, it means that you don’t start from scratch. All topic types in DITA are derived from a base topic type called <db:code>topic</db:code> by a process called specialization.</db:para>
<db:para>What is specialization? We noted that XML is an abstract language, meaning that its syntax defines abstract structures that do not occur in documents: elements, attributes, etc. To create a markup language in XML, you define named elements and attributes for the structures you are creating. Thus in <db:indexterm><db:primary>DocBook</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>DocBook</db:secondary></db:indexterm><db:indexterm><db:primary>DocBook</db:primary></db:indexterm><db:indexterm><db:primary>standard</db:primary><db:secondary>DocBook</db:secondary></db:indexterm>DocBook <db:code>para</db:code> is a type of element. <db:code>para</db:code> has what is called an “is-a” relationship to elements. This is a type of specialization. <db:code>para</db:code> “is-an” element, but it is a special type of element. An XML parser will process it generically as an element, reporting its name to the application layer. The application layer must then supply a rule that processes just this specialized <db:code>para</db:code> element (and not the also specialized but different <db:code>title</db:code> element).</db:para>
<db:para>DITA specialization follows the same principle, but moves it up a level. The base <db:code>topic</db:code> topic type is the abstract structure. More specific types like <db:code>knitting-pattern</db:code> or <db:code>ingredients-list</db:code> are specializations of <db:code>topic</db:code> (or of other topic types that are specializations of <db:code>topic</db:code>). A generic DITA processor can process them as a instance of <db:code>topic</db:code>, but it would require additional code to process them specifically as <db:code>knitting-pattern</db:code> or <db:code>ingredient-list</db:code>. Each of these specialized types has an “is-a” relationship with the type is was specialized from. So <db:code>knitting-pattern</db:code> “is-a” topic.</db:para>
<db:para>But DITA specialization is different from simply creating new named elements in XML in a number of ways.</db:para>
<db:para>First, the base DITA topic type is not an abstraction like an XML element. You cannot create an XML element without inventing a name for it. The base DITA topic type, on the other hand, is a fully implemented topic type that you can use directly. You can, and people do, write directly in the base topic type without inventing anything new. We noted in <db:xref linkend="chapter.rhetorical_structure"/> that it is sometimes easy to treat what is intended as a meta model as a generic model. This is the case here. All topic types in DITA are derived by specialization from the generic <db:code>topic</db:code> type. They all have an “is-a” relationship to this generic type.</db:para>
<db:para>One consequence of this is that while the set of topic types you can create with DITA may be unbounded, it is not universal. The generic topic type has specific characteristics and if a specialized topic has an is-a relationship to the generic topic, the structures in the specialized topics have a corresponding is-a relationship to structures in the generic type. Thus there can be information types that cannot reasonably be said to have an is-a relationship to a DITA generic topic. That is, there can be information types such that processing them using the code of the type they are specialized from would produce no meaningful result. For example, an information type that factored out most of the text that would appear in the published version would not process meaningfully as a generic topic because the factored-out text would not be restored.</db:para>
<db:para>To specialize a topic type, you specialize the root element and any child elements or attributes that you need to define your new topic type. Each specialized element or attribute should have and is-a relationship to the element it specializes. Thus a procedure element might be a specialization of an ordered list element and its step elements might be specializations a of a list item element. (You can see that in this case, processing a procedure as an ordered list would produce meaningful output, but that you might also want to specialize the output of steps in a procedure, perhaps by prefixing each step with “Step 1:" rather than just “1.”.)</db:para>
<db:para>The second way in which specialization differs from giving names to abstract elements is that specialization is recursive. That is, suppose you have a topic type <db:code>animal-description</db:code>, which is a specialization of <db:code>topic</db:code>. You want to impose additional constraints on the description of different types of animal, so you create more specialized types <db:code>fish-description</db:code> and <db:code>mammal-description</db:code> which are specializations of <db:code>animal-description</db:code> (and would be processed like an <db:code>animal-description</db:code> if no other processing were specified for them). Then you might decide to impose still more constraints on the description of different kinds of mammals, so you create a type, <db:code>horse-description</db:code> that is a specialization of <db:code>mammal-description</db:code>. This type will be processed as a <db:code>mammal-description</db:code> if no specific processing is provided for <db:code>horse-description</db:code>; as <db:code>animal-description</db:code> if no specific <db:code>mammal-description</db:code> processing is provided; and as <db:code>topic</db:code> if no specific <db:code>animal-description</db:code> processing is provided.</db:para>
<db:para>The third way in which information typing in DITA differs from doing it from scratch is that DITA information types share a common approach to processing and to information architecture. In particular, they inherit a common set of <db:indexterm><db:primary>management domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>management domain</db:secondary></db:indexterm>management domain structures and their associated management semantics. It is possible to ignore all of these things, but if you do so, the value of using DITA for information typing is reduced because you then have to invent your own ways of doing these things that are contrary to the partitioning provide by the architecture.</db:para>
<db:para>As a generality, the fewer pieces of an architecture you use, the less value there is to basing your work on that architecture, both because you have more work to do, and because you take less advantage of the infrastructure or tools and expertise surrounding that architecture, and create a system that is less understandable to people versed in the architecture. All this betrays a poor fit between the system partitioning you are creating and the partitioning that the architecture provides. All architectures come with overheads and if you don’t use their features, you still have to live with their overheads, which adds cost and complexity to your system. Thus while you can use DITA and depart from the default DITA way of doing things, the value of using DITA diminishes the further you depart from the DITA way. The same would be true of any other information typing architecture.</db:para>
</db:section>
<db:section>
<db:title>Specializing between domains</db:title>
<db:para>We have talked a lot about the benefits of moving content from the document domain to the subject domain as a means to factor out constraints in the document domain and as a way to enable multiple structured writing algorithms and improve functional lucidity.</db:para>
<db:para>DITA’s base types clearly belong to the document domain. Clearly you can use specialization to create more specialized document domain structures. But can you use specialization to create subject-domain structures that factor out aspects of the document domain?</db:para>
<db:para>In formal terms the answer is no. Subject domain information does not have an is-a relationship to document domain information precisely because it is the document domain structures that you factor out when you move to the subject domain.</db:para>
<db:para>Take the list of ingredients in a recipe. In the document domain, they could be presented as a list or as a table. In subject domain terms they are actually more of a table (database sense) than a list. That is, a set of records with a defined semantic structure:</db:para>
<db:programlisting language="sam">
ingredients:: ingredient, quantity, unit
    eggs, 3, each
    salt, 1, tsp
    butter, .5, cup 
</db:programlisting>
<db:para>How do you create this record structure by specializing document domain elements? What is the best starting point to specialize from? A table is the most obvious candidate because it is structured like a set of records. However, the more common presentation of an ingredient list is as a list. But there is no structure in a list that supports dividing a list item into three named fields. The ingredient structure above is neither a list nor a table; it is a record set, a data structure whose contents could be presented in many different ways in the document domain, but is not a specialization of any of them.</db:para>
<db:para>We can make a distinction between two main types of <db:indexterm><db:primary>subject domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject domain</db:secondary></db:indexterm>subject domain languages, the rhetorical subject domain, where the focus in on how to present information on a particular subject, and the data-oriented subject domain, where the focus in on capturing the various pieces of information that are required on a subject while factoring out the presentation. Both present problems for specialization.</db:para>
<db:para>One of the elementary things that the rhetorical subject domain does is to factor out the titles of various sections of a topic, replacing the generic structure of sections and titles with specific names blocks whose titles can be supplied consistently by the <db:indexterm><db:primary>presentation algorithm</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>presentation algorithm</db:secondary></db:indexterm>presentation algorithm. These named sections do have a kind of is-a relationship to generic sections, in that they are sections. But this is not a true is-a relationship because a generic section requires a title, and a rhetorical block cannot have on. It therefore does not have all the characteristics of the thing it is specialized from, and therefore it is not a section. It is a named block of text that, in the document domain, would be expressed as a section, but that is not the same things as falling back to being a section. Processing such a block as a section would result in the text following on from that of the previous section with no title, thus lacking the defining document and media domain characteristic of a section.</db:para>
<db:para>For the data-oriented subject domain, which, let us remember, can include material recorded in databases and source code, there is no necessary relationship between the structure of content in the source and the presentation of content in the document domain. Information that is in separate fields in the subject domain may need to be combined to form sentences by the <db:indexterm><db:primary>presentation algorithm</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>presentation algorithm</db:secondary></db:indexterm>presentation algorithm before it is even readable as a piece of content. Such structures clearly have no is-a relationship to a DITA generic topic.</db:para>
<db:para>More generally, as we have seen, the <db:indexterm><db:primary>subject domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject domain</db:secondary></db:indexterm>subject domain changes the algorithm for each function so inheritance of the code is of little or no value. Subject domain algorithms work differently. And since all subject domain algorithms work on the same structures, rather than there being separate structures for each algorithm, as in the document and management domains, there is little scope for inheritance of processing code from the document domain to the subject domain. Rather, the document domain is semantically downstream from the subject domain. Subject domain algorithms essentially create document domain structures as part of the publication process, as we saw in <db:xref linkend="chapter.publishing"/>.</db:para>
</db:section>
<db:section>
<db:title>DocBook</db:title>
<db:para><db:indexterm><db:primary>DocBook</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>DocBook</db:secondary></db:indexterm><db:indexterm><db:primary>DocBook</db:primary></db:indexterm><db:indexterm><db:primary>standard</db:primary><db:secondary>DocBook</db:secondary></db:indexterm>DocBook is not really extensible in the same sense as the other languages mentioned here, but it still deserves a mention. DocBook does not provide an extension mechanism like <db:indexterm><db:primary>DITA</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>DITA</db:secondary></db:indexterm><db:indexterm><db:primary>DITA</db:primary></db:indexterm><db:indexterm><db:primary>standard</db:primary><db:secondary>DITA</db:secondary></db:indexterm>DITA’s specialization. What it does provide is a deliberately modular construction that makes it easy to create new schemas that include elements from DocBook. DocBook takes full advantage of the extensibility features built into XML schema languages.</db:para>
<db:para>Does the fact that DocBook does not invent its own extension mechanism means that it is not as extensible as DITA? No. By relying on XML’s own extensibility features, which are both more comprehensive and lower level than DITA’s specialization mechanism, DocBook is as extensible as it is possible for any XML vocabulary to be.</db:para>
<db:para>Where it differs from DITA is that there is no fall-back processing.  Extensions DocBook are not DocBook. They are new languages that incorporate DocBook structures. The extensions cannot be processed by standard DocBook tool chains, though the incorporated DocBook structures obviously can. DITA’s specialization mechanism means that a specialized topic will always pass through the DITA publication process, though whether it will be presented in a useful or comprehensible way very much depends on how well the is-a relationship between specialization and base was maintained. If you would rather ensure that topics always pass through the publication process, even if the results are gibberish, DITA will support that. If you want to ensure that errors are raised if any structure is not recognized by the publishing tool chain (thus avoiding accidental gibberish) then DocBook’s extension mechanism will give you that.</db:para>
<db:para>Another aspect of DocBook customization deserves to be mentioned here even though it is not strictly speaking extension. DocBook has a huge tag set and it is quite conceivable that if you want a small constrained document domain markup languages that you can create one by sub-setting DocBook. DocBook provides for just about every document structure out there, so if you are building a document domain language, chances are the pieces you need are in there.</db:para>
<db:para>The great advantage of creating a new language as a subset of DocBook is that the result is also a valid DocBook document and can therefore be published by the DocBook tool chain. You will not have to write any algorithms at all if you take this approach. Creating a subset of DocBook can therefore allow you to impose more constraints and improve <db:indexterm><db:primary>functional lucidity</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>functional lucidity</db:secondary></db:indexterm>functional lucidity significantly compared to standard DocBook without having to write any processing code at all.</db:para>
<db:para>Technically speaking, any XML-based markup language is extensible in the same way that DocBook is. However, DocBook’s structure, and the implementation of its schemas, was designed deliberately to support both extension and sub-setting of DocBook, something which is not true for many markup languages.</db:para>
</db:section>
<db:section>
<db:title>RestructuredText</db:title>
<db:para>RestructuredText has a number of blocks for things like paragraphs, titles, and lists that are defined with a concrete syntax. It defines other blocks using directives:</db:para>
<db:programlisting language="reStructuredText">
.. image:: images/biohazard.png
   :height: 100
   :width: 200
   :scale: 50
   :alt: alternate text
</db:programlisting>
<db:para>It is extensible by adding new directives to the language. However, there is no schema language for RestructuredText. To create a new directive, you have to create the code that processes it.</db:para>
<db:para>There is an important distinction to be made between languages that are extensible by schema and those that are extensible by writing code to process the extension. If a language is extended by writing processing code for the extension, the only way to know if the input is valid is by processing it. If it raises a processing error, it is invalid.</db:para>
<db:para>If you have only one processor for a language, you can treat that processor as normative. That is, the definition of a correct file is any file that can be successfully processed by the normative processor. The language, in other words, is defined by the processor. But if you have multiple processors, how do you determine who is at fault when of of those processor fails to process a given input file? Is the processor incorrect or the source file?</db:para>
<db:para>A schema creates a language definition that is independent of any processor. (In other words, it partitions and redirects the complexity of validation in language design.) It is the schema that is normative, not any of the processors. If the source file is valid per the schema, the processor is at fault if it does not process that file correctly. If the source file is not valid per the schema, the blame lies with the source file.</db:para>
<db:para>In the case of RestructuredText, the capacity of the processor to be extended in this way is built into the processor architecture. It is not like you have to hack around in the code to add your extensions. There is a specific and well documented way to do it. But while RestructuredText allows you to extend it by adding new directives, it does not have a constraint mechanism. There is no mechanism (other than by hacking into the code) to restrict the use either of new directives or the existing directives and structures.</db:para>
</db:section>
<db:section>
<db:title>TeX</db:title>
<db:para>TeX (pronounced “Tek”) is a typesetting system invented by Donald Knuth in 1978. As a typesetting language it is a concrete <db:indexterm><db:primary>media domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>media domain</db:secondary></db:indexterm>media domain language. But Knuth also included a macro language in TeX which allows users to define new commands in terms of existing commands. (I say commands because that is the term used in TeX. Markup in the media domain tends to be much more imperative than markup in the subject domain, which is entirely descriptive, so “commands” is an appropriate name for TeX’s tags.) This macro language has been used to extend TeX, most notably in the form of <db:indexterm><db:primary>LaTeX</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>LaTeX</db:secondary></db:indexterm>LaTeX, a <db:indexterm><db:primary>document-domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>document-domain</db:secondary></db:indexterm>document-domain language that we looked at in <db:xref linkend="chapter.heavyweight"/>.</db:para>
<db:para>As we noted with RestructuredText, extension of a language is not the same thing as constraint. Introducing new commands does not create a constraint mechanism.</db:para>
</db:section>
<db:section>
<db:title>SAM</db:title>
<db:para>While lightweight languages provide great <db:indexterm><db:primary>functional lucidity</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>functional lucidity</db:secondary></db:indexterm>functional lucidity, they suffer from limited extensibility (which generally requires writing code) and a general lack of constraint mechanisms. I believe that a fully extensible, fully constrainable lightweight markup language would be a valuable addition to the structured writing toolkit. This is why I have developed <db:indexterm><db:primary>SAM</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>SAM</db:secondary></db:indexterm>SAM, the markup language used for most of the examples in this book and for writing the book itself.</db:para>
<db:para>As described in <db:xref linkend="chapter.markup"/>, SAM is a hybrid markup language which combines implicit syntax similar to <db:indexterm><db:primary>MarkDown</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>MarkDown</db:secondary></db:indexterm>MarkDown with an explicit syntax for defining abstract structures called blocks, recordsets, and annotations, and with specific concrete markup for common features such as insertions, citations, and variable definitions.</db:para>
<db:para>SAM, like <db:indexterm><db:primary>XML</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>XML</db:secondary></db:indexterm><db:indexterm><db:primary>XML</db:primary></db:indexterm><db:indexterm><db:primary>standard</db:primary><db:secondary>XML</db:secondary></db:indexterm>XML, is for defining specific markup languages. However, all languages defined in SAM share a small common base set of text structures for which SAM provides concrete syntax. This allows SAM to combine lightweight syntax for the most common text structures with the ability to define specific constrained markup languages for particular purposes, particularly <db:indexterm><db:primary>subject domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject domain</db:secondary></db:indexterm>subject domain languages. In other words, SAM represents a different partitioning of the markup design process from both the common lightweight languages and from XML.</db:para>
<db:para>SAM is designed to be extensible and constrainable through a schema language (this is not complete at time of writing, but hopefully will be available by the time you read this). The intent is that the schema language should be able not only to define and constrain new block structures, but to constrain the use of the concrete structures as well, and to constrain the values of fields using patterns.</db:para>
<db:para>SAM is not designed to be nearly as general as XML in its applications. As a result, its syntax is simple and more <db:indexterm><db:primary>functionally lucid</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>functionally lucid</db:secondary></db:indexterm>functionally lucid and its schema language should also be simpler and make it much easier for writers to develop their own SAM-based markup languages.</db:para>
<db:para>I use SAM for the majority of the examples in this book because SAM is designed to make structure clear and that is all I have needed to do in most examples. All the examples could be expressed in XML as well. Using XML would just have made them harder to follow. Naturally, to write in SAM you would need to know more about the rules of the language, but you should be able to read a typical SAM document and understand its structure with little or no instruction.</db:para>
<db:para>This is similar, but not identical, to the aim of mainstream concrete and hybrid languages such as <db:indexterm><db:primary>Markdown</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>Markdown</db:secondary></db:indexterm>Markdown and <db:indexterm><db:primary>Restructured Text</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>Restructured Text</db:secondary></db:indexterm>Restructured Text, which is to have the source file be readable as a document. In other words, they strive to make the document structure clear from the markup. They are document domain languages, and they strive to make sure that the markup expresses the document structure they create in a way that is readable. SAM has the same goal, except that SAM was designed primarily for creating subject domain languages. As such, it is designed to make the subject domain structure of the document clear to the reader.</db:para>
<db:para>A SAM document may not look as much like a finished document as a <db:indexterm><db:primary>Markdown</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>Markdown</db:secondary></db:indexterm>Markdown or <db:indexterm><db:primary>reStructuredText</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>reStructuredText</db:secondary></db:indexterm>reStructuredText document. For example, it does not use underlines to visually denote different levels of header. Instead, it focuses on creating a hierarchy of named blocks and fields. In doing so, it uses the kind of markup people commonly use to create named blocks of text and to express a hierarchical relationship between them. Blocks are introduced with a name followed by a colon, and hierarchy is expressed through indentation.</db:para>
<db:programlisting language="SAM">
examples: Basic SAM structures

    example: Paragraphs
        The is a sample paragraph. It is inside
        the {block}(structure) called `example`.
        It contains two {annotations}(structure),
        including this one. It ends with a blank
        line.

        This is another paragraph.

    example: Lists

        Then there is a list:

        1. First item.
        2. Second item.
        3. Third item.

    example: Block quote

        Next is a block quote with a {citation}(structure).

        """[Mother Goose]
            Humpty Dumpty sat on a wall.
</db:programlisting>
<db:para>SAM is an open source project. A description of the language and a set of associated tools are available from <db:link xlink:href="https://github.com/mbakeranalecta/sam">https://github.com/mbakeranalecta/sam</db:link>.</db:para>
</db:section>
<db:section>
<db:title>SPFE</db:title>
<db:para><db:indexterm><db:primary>SPFE</db:primary></db:indexterm><db:indexterm><db:primary>tool</db:primary><db:secondary>SPFE</db:secondary></db:indexterm>SPFE is another project of mine. It is designed to be a framework for implementing structured writing algorithms and its structure follows the model I laid out in <db:xref linkend="chapter.publishing"/>. It is tempting to compare it to <db:indexterm><db:primary>DITA</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>DITA</db:secondary></db:indexterm><db:indexterm><db:primary>DITA</db:primary></db:indexterm><db:indexterm><db:primary>standard</db:primary><db:secondary>DITA</db:secondary></db:indexterm>DITA as an information typing architecture, but as I commented before, architectures are not necessarily parallel to each other and often differ in their emphasis. SPFE takes a different approach to the partitioning and distribution of content complexity, with a major emphasis on directing content management and information architecture complexity away from writers. Individual writers working in a SPFE system should have to know little or nothing about how SPFE works, as long as they follow the constraints of the markup language they are using.</db:para>
<db:para>SPFE is principally designed for <db:indexterm><db:primary>subject domain</db:primary></db:indexterm><db:indexterm><db:primary>concept</db:primary><db:secondary>subject domain</db:secondary></db:indexterm>subject domain markup. As such, it does not start with a generic document domain topic type like DITA. SPFE does not require any particular schema, though it does require that schemas meet certain constraints.</db:para>
<db:para>But SPFE does not leave it entirely to you to develop schemas from scratch. Instead, it supports building schemas from pre-built components. The pre-built components include a collection of semantic blocks and the default processing code for each stage of the publishing algorithm. SPFE also allows you to define your own reusable structured components with processing code. This is, essentially, extensibility through composition, rather than extensibility through specialization (as in <db:indexterm><db:primary>DITA</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>DITA</db:secondary></db:indexterm><db:indexterm><db:primary>DITA</db:primary></db:indexterm><db:indexterm><db:primary>standard</db:primary><db:secondary>DITA</db:secondary></db:indexterm>DITA) or extensibility through processor extension (and in <db:indexterm><db:primary>reStructuredText</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>reStructuredText</db:secondary></db:indexterm>reStructuredText). Constraints are supported through normal schema mechanisms and by selecting the minimal required structural components for the individual case.</db:para>
<db:para>By strictly segregating the presentation and formatting layers, SPFE reduces the effort required to process custom markup formats. Custom format are processed to a common document-domain markup language which it then processed to all required media-domain output formats. The SPFE Open Tool Kit includes a basic document domain language for this purpose, but you can also use DocBook or DITA in this role, allowing you to take advantage of their existing publishing capabilities. This also allows you to install SPFE as an authoring layer on top of an existing DITA or DocBook tool chain.</db:para>
<db:para>To create a subject-domain markup language in SPFE, therefore, all you have to define for yourself are the key subject-domain fields and blocks that are essential to your business. All the other elements you need, such as paragraphs, lists, tables, and common annotations, you can include from the pre-built components, along with their default processing code.</db:para>
<db:para>Among its default processing steps, the SPFE process includes the subject-based linking algorithms described in <db:xref linkend="chapter.linking"/> and the subject-based composition and architecture algorithms described in <db:xref linkend="chapter.composition"/> and <db:xref linkend="chapter.architecture"/>, including bottom-up information architecture. The <db:indexterm><db:primary>conformance algorithm</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>conformance algorithm</db:secondary></db:indexterm>conformance and <db:indexterm><db:primary>audit</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>audit</db:secondary></db:indexterm>audit algorithms are well-supported as well.</db:para>
<db:para>While it has support for <db:indexterm><db:primary>reuse</db:primary></db:indexterm><db:indexterm><db:primary>algorithm</db:primary><db:secondary>reuse</db:secondary></db:indexterm>reuse, SPFE is not as focused on content reuse or content management as <db:indexterm><db:primary>DITA</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>DITA</db:secondary></db:indexterm><db:indexterm><db:primary>DITA</db:primary></db:indexterm><db:indexterm><db:primary>standard</db:primary><db:secondary>DITA</db:secondary></db:indexterm>DITA. It deliberately limits some of the forms of reuse that tend to produce unmanageable complexity. While it can produce books and top-down information architectures, its main focus is hypertext and bottom-up information architectures. SPFE does not define or require maps as an assembly mechanisms, though you could implement maps in SPFE if you wanted them. SPFE’s processing model is modeled on a software build architecture and it is designed to work well with a <db:indexterm><db:primary>version control system</db:primary></db:indexterm><db:indexterm><db:primary>tool</db:primary><db:secondary>version control system</db:secondary></db:indexterm>version control system system as a repository rather than a content management system. One of its key design objectives is that writers should have to know little or nothing about how SPFE works.</db:para>
<db:para>Both <db:indexterm><db:primary>SAM</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>SAM</db:secondary></db:indexterm>SAM and <db:indexterm><db:primary>XML</db:primary></db:indexterm><db:indexterm><db:primary>language</db:primary><db:secondary>XML</db:secondary></db:indexterm><db:indexterm><db:primary>XML</db:primary></db:indexterm><db:indexterm><db:primary>standard</db:primary><db:secondary>XML</db:secondary></db:indexterm>XML are supported as markup syntax for SPFE, and you can freely mix and match SAM and XML content.</db:para>
<db:para>SPFE is an open source project available from <db:link xlink:href="http://spfeopentoolkit.org">http://spfeopentoolkit.org</db:link>.</db:para>
<db:indexterm class="endofrange" startref="idm725475484"/><db:indexterm class="endofrange" startref="idm725473180"/><db:indexterm class="endofrange" startref="idm725475484x"/><db:indexterm class="endofrange" startref="idm725473180x"/></db:section>
</db:chapter>
